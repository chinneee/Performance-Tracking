{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd56f6",
   "metadata": {},
   "source": [
    "# Append Selected Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f02d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_CA\\Tháng 9\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 12 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_CA\\Tháng 9\n",
      "Processing: SA_Campaign_List_20251001_20251001_goaUh8.xlsx\n",
      "  - Original shape: (49, 33)\n",
      "  - After cleaning: (49, 32)\n",
      "  - Extracted date: 2025-10-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (49, 20)\n",
      "  - Data preservation check: 49 rows maintained\n",
      "Processing: SA_Campaign_List_20251002_20251002_S0JJYu.xlsx\n",
      "  - Original shape: (73, 33)\n",
      "  - After cleaning: (73, 32)\n",
      "  - Extracted date: 2025-10-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (73, 20)\n",
      "  - Data preservation check: 73 rows maintained\n",
      "Processing: SA_Campaign_List_20251003_20251003_uxPlGp.xlsx\n",
      "  - Original shape: (72, 33)\n",
      "  - After cleaning: (72, 32)\n",
      "  - Extracted date: 2025-10-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (72, 20)\n",
      "  - Data preservation check: 72 rows maintained\n",
      "Processing: SA_Campaign_List_20251004_20251004_tUtJTm.xlsx\n",
      "  - Original shape: (69, 33)\n",
      "  - After cleaning: (69, 32)\n",
      "  - Extracted date: 2025-10-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (69, 20)\n",
      "  - Data preservation check: 69 rows maintained\n",
      "Processing: SA_Campaign_List_20251005_20251005_70ljI4.xlsx\n",
      "  - Original shape: (74, 33)\n",
      "  - After cleaning: (74, 32)\n",
      "  - Extracted date: 2025-10-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (74, 20)\n",
      "  - Data preservation check: 74 rows maintained\n",
      "Processing: SA_Campaign_List_20251006_20251006_xWMlFo.xlsx\n",
      "  - Original shape: (78, 33)\n",
      "  - After cleaning: (78, 32)\n",
      "  - Extracted date: 2025-10-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (78, 20)\n",
      "  - Data preservation check: 78 rows maintained\n",
      "Processing: SA_Campaign_List_20251007_20251007_GN9Z2J.xlsx\n",
      "  - Original shape: (87, 33)\n",
      "  - After cleaning: (87, 32)\n",
      "  - Extracted date: 2025-10-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (87, 20)\n",
      "  - Data preservation check: 87 rows maintained\n",
      "Processing: SA_Campaign_List_20251008_20251008_LOnWon.xlsx\n",
      "  - Original shape: (95, 33)\n",
      "  - After cleaning: (95, 32)\n",
      "  - Extracted date: 2025-10-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (95, 20)\n",
      "  - Data preservation check: 95 rows maintained\n",
      "Processing: SA_Campaign_List_20251009_20251009_PkqTLA.xlsx\n",
      "  - Original shape: (88, 33)\n",
      "  - After cleaning: (88, 32)\n",
      "  - Extracted date: 2025-10-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (88, 20)\n",
      "  - Data preservation check: 88 rows maintained\n",
      "Processing: SA_Campaign_List_20251010_20251010_l62RhM.xlsx\n",
      "  - Original shape: (70, 33)\n",
      "  - After cleaning: (70, 32)\n",
      "  - Extracted date: 2025-10-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (70, 20)\n",
      "  - Data preservation check: 70 rows maintained\n",
      "Processing: SA_Campaign_List_20251011_20251011_R3iYGZ.xlsx\n",
      "  - Original shape: (58, 33)\n",
      "  - After cleaning: (58, 32)\n",
      "  - Extracted date: 2025-10-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (58, 20)\n",
      "  - Data preservation check: 58 rows maintained\n",
      "Processing: SA_Campaign_List_20251012_20251012_ykWNv4.xlsx\n",
      "  - Original shape: (57, 33)\n",
      "  - After cleaning: (57, 32)\n",
      "  - Extracted date: 2025-10-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (57, 20)\n",
      "  - Data preservation check: 57 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 12 files\n",
      "  - Failed: 0 files\n",
      "Combined 12 files into 870 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 870\n",
      "Total columns: 20\n",
      "Date range: 2025-10-01 00:00:00 to 2025-10-12 00:00:00\n",
      "Unique ASINs: 28\n",
      "Data successfully saved to: Month9_Ads_Data_CA_20251013_124548.xlsx\n",
      "\n",
      "=== Uploading to Google Sheet ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 559\u001b[39m\n\u001b[32m    556\u001b[39m sheet1 = spreadsheet.worksheet(\u001b[33m\"\u001b[39m\u001b[33mRaw_XN_Q4_2025_CA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# Get existing row count\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m existing_rows = \u001b[43mget_existing_row_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExisting rows in sheet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Append new data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 467\u001b[39m, in \u001b[36mget_existing_row_count\u001b[39m\u001b[34m(sheet)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03mGet the number of existing rows in the Google Sheet (excluding header)\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     all_values = \u001b[43msheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Return count minus header row (assuming first row is header)\u001b[39;00m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_values) - \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_values) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\worksheet.py:486\u001b[39m, in \u001b[36mWorksheet.get_all_values\u001b[39m\u001b[34m(self, range_name, major_dimension, value_render_option, date_time_render_option, combine_merged_cells, maintain_size, pad_values, return_type)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_all_values\u001b[39m(\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    476\u001b[39m     range_name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    483\u001b[39m     return_type: GridRangeType = GridRangeType.ListOfLists,\n\u001b[32m    484\u001b[39m ) -> Union[ValueRange, List[List[Any]]]:\n\u001b[32m    485\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias to :meth:`~gspread.worksheet.Worksheet.get_values`\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrange_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrange_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmajor_dimension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmajor_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_render_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_render_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_time_render_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_time_render_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombine_merged_cells\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_merged_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaintain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaintain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\worksheet.py:463\u001b[39m, in \u001b[36mWorksheet.get_values\u001b[39m\u001b[34m(self, range_name, major_dimension, value_render_option, date_time_render_option, combine_merged_cells, maintain_size, pad_values, return_type)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_values\u001b[39m(\n\u001b[32m    447\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    448\u001b[39m     range_name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    455\u001b[39m     return_type: GridRangeType = GridRangeType.ListOfLists,\n\u001b[32m    456\u001b[39m ) -> Union[ValueRange, List[List[Any]]]:\n\u001b[32m    457\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for :meth:`~gspread.worksheet.Worksheet.get`...\u001b[39;00m\n\u001b[32m    458\u001b[39m \n\u001b[32m    459\u001b[39m \u001b[33;03m    with ``return_type`` set to ``List[List[Any]]``\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    and ``pad_values`` set to ``True``\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[33;03m    (legacy method)\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrange_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrange_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmajor_dimension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmajor_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_render_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_render_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_time_render_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_time_render_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombine_merged_cells\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_merged_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaintain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaintain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\worksheet.py:958\u001b[39m, in \u001b[36mWorksheet.get\u001b[39m\u001b[34m(self, range_name, major_dimension, value_render_option, date_time_render_option, combine_merged_cells, maintain_size, pad_values, return_type)\u001b[39m\n\u001b[32m    950\u001b[39m get_range_name = absolute_range_name(\u001b[38;5;28mself\u001b[39m.title, range_name)\n\u001b[32m    952\u001b[39m params: ParamsType = {\n\u001b[32m    953\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmajorDimension\u001b[39m\u001b[33m\"\u001b[39m: major_dimension,\n\u001b[32m    954\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalueRenderOption\u001b[39m\u001b[33m\"\u001b[39m: value_render_option,\n\u001b[32m    955\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdateTimeRenderOption\u001b[39m\u001b[33m\"\u001b[39m: date_time_render_option,\n\u001b[32m    956\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspreadsheet_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_range_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m values = response.get(\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m, [[]])\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pad_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\http_client.py:236\u001b[39m, in \u001b[36mHTTPClient.values_get\u001b[39m\u001b[34m(self, id, range, params)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lower-level method that directly calls `GET spreadsheets/<ID>/values/<range> <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/get>`_.\u001b[39;00m\n\u001b[32m    227\u001b[39m \n\u001b[32m    228\u001b[39m \u001b[33;03m:param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to retrieve.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m \u001b[33;03m.. versionadded:: 3.0\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    235\u001b[39m url = SPREADSHEET_VALUES_URL % (\u001b[38;5;28mid\u001b[39m, quote(\u001b[38;5;28mrange\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\http_client.py:114\u001b[39m, in \u001b[36mHTTPClient.request\u001b[39m\u001b[34m(self, method, endpoint, params, data, json, files, headers)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    106\u001b[39m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    113\u001b[39m ) -> Response:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.ok:\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\auth\\transport\\requests.py:540\u001b[39m, in \u001b[36mAuthorizedSession.request\u001b[39m\u001b[34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[32m    539\u001b[39m     _helpers.request_log(_LOGGER, method, url, data, headers)\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAuthorizedSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m remaining_time = guard.remaining_timeout\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group','ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_CA\", \"Tháng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing Tháng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(\n",
    "            ['Date', 'Sales'], \n",
    "            ascending=[True, False],   # Date ↑, Sales ↓\n",
    "            na_position='last'\n",
    "        ).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data_CA', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    with auto row expansion and safe datetime/NaN handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # Xác định row bắt đầu append\n",
    "        start_row = existing_rows + 2\n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Copy & xử lý dữ liệu\n",
    "        safe_df = new_df.copy()\n",
    "        \n",
    "        # Format cột datetime thành M/D/YYYY\n",
    "        datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "        for col in datetime_cols:\n",
    "            safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "        \n",
    "        # Thay NaN = \"\"\n",
    "        safe_df = safe_df.fillna(\"\")\n",
    "        \n",
    "        # Convert sang list of lists\n",
    "        values = safe_df.values.tolist()\n",
    "        \n",
    "        # 🔥 Đảm bảo sheet có đủ rows\n",
    "        needed_rows = start_row + len(values) - 1\n",
    "        if needed_rows > sheet.row_count:\n",
    "            add_count = needed_rows - sheet.row_count\n",
    "            print(f\"Adding {add_count} new rows to sheet...\")\n",
    "            sheet.add_rows(add_count)\n",
    "        \n",
    "        # Append dữ liệu\n",
    "        sheet.update(f\"A{start_row}\", values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            safe_df = new_df.copy()\n",
    "            datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "            for col in datetime_cols:\n",
    "                safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "            safe_df = safe_df.fillna(\"\")\n",
    "            \n",
    "            set_with_dataframe(sheet, safe_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_CA_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1GpPsWt_fWCfHnEdFQJIsNBebhqFnIiExsHA8SjNUhFk\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q4_2025_CA\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Sessions', 'VAT', 'Shipping']\n",
      "\n",
      "📈 Combining 64 dataframes...\n",
      "✅ Combined data shape: (15148, 21)\n",
      "📅 Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "📤 Uploading to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_21800\\935954928.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded 15148 rows to Google Sheets\n",
      "🔗 Sheet: Raw_SB_H2_2025_US\n",
      "📋 Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "🎉 Successfully processed 64 files:\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 2\n",
      "Total files: 64\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-03 11:28:27\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # Làm sạch tên cột\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Tạo mapping cho các tên cột có thể khác nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # Tìm cột khớp chính xác hoặc gần giống\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khớp chính xác\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khớp một phần cho một số trường hợp đặc biệt\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chỉ giữ lại các cột cần thiết\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # Thêm các cột thiếu với giá trị None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sắp xếp lại theo thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"📋 Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            print(f\"📋 Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618bef3",
   "metadata": {},
   "source": [
    "# SellerBoard Tháng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "📊 Current data rows in sheet: 114\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_01_10_2025-01_10_2025_(2025_10_15_00_11_796).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_02_10_2025-02_10_2025_(2025_10_15_00_11_641).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_03_10_2025-03_10_2025_(2025_10_15_00_11_475).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_04_10_2025-04_10_2025_(2025_10_15_00_11_788).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_05_10_2025-05_10_2025_(2025_10_15_00_11_998).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_06_10_2025-06_10_2025_(2025_10_15_00_11_141).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_07_10_2025-07_10_2025_(2025_10_15_00_11_973).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_08_10_2025-08_10_2025_(2025_10_15_00_12_928).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_09_10_2025-09_10_2025_(2025_10_15_00_12_612).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_10_10_2025-10_10_2025_(2025_10_15_00_12_605).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_11_10_2025-11_10_2025_(2025_10_15_00_12_362).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_15_00_12_804).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_13_10_2025-13_10_2025_(2025_10_15_00_13_449).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_14_10_2025-14_10_2025_(2025_10_15_00_13_462).xlsx\n",
      "📤 Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_9992\\1348838899.py:199: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_9992\\1348838899.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 472 rows to Google Sheets\n",
      "\n",
      "📊 SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 24\n",
      "Standard columns: 21\n",
      "Last run: 2025-10-15 14:14:21\n",
      "\n",
      "September files processed:\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_10_10_2025-10_10_2025_(2025_10_14_20_00_161).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_11_10_2025-11_10_2025_(2025_10_14_20_00_859).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_14_20_00_482).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_13_10_2025-13_10_2025_(2025_10_14_20_00_560).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_14_10_2025-14_10_2025_(2025_10_14_20_00_980).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_01_10_2025-01_10_2025_(2025_10_12_22_42_357).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_02_10_2025-02_10_2025_(2025_10_12_22_42_791).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_03_10_2025-03_10_2025_(2025_10_12_22_43_920).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_04_10_2025-04_10_2025_(2025_10_12_22_43_632).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_12_22_44_465).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_01_10_2025-01_10_2025_(2025_10_15_00_11_796).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_02_10_2025-02_10_2025_(2025_10_15_00_11_641).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_03_10_2025-03_10_2025_(2025_10_15_00_11_475).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_04_10_2025-04_10_2025_(2025_10_15_00_11_788).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_05_10_2025-05_10_2025_(2025_10_15_00_11_998).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_06_10_2025-06_10_2025_(2025_10_15_00_11_141).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_07_10_2025-07_10_2025_(2025_10_15_00_11_973).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_08_10_2025-08_10_2025_(2025_10_15_00_12_928).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_09_10_2025-09_10_2025_(2025_10_15_00_12_612).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_10_10_2025-10_10_2025_(2025_10_15_00_12_605).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_11_10_2025-11_10_2025_(2025_10_15_00_12_362).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_15_00_12_804).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_13_10_2025-13_10_2025_(2025_10_15_00_13_449).xlsx\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_14_10_2025-14_10_2025_(2025_10_15_00_13_462).xlsx\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho tháng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # Thêm các cột thiếu\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sắp xếp đúng thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 10 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chỉ xử lý file tháng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Lấy tất cả giá trị trong cột A để đếm số dòng có dữ liệu\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trừ đi header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"📊 Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Đảm bảo cột theo đúng thứ tự chuẩn\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"ℹ️ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"📤 Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # để Sheets giữ trống\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"✅ Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 10 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  • {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_CA/Tháng 10\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1GpPsWt_fWCfHnEdFQJIsNBebhqFnIiExsHA8SjNUhFk\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_CA\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"🗓️ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"❌ Operation cancelled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
