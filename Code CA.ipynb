{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217392d9",
   "metadata": {},
   "source": [
    "# Monthly Performance (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b14709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing with Data Preservation...\n",
      "============================================================\n",
      "Found 3 folders to process\n",
      "\n",
      "=== Processing Tháng 7 ===\n",
      "Found 31 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 7\n",
      "Processing: SA_Campaign_List_20250701_20250701_CNW4yt.xlsx\n",
      "  - Original shape: (675, 33)\n",
      "  - After cleaning: (675, 32)\n",
      "  - Extracted date: 2025-07-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (675, 32)\n",
      "  - Data preservation check: 675 rows maintained\n",
      "Processing: SA_Campaign_List_20250702_20250702_2akRmQ.xlsx\n",
      "  - Original shape: (551, 33)\n",
      "  - After cleaning: (551, 32)\n",
      "  - Extracted date: 2025-07-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (551, 32)\n",
      "  - Data preservation check: 551 rows maintained\n",
      "Processing: SA_Campaign_List_20250703_20250703_W0ZhJk.xlsx\n",
      "  - Original shape: (532, 33)\n",
      "  - After cleaning: (532, 32)\n",
      "  - Extracted date: 2025-07-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (532, 32)\n",
      "  - Data preservation check: 532 rows maintained\n",
      "Processing: SA_Campaign_List_20250704_20250704_08dyuo.xlsx\n",
      "  - Original shape: (506, 33)\n",
      "  - After cleaning: (506, 32)\n",
      "  - Extracted date: 2025-07-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (506, 32)\n",
      "  - Data preservation check: 506 rows maintained\n",
      "Processing: SA_Campaign_List_20250705_20250705_pKy2B5.xlsx\n",
      "  - Original shape: (504, 33)\n",
      "  - After cleaning: (504, 32)\n",
      "  - Extracted date: 2025-07-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (504, 32)\n",
      "  - Data preservation check: 504 rows maintained\n",
      "Processing: SA_Campaign_List_20250706_20250706_JjWjI1.xlsx\n",
      "  - Original shape: (510, 33)\n",
      "  - After cleaning: (510, 32)\n",
      "  - Extracted date: 2025-07-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (510, 32)\n",
      "  - Data preservation check: 510 rows maintained\n",
      "Processing: SA_Campaign_List_20250707_20250707_c0sH3M.xlsx\n",
      "  - Original shape: (472, 33)\n",
      "  - After cleaning: (472, 32)\n",
      "  - Extracted date: 2025-07-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (472, 32)\n",
      "  - Data preservation check: 472 rows maintained\n",
      "Processing: SA_Campaign_List_20250708_20250708_FI3Lhd.xlsx\n",
      "  - Original shape: (496, 33)\n",
      "  - After cleaning: (496, 32)\n",
      "  - Extracted date: 2025-07-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (496, 32)\n",
      "  - Data preservation check: 496 rows maintained\n",
      "Processing: SA_Campaign_List_20250709_20250709_vtQQFP.xlsx\n",
      "  - Original shape: (475, 33)\n",
      "  - After cleaning: (475, 32)\n",
      "  - Extracted date: 2025-07-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (475, 32)\n",
      "  - Data preservation check: 475 rows maintained\n",
      "Processing: SA_Campaign_List_20250710_20250710_6lHFO5.xlsx\n",
      "  - Original shape: (440, 33)\n",
      "  - After cleaning: (440, 32)\n",
      "  - Extracted date: 2025-07-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (440, 32)\n",
      "  - Data preservation check: 440 rows maintained\n",
      "Processing: SA_Campaign_List_20250711_20250711_mfaJlo.xlsx\n",
      "  - Original shape: (403, 33)\n",
      "  - After cleaning: (403, 32)\n",
      "  - Extracted date: 2025-07-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (403, 32)\n",
      "  - Data preservation check: 403 rows maintained\n",
      "Processing: SA_Campaign_List_20250712_20250712_OPyVWF.xlsx\n",
      "  - Original shape: (379, 33)\n",
      "  - After cleaning: (379, 32)\n",
      "  - Extracted date: 2025-07-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (379, 32)\n",
      "  - Data preservation check: 379 rows maintained\n",
      "Processing: SA_Campaign_List_20250713_20250713_louFD6.xlsx\n",
      "  - Original shape: (398, 33)\n",
      "  - After cleaning: (398, 32)\n",
      "  - Extracted date: 2025-07-13 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (398, 32)\n",
      "  - Data preservation check: 398 rows maintained\n",
      "Processing: SA_Campaign_List_20250714_20250714_aC6bHD.xlsx\n",
      "  - Original shape: (412, 33)\n",
      "  - After cleaning: (412, 32)\n",
      "  - Extracted date: 2025-07-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (412, 32)\n",
      "  - Data preservation check: 412 rows maintained\n",
      "Processing: SA_Campaign_List_20250715_20250715_vPt2jW.xlsx\n",
      "  - Original shape: (538, 33)\n",
      "  - After cleaning: (538, 32)\n",
      "  - Extracted date: 2025-07-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (538, 32)\n",
      "  - Data preservation check: 538 rows maintained\n",
      "Processing: SA_Campaign_List_20250716_20250716_jkzgrH.xlsx\n",
      "  - Original shape: (580, 33)\n",
      "  - After cleaning: (580, 32)\n",
      "  - Extracted date: 2025-07-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (580, 32)\n",
      "  - Data preservation check: 580 rows maintained\n",
      "Processing: SA_Campaign_List_20250717_20250717_zjFW3K.xlsx\n",
      "  - Original shape: (579, 33)\n",
      "  - After cleaning: (579, 32)\n",
      "  - Extracted date: 2025-07-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (579, 32)\n",
      "  - Data preservation check: 579 rows maintained\n",
      "Processing: SA_Campaign_List_20250718_20250718_MVGJ8X.xlsx\n",
      "  - Original shape: (534, 33)\n",
      "  - After cleaning: (534, 32)\n",
      "  - Extracted date: 2025-07-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (534, 32)\n",
      "  - Data preservation check: 534 rows maintained\n",
      "Processing: SA_Campaign_List_20250719_20250719_xKHNWY.xlsx\n",
      "  - Original shape: (521, 33)\n",
      "  - After cleaning: (521, 32)\n",
      "  - Extracted date: 2025-07-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (521, 32)\n",
      "  - Data preservation check: 521 rows maintained\n",
      "Processing: SA_Campaign_List_20250720_20250720_BkGmc5.xlsx\n",
      "  - Original shape: (508, 33)\n",
      "  - After cleaning: (508, 32)\n",
      "  - Extracted date: 2025-07-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (508, 32)\n",
      "  - Data preservation check: 508 rows maintained\n",
      "Processing: SA_Campaign_List_20250721_20250721_vLBvsQ.xlsx\n",
      "  - Original shape: (549, 33)\n",
      "  - After cleaning: (549, 32)\n",
      "  - Extracted date: 2025-07-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (549, 32)\n",
      "  - Data preservation check: 549 rows maintained\n",
      "Processing: SA_Campaign_List_20250722_20250722_LBZesQ.xlsx\n",
      "  - Original shape: (500, 33)\n",
      "  - After cleaning: (500, 32)\n",
      "  - Extracted date: 2025-07-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (500, 32)\n",
      "  - Data preservation check: 500 rows maintained\n",
      "Processing: SA_Campaign_List_20250723_20250723_WXbWly.xlsx\n",
      "  - Original shape: (509, 33)\n",
      "  - After cleaning: (509, 32)\n",
      "  - Extracted date: 2025-07-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (509, 32)\n",
      "  - Data preservation check: 509 rows maintained\n",
      "Processing: SA_Campaign_List_20250724_20250724_1d8wGi.xlsx\n",
      "  - Original shape: (503, 33)\n",
      "  - After cleaning: (503, 32)\n",
      "  - Extracted date: 2025-07-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (503, 32)\n",
      "  - Data preservation check: 503 rows maintained\n",
      "Processing: SA_Campaign_List_20250725_20250725_K1xXeF.xlsx\n",
      "  - Original shape: (486, 33)\n",
      "  - After cleaning: (486, 32)\n",
      "  - Extracted date: 2025-07-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (486, 32)\n",
      "  - Data preservation check: 486 rows maintained\n",
      "Processing: SA_Campaign_List_20250726_20250726_zhI5fA.xlsx\n",
      "  - Original shape: (464, 33)\n",
      "  - After cleaning: (464, 32)\n",
      "  - Extracted date: 2025-07-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (464, 32)\n",
      "  - Data preservation check: 464 rows maintained\n",
      "Processing: SA_Campaign_List_20250727_20250727_sOnDfC.xlsx\n",
      "  - Original shape: (467, 33)\n",
      "  - After cleaning: (467, 32)\n",
      "  - Extracted date: 2025-07-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (467, 32)\n",
      "  - Data preservation check: 467 rows maintained\n",
      "Processing: SA_Campaign_List_20250728_20250728_MKABca.xlsx\n",
      "  - Original shape: (448, 33)\n",
      "  - After cleaning: (448, 32)\n",
      "  - Extracted date: 2025-07-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (448, 32)\n",
      "  - Data preservation check: 448 rows maintained\n",
      "Processing: SA_Campaign_List_20250729_20250729_8hvkPD.xlsx\n",
      "  - Original shape: (439, 33)\n",
      "  - After cleaning: (439, 32)\n",
      "  - Extracted date: 2025-07-29 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (439, 32)\n",
      "  - Data preservation check: 439 rows maintained\n",
      "Processing: SA_Campaign_List_20250730_20250730_xkV24m.xlsx\n",
      "  - Original shape: (428, 33)\n",
      "  - After cleaning: (428, 32)\n",
      "  - Extracted date: 2025-07-30 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (428, 32)\n",
      "  - Data preservation check: 428 rows maintained\n",
      "Processing: SA_Campaign_List_20250731_20250731_k4waFa.xlsx\n",
      "  - Original shape: (413, 33)\n",
      "  - After cleaning: (413, 32)\n",
      "  - Extracted date: 2025-07-31 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (413, 32)\n",
      "  - Data preservation check: 413 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 31 files\n",
      "  - Failed: 0 files\n",
      "Combined 31 files into 15219 total rows\n",
      "\n",
      "=== Processing Tháng 8 ===\n",
      "Found 31 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 8\n",
      "Processing: SA_Campaign_List_20250801_20250801_ODVMwJ.xlsx\n",
      "  - Original shape: (420, 33)\n",
      "  - After cleaning: (420, 32)\n",
      "  - Extracted date: 2025-08-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (420, 32)\n",
      "  - Data preservation check: 420 rows maintained\n",
      "Processing: SA_Campaign_List_20250802_20250802_pdqbqo.xlsx\n",
      "  - Original shape: (422, 33)\n",
      "  - After cleaning: (422, 32)\n",
      "  - Extracted date: 2025-08-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (422, 32)\n",
      "  - Data preservation check: 422 rows maintained\n",
      "Processing: SA_Campaign_List_20250803_20250803_757eu7.xlsx\n",
      "  - Original shape: (423, 33)\n",
      "  - After cleaning: (423, 32)\n",
      "  - Extracted date: 2025-08-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (423, 32)\n",
      "  - Data preservation check: 423 rows maintained\n",
      "Processing: SA_Campaign_List_20250804_20250804_v9LHCg.xlsx\n",
      "  - Original shape: (383, 33)\n",
      "  - After cleaning: (383, 32)\n",
      "  - Extracted date: 2025-08-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (383, 32)\n",
      "  - Data preservation check: 383 rows maintained\n",
      "Processing: SA_Campaign_List_20250805_20250805_UTTEFl.xlsx\n",
      "  - Original shape: (379, 33)\n",
      "  - After cleaning: (379, 32)\n",
      "  - Extracted date: 2025-08-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (379, 32)\n",
      "  - Data preservation check: 379 rows maintained\n",
      "Processing: SA_Campaign_List_20250806_20250806_tt1UAD.xlsx\n",
      "  - Original shape: (359, 33)\n",
      "  - After cleaning: (359, 32)\n",
      "  - Extracted date: 2025-08-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (359, 32)\n",
      "  - Data preservation check: 359 rows maintained\n",
      "Processing: SA_Campaign_List_20250807_20250807_83DLWZ.xlsx\n",
      "  - Original shape: (354, 33)\n",
      "  - After cleaning: (354, 32)\n",
      "  - Extracted date: 2025-08-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (354, 32)\n",
      "  - Data preservation check: 354 rows maintained\n",
      "Processing: SA_Campaign_List_20250808_20250808_7jZ5uD.xlsx\n",
      "  - Original shape: (350, 33)\n",
      "  - After cleaning: (350, 32)\n",
      "  - Extracted date: 2025-08-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (350, 32)\n",
      "  - Data preservation check: 350 rows maintained\n",
      "Processing: SA_Campaign_List_20250809_20250809_V6OrOz.xlsx\n",
      "  - Original shape: (350, 33)\n",
      "  - After cleaning: (350, 32)\n",
      "  - Extracted date: 2025-08-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (350, 32)\n",
      "  - Data preservation check: 350 rows maintained\n",
      "Processing: SA_Campaign_List_20250810_20250810_ziMZFP.xlsx\n",
      "  - Original shape: (375, 33)\n",
      "  - After cleaning: (375, 32)\n",
      "  - Extracted date: 2025-08-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (375, 32)\n",
      "  - Data preservation check: 375 rows maintained\n",
      "Processing: SA_Campaign_List_20250811_20250811_H6LBYK.xlsx\n",
      "  - Original shape: (375, 33)\n",
      "  - After cleaning: (375, 32)\n",
      "  - Extracted date: 2025-08-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (375, 32)\n",
      "  - Data preservation check: 375 rows maintained\n",
      "Processing: SA_Campaign_List_20250812_20250812_pqVwu9.xlsx\n",
      "  - Original shape: (389, 33)\n",
      "  - After cleaning: (389, 32)\n",
      "  - Extracted date: 2025-08-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (389, 32)\n",
      "  - Data preservation check: 389 rows maintained\n",
      "Processing: SA_Campaign_List_20250813_20250813_83tOqn.xlsx\n",
      "  - Original shape: (369, 33)\n",
      "  - After cleaning: (369, 32)\n",
      "  - Extracted date: 2025-08-13 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (369, 32)\n",
      "  - Data preservation check: 369 rows maintained\n",
      "Processing: SA_Campaign_List_20250814_20250814_MTGB0G.xlsx\n",
      "  - Original shape: (387, 33)\n",
      "  - After cleaning: (387, 32)\n",
      "  - Extracted date: 2025-08-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (387, 32)\n",
      "  - Data preservation check: 387 rows maintained\n",
      "Processing: SA_Campaign_List_20250815_20250815_MEnGN8.xlsx\n",
      "  - Original shape: (386, 33)\n",
      "  - After cleaning: (386, 32)\n",
      "  - Extracted date: 2025-08-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (386, 32)\n",
      "  - Data preservation check: 386 rows maintained\n",
      "Processing: SA_Campaign_List_20250816_20250816_xNyU1X.xlsx\n",
      "  - Original shape: (376, 33)\n",
      "  - After cleaning: (376, 32)\n",
      "  - Extracted date: 2025-08-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (376, 32)\n",
      "  - Data preservation check: 376 rows maintained\n",
      "Processing: SA_Campaign_List_20250817_20250817_LSkH2D.xlsx\n",
      "  - Original shape: (383, 33)\n",
      "  - After cleaning: (383, 32)\n",
      "  - Extracted date: 2025-08-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (383, 32)\n",
      "  - Data preservation check: 383 rows maintained\n",
      "Processing: SA_Campaign_List_20250818_20250818_KJOIni.xlsx\n",
      "  - Original shape: (343, 33)\n",
      "  - After cleaning: (343, 32)\n",
      "  - Extracted date: 2025-08-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (343, 32)\n",
      "  - Data preservation check: 343 rows maintained\n",
      "Processing: SA_Campaign_List_20250819_20250819_BjSKl0.xlsx\n",
      "  - Original shape: (344, 33)\n",
      "  - After cleaning: (344, 32)\n",
      "  - Extracted date: 2025-08-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (344, 32)\n",
      "  - Data preservation check: 344 rows maintained\n",
      "Processing: SA_Campaign_List_20250820_20250820_bUIZ5d.xlsx\n",
      "  - Original shape: (331, 33)\n",
      "  - After cleaning: (331, 32)\n",
      "  - Extracted date: 2025-08-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (331, 32)\n",
      "  - Data preservation check: 331 rows maintained\n",
      "Processing: SA_Campaign_List_20250821_20250821_VAhWXA.xlsx\n",
      "  - Original shape: (339, 33)\n",
      "  - After cleaning: (339, 32)\n",
      "  - Extracted date: 2025-08-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (339, 32)\n",
      "  - Data preservation check: 339 rows maintained\n",
      "Processing: SA_Campaign_List_20250822_20250822_xbWhJE.xlsx\n",
      "  - Original shape: (331, 33)\n",
      "  - After cleaning: (331, 32)\n",
      "  - Extracted date: 2025-08-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (331, 32)\n",
      "  - Data preservation check: 331 rows maintained\n",
      "Processing: SA_Campaign_List_20250823_20250823_3Sr0fy.xlsx\n",
      "  - Original shape: (326, 33)\n",
      "  - After cleaning: (326, 32)\n",
      "  - Extracted date: 2025-08-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (326, 32)\n",
      "  - Data preservation check: 326 rows maintained\n",
      "Processing: SA_Campaign_List_20250824_20250824_lko04t.xlsx\n",
      "  - Original shape: (325, 33)\n",
      "  - After cleaning: (325, 32)\n",
      "  - Extracted date: 2025-08-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (325, 32)\n",
      "  - Data preservation check: 325 rows maintained\n",
      "Processing: SA_Campaign_List_20250825_20250825_pbuxXB.xlsx\n",
      "  - Original shape: (318, 33)\n",
      "  - After cleaning: (318, 32)\n",
      "  - Extracted date: 2025-08-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (318, 32)\n",
      "  - Data preservation check: 318 rows maintained\n",
      "Processing: SA_Campaign_List_20250826_20250826_oz83uE.xlsx\n",
      "  - Original shape: (317, 33)\n",
      "  - After cleaning: (317, 32)\n",
      "  - Extracted date: 2025-08-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (317, 32)\n",
      "  - Data preservation check: 317 rows maintained\n",
      "Processing: SA_Campaign_List_20250827_20250827_6nNdV8.xlsx\n",
      "  - Original shape: (308, 33)\n",
      "  - After cleaning: (308, 32)\n",
      "  - Extracted date: 2025-08-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (308, 32)\n",
      "  - Data preservation check: 308 rows maintained\n",
      "Processing: SA_Campaign_List_20250828_20250828_WQrZWL.xlsx\n",
      "  - Original shape: (299, 33)\n",
      "  - After cleaning: (299, 32)\n",
      "  - Extracted date: 2025-08-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (299, 32)\n",
      "  - Data preservation check: 299 rows maintained\n",
      "Processing: SA_Campaign_List_20250829_20250829_KDJ6s3.xlsx\n",
      "  - Original shape: (274, 33)\n",
      "  - After cleaning: (274, 32)\n",
      "  - Extracted date: 2025-08-29 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (274, 32)\n",
      "  - Data preservation check: 274 rows maintained\n",
      "Processing: SA_Campaign_List_20250830_20250830_oOIRl4.xlsx\n",
      "  - Original shape: (270, 33)\n",
      "  - After cleaning: (270, 32)\n",
      "  - Extracted date: 2025-08-30 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (270, 32)\n",
      "  - Data preservation check: 270 rows maintained\n",
      "Processing: SA_Campaign_List_20250831_20250831_bJrF9h.xlsx\n",
      "  - Original shape: (268, 33)\n",
      "  - After cleaning: (268, 32)\n",
      "  - Extracted date: 2025-08-31 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (268, 32)\n",
      "  - Data preservation check: 268 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 31 files\n",
      "  - Failed: 0 files\n",
      "Combined 31 files into 10873 total rows\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 2 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 9\n",
      "Processing: SA_Campaign_List_20250901_20250901_Cugr13.xlsx\n",
      "  - Original shape: (272, 33)\n",
      "  - After cleaning: (272, 32)\n",
      "  - Extracted date: 2025-09-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (272, 32)\n",
      "  - Data preservation check: 272 rows maintained\n",
      "Processing: SA_Campaign_List_20250902_20250902_i4tLnT.xlsx\n",
      "  - Original shape: (273, 33)\n",
      "  - After cleaning: (273, 32)\n",
      "  - Extracted date: 2025-09-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (273, 32)\n",
      "  - Data preservation check: 273 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 2 files\n",
      "  - Failed: 0 files\n",
      "Combined 2 files into 545 total rows\n",
      "\n",
      "=== Combining Data from All Folders ===\n",
      "Successfully combined data from 3 folders\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 26637\n",
      "Total columns: 32\n",
      "  - Tháng 7: 15219 rows\n",
      "  - Tháng 8: 10873 rows\n",
      "  - Tháng 9: 545 rows\n",
      "Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "Unique ASINs: 143\n",
      "Data successfully saved to: Combined_Ads_Data_Safe_20250903_121643.xlsx\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      ASIN       Date Campaign type                                                                Campaign Status Country                                           Portfolio  Daily Budget      Bidding Strategy  Top-of-search IS  Avg.time in Budget  Impressions  Clicks    CTR  Spend  CPC  Orders  Sales  Units  CVR    ACOS  ROAS  CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU Orders Other SKU  Units Same SKU Units Other SKU Extra_Target type  Extra_Current Budget Extra_SP Off-site Ads Strategy\n",
      "B089QFYLFB 2025-07-01            SP       B089QFYLFB_12oz_Fifty fabulous rose_women 50th birthday gifts_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN          10.0 Dynamic bids and down            0.0704                 100          553       2 0.0036   5.82 2.91       1  19.98      1  0.5  0.2913  3.43 5.82           19.98              0.0                1               --               1              --            manual                   0.0                        NOT_SET\n",
      "B089QFYLFB 2025-07-01            SP B089QFYLFB_12oz_Fifty fabulous rose_happy 50th birthday decorations_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN           5.0 Dynamic bids and down            0.0117                 100           25       1 0.0400   2.20 2.20       0   0.00      0  0.0      --  0.00 0.00            0.00              0.0                0               --               0              --            manual                   0.0                        NOT_SET\n",
      "B089QFYLFB 2025-07-01            SP  B089QFYLFB_12oz_Fifty fabulous rose_fifty birthday gifts for women_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN           5.0 Dynamic bids and down            0.0882                 100            6       0 0.0000   0.00  NaN       0   0.00      0  NaN      --   NaN 0.00            0.00              0.0                0               --               0              --            manual                   0.0                        NOT_SET\n",
      "\n",
      "=== Column List ===\n",
      " 1. ASIN\n",
      " 2. Date\n",
      " 3. Campaign type\n",
      " 4. Campaign\n",
      " 5. Status\n",
      " 6. Country\n",
      " 7. Portfolio\n",
      " 8. Daily Budget\n",
      " 9. Bidding Strategy\n",
      "10. Top-of-search IS\n",
      "11. Avg.time in Budget\n",
      "12. Impressions\n",
      "13. Clicks\n",
      "14. CTR\n",
      "15. Spend\n",
      "16. CPC\n",
      "17. Orders\n",
      "18. Sales\n",
      "19. Units\n",
      "20. CVR\n",
      "21. ACOS\n",
      "22. ROAS\n",
      "23. CPA\n",
      "24. Sales Same SKU\n",
      "25. Sales Other SKU\n",
      "26. Orders Same SKU\n",
      "27. Orders Other SKU\n",
      "28. Units Same SKU\n",
      "29. Units Other SKU\n",
      "30. Extra_Target type\n",
      "31. Extra_Current Budget\n",
      "32. Extra_SP Off-site Ads Strategy\n",
      "\n",
      "============================================================\n",
      "Processing completed with data preservation safeguards!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS', 'CPA', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU', 'Orders Same SKU', 'Units Same SKU', \n",
    "                      'Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU'\n",
    "        ]\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # Add any additional columns that weren't in the required list (preserve extra data)\n",
    "        extra_columns = [col for col in df.columns if col not in required_columns and col not in ['ASIN', 'Date']]\n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process Ads folders with data preservation\n",
    "    \"\"\"\n",
    "    # Define folder paths - FIXED: Variable naming issue\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m7_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 7\")\n",
    "    ads_m8_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 8\")\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 9\")  # FIXED: Was overwriting ads_m8_path\n",
    "    \n",
    "    # Check if folders exist and prepare processing list\n",
    "    folders_to_process = []\n",
    "    \n",
    "    if os.path.exists(ads_m7_path):\n",
    "        folders_to_process.append((\"Tháng 7\", ads_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(ads_m8_path):\n",
    "        folders_to_process.append((\"Tháng 8\", ads_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m8_path} not found\")\n",
    "        \n",
    "    if os.path.exists(ads_m9_path):\n",
    "        folders_to_process.append((\"Tháng 9\", ads_m9_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(folders_to_process)} folders to process\")\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    processing_summary = []\n",
    "    \n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "            processing_summary.append(f\"{folder_name}: {len(df)} rows\")\n",
    "        else:\n",
    "            processing_summary.append(f\"{folder_name}: No data\")\n",
    "    \n",
    "    # Combine all data from folders\n",
    "    if all_dataframes:\n",
    "        print(f\"\\n=== Combining Data from All Folders ===\")\n",
    "        try:\n",
    "            final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully combined data from {len(all_dataframes)} folders\")\n",
    "            \n",
    "            # Safe duplicate removal (only if key columns exist)\n",
    "            if 'ASIN' in final_df.columns and 'Campaign' in final_df.columns and 'Date' in final_df.columns:\n",
    "                original_rows = len(final_df)\n",
    "                final_df = final_df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "                removed_duplicates = original_rows - len(final_df)\n",
    "                print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "            \n",
    "            # Safe sorting\n",
    "            try:\n",
    "                final_df = final_df.sort_values(['ASIN', 'Date'], na_position='last')\n",
    "                final_df = final_df.reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error sorting data: {e}\")\n",
    "            \n",
    "            print(f\"\\n=== Final Results ===\")\n",
    "            print(f\"Total rows: {len(final_df)}\")\n",
    "            print(f\"Total columns: {len(final_df.columns)}\")\n",
    "            \n",
    "            for summary in processing_summary:\n",
    "                print(f\"  - {summary}\")\n",
    "            \n",
    "            if 'Date' in final_df.columns:\n",
    "                try:\n",
    "                    date_min = final_df['Date'].min()\n",
    "                    date_max = final_df['Date'].max()\n",
    "                    print(f\"Date range: {date_min} to {date_max}\")\n",
    "                except:\n",
    "                    print(\"Date range: Unable to determine\")\n",
    "            \n",
    "            if 'ASIN' in final_df.columns:\n",
    "                try:\n",
    "                    unique_asins = final_df['ASIN'].nunique()\n",
    "                    print(f\"Unique ASINs: {unique_asins}\")\n",
    "                except:\n",
    "                    print(\"Unique ASINs: Unable to determine\")\n",
    "            \n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error combining data: {e}\")\n",
    "            # Return the largest DataFrame as fallback\n",
    "            if all_dataframes:\n",
    "                largest_df = max(all_dataframes, key=len)\n",
    "                print(f\"Returning largest DataFrame with {len(largest_df)} rows as fallback\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(\"No data to process.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Keep all the other utility functions unchanged\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Combined_Ads_Data', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def display_sample(df, rows=3):\n",
    "    \"\"\"Display sample data with proper formatting\"\"\"\n",
    "    try:\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 20)\n",
    "        print(df.head(rows).to_string(index=False))\n",
    "        pd.reset_option('display.max_columns')\n",
    "        pd.reset_option('display.width')\n",
    "        pd.reset_option('display.max_colwidth')\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying sample: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    print(\"Starting Amazon Ads Data Processing with Data Preservation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result_df = main()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save combined data\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Combined_Ads_Data_Safe_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            # Display sample\n",
    "            print(f\"\\nSample data (first 3 rows):\")\n",
    "            display_sample(result_df)\n",
    "            \n",
    "            print(f\"\\n=== Column List ===\")\n",
    "            for i, col in enumerate(result_df.columns, 1):\n",
    "                print(f\"{i:2d}. {col}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Processing completed with data preservation safeguards!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_XN_Q3_2025_US\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd56f6",
   "metadata": {},
   "source": [
    "# Append Selected Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f02d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_CA\\Tháng 9\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 9 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_CA\\Tháng 9\n",
      "Processing: SA_Campaign_List_20250901_20250901_Puk9bN.xlsx\n",
      "  - Original shape: (41, 33)\n",
      "  - After cleaning: (41, 32)\n",
      "  - Extracted date: 2025-09-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (41, 29)\n",
      "  - Data preservation check: 41 rows maintained\n",
      "Processing: SA_Campaign_List_20250902_20250902_c6f3TZ.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Original shape: (41, 33)\n",
      "  - After cleaning: (41, 32)\n",
      "  - Extracted date: 2025-09-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (41, 29)\n",
      "  - Data preservation check: 41 rows maintained\n",
      "Processing: SA_Campaign_List_20250903_20250903_fTfLbo.xlsx\n",
      "  - Original shape: (41, 33)\n",
      "  - After cleaning: (41, 32)\n",
      "  - Extracted date: 2025-09-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (41, 29)\n",
      "  - Data preservation check: 41 rows maintained\n",
      "Processing: SA_Campaign_List_20250904_20250904_DMIVSS.xlsx\n",
      "  - Original shape: (44, 33)\n",
      "  - After cleaning: (44, 32)\n",
      "  - Extracted date: 2025-09-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (44, 29)\n",
      "  - Data preservation check: 44 rows maintained\n",
      "Processing: SA_Campaign_List_20250905_20250905_zTVogP.xlsx\n",
      "  - Original shape: (47, 33)\n",
      "  - After cleaning: (47, 32)\n",
      "  - Extracted date: 2025-09-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (47, 29)\n",
      "  - Data preservation check: 47 rows maintained\n",
      "Processing: SA_Campaign_List_20250906_20250906_py6NRe.xlsx\n",
      "  - Original shape: (45, 33)\n",
      "  - After cleaning: (45, 32)\n",
      "  - Extracted date: 2025-09-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (45, 29)\n",
      "  - Data preservation check: 45 rows maintained\n",
      "Processing: SA_Campaign_List_20250907_20250907_8iMuX2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Original shape: (45, 33)\n",
      "  - After cleaning: (45, 32)\n",
      "  - Extracted date: 2025-09-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (45, 29)\n",
      "  - Data preservation check: 45 rows maintained\n",
      "Processing: SA_Campaign_List_20250908_20250908_XMoAbg.xlsx\n",
      "  - Original shape: (46, 33)\n",
      "  - After cleaning: (46, 32)\n",
      "  - Extracted date: 2025-09-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (46, 29)\n",
      "  - Data preservation check: 46 rows maintained\n",
      "Processing: SA_Campaign_List_20250909_20250909_5FdPCK.xlsx\n",
      "  - Original shape: (51, 33)\n",
      "  - After cleaning: (51, 32)\n",
      "  - Extracted date: 2025-09-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: CPC - too many conversion failures, keeping original\n",
      "Warning: CVR - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: ROAS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (51, 29)\n",
      "  - Data preservation check: 51 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 9 files\n",
      "  - Failed: 0 files\n",
      "Combined 9 files into 401 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 401\n",
      "Total columns: 29\n",
      "Date range: 2025-09-01 00:00:00 to 2025-09-09 00:00:00\n",
      "Unique ASINs: 14\n",
      "Data successfully saved to: Month9_Ads_Data_20250910_142526.xlsx\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      ASIN       Date Campaign type                                                                                                     Campaign     Status Country                                      Portfolio  Daily Budget       Bidding Strategy  Top-of-search IS  Avg.time in Budget  Impressions  Clicks    CTR  Spend   CPC  Orders  Sales  Units CVR    ACOS    ROAS   CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU Orders Other SKU  Units Same SKU Units Other SKU\n",
      "B08R8PXYL6 2025-09-01            SP                                                               B08R8PXYL6_12oz_Sixty fabulous rose_auto 17h06 Delivering      CA B08R8PXYL6_TUMBLER 12_SIXTY FABULOUS ROSE GOLD           5.0 Dynamic bids down only            0.0000                 100          317       1 0.0032   0.03  0.03       1  24.98      1   1  0.0012  832.67  0.03           24.98              0.0                1               --               1              --\n",
      "B08R8PXYL6 2025-09-01            SP SP_B08R8PXYL6_Tumbler 12_Sixty Fabulous Rose Gold_Manual_KW_60th birthday decorations for women_ex_Up & Down Delivering      CA B08R8PXYL6_TUMBLER 12_SIXTY FABULOUS ROSE GOLD           5.0  Dynamic bids and down            0.0000                 100            4       0 0.0000   0.00    --       0   0.00      0  --      --      --    --            0.00              0.0                0               --               0              --\n",
      "B08R8PXYL6 2025-09-01            SP                                                                 B08R8PXYL6_12oz_Sixty fabulous_all key 16h31 Delivering      CA B08R8PXYL6_TUMBLER 12_SIXTY FABULOUS ROSE GOLD           5.0 Dynamic bids down only            0.1875                 100          367       0 0.0000   0.00    --       0   0.00      0  --      --      --    --            0.00              0.0                0               --               0              --\n",
      "\n",
      "=== Column List ===\n",
      " 1. ASIN\n",
      " 2. Date\n",
      " 3. Campaign type\n",
      " 4. Campaign\n",
      " 5. Status\n",
      " 6. Country\n",
      " 7. Portfolio\n",
      " 8. Daily Budget\n",
      " 9. Bidding Strategy\n",
      "10. Top-of-search IS\n",
      "11. Avg.time in Budget\n",
      "12. Impressions\n",
      "13. Clicks\n",
      "14. CTR\n",
      "15. Spend\n",
      "16. CPC\n",
      "17. Orders\n",
      "18. Sales\n",
      "19. Units\n",
      "20. CVR\n",
      "21. ACOS\n",
      "22. ROAS\n",
      "23. CPA\n",
      "24. Sales Same SKU\n",
      "25. Sales Other SKU\n",
      "26. Orders Same SKU\n",
      "27. Orders Other SKU\n",
      "28. Units Same SKU\n",
      "29. Units Other SKU\n",
      "\n",
      "=== Uploading to Google Sheet ===\n",
      "Existing rows in sheet: 3674\n",
      "Appending 401 rows starting from row 3676\n",
      "Updating range: A3676:]4076\n",
      "Error appending to Google Sheet: Object of type Timestamp is not JSON serializable\n",
      "Trying fallback method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\4164627072.py:509: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(range_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback append completed\n",
      "Google Sheet update completed successfully!\n",
      "\n",
      "============================================================\n",
      "Month 9 processing completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS', 'CPA', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU', 'Orders Same SKU', 'Units Same SKU', \n",
    "                      'Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_CA\", \"Tháng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing Tháng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(['ASIN', 'Date'], na_position='last')\n",
    "        df = df.reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def display_sample(df, rows=3):\n",
    "    \"\"\"Display sample data with proper formatting\"\"\"\n",
    "    try:\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 20)\n",
    "        print(df.head(rows).to_string(index=False))\n",
    "        pd.reset_option('display.max_columns')\n",
    "        pd.reset_option('display.width')\n",
    "        pd.reset_option('display.max_colwidth')\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying sample: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # Calculate the starting row for appending (existing_rows + 2 to account for header)\n",
    "        start_row = existing_rows + 2\n",
    "        \n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Convert DataFrame to list of lists for batch update\n",
    "        values = new_df.values.tolist()\n",
    "        \n",
    "        # Prepare the range for batch update\n",
    "        end_col_letter = chr(ord('A') + len(new_df.columns) - 1)  # Convert to column letter\n",
    "        range_name = f\"A{start_row}:{end_col_letter}{start_row + len(values) - 1}\"\n",
    "        \n",
    "        print(f\"Updating range: {range_name}\")\n",
    "        \n",
    "        # Batch update the sheet\n",
    "        sheet.update(range_name, values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        # Fallback: try using set_with_dataframe but with an offset\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            # This will overwrite, but we'll note the issue\n",
    "            set_with_dataframe(sheet, new_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            # Display sample\n",
    "            print(f\"\\nSample data (first 3 rows):\")\n",
    "            display_sample(result_df)\n",
    "            \n",
    "            print(f\"\\n=== Column List ===\")\n",
    "            for i, col in enumerate(result_df.columns, 1):\n",
    "                print(f\"{i:2d}. {col}\")\n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q3_2025_CA\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Sessions', 'VAT', 'Shipping']\n",
      "\n",
      "📈 Combining 64 dataframes...\n",
      "✅ Combined data shape: (15148, 21)\n",
      "📅 Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "📤 Uploading to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_21800\\935954928.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded 15148 rows to Google Sheets\n",
      "🔗 Sheet: Raw_SB_H2_2025_US\n",
      "📋 Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "🎉 Successfully processed 64 files:\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 2\n",
      "Total files: 64\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-03 11:28:27\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # Làm sạch tên cột\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Tạo mapping cho các tên cột có thể khác nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # Tìm cột khớp chính xác hoặc gần giống\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khớp chính xác\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khớp một phần cho một số trường hợp đặc biệt\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chỉ giữ lại các cột cần thiết\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # Thêm các cột thiếu với giá trị None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sắp xếp lại theo thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"📋 Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            print(f\"📋 Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618bef3",
   "metadata": {},
   "source": [
    "# SellerBoard Tháng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "📊 Current data rows in sheet: 1912\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(00_20_30_218).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(00_20_43_104).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(00_20_55_443).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(00_21_08_726).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(00_21_20_133).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(00_21_33_587).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(00_21_45_630).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(00_21_57_464).xlsx\n",
      "➕ New September file detected: NewEleven_Dashboard Products Group by ASIN_09_09_2025-09_09_2025_(00_22_07_850).xlsx\n",
      "📤 Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\2261602916.py:199: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23940\\2261602916.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 249 rows to Google Sheets\n",
      "\n",
      "📊 SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 9\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-10 14:25:44\n",
      "\n",
      "September files processed:\n",
      "  • NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(00_20_30_218).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(00_20_43_104).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(00_20_55_443).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(00_21_08_726).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(00_21_20_133).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(00_21_33_587).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(00_21_45_630).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(00_21_57_464).xlsx\n",
      "  • NewEleven_Dashboard Products Group by ASIN_09_09_2025-09_09_2025_(00_22_07_850).xlsx\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho tháng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # Thêm các cột thiếu\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sắp xếp đúng thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 9 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chỉ xử lý file tháng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Lấy tất cả giá trị trong cột A để đếm số dòng có dữ liệu\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trừ đi header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"📊 Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Đảm bảo cột theo đúng thứ tự chuẩn\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"ℹ️ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"📤 Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # để Sheets giữ trống\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"✅ Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 9 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  • {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_CA\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_CA\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"🗓️ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"❌ Operation cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eedbdb7",
   "metadata": {},
   "source": [
    "# XNurta H2 2024 (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f100f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode for XNurta 2024 Q3 data:\n",
      "1. Initial run (reprocess all Q3 2024 files: Jul-Sep)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing Q3 2024 XNurta files (Jul-Sep)\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "🗑️ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "============================================================\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "📊 Processing: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "🔄 Initial run: Processing Q3 2024 file SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "📊 Processing: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "   🗑️ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "✅ Successfully processed: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "\n",
      "📈 Combining 92 dataframes...\n",
      "✅ Combined data shape: (41239, 62)\n",
      "📅 Date range: 2024-07-01 00:00:00 to 2024-09-30 00:00:00\n",
      "📤 Uploading to Google Sheets...\n",
      "✅ Successfully uploaded 41239 rows to Google Sheets\n",
      "🔗 Sheet: Raw_XNurta_H2_2024\n",
      "\n",
      "🎉 Successfully processed 92 files:\n",
      "   ✓ SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "   ✓ SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "   ✓ SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "   ✓ SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "   ✓ SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "   ✓ SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "   ✓ SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "   ✓ SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "   ✓ SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "   ✓ SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "   ✓ SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "   ✓ SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "   ✓ SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "   ✓ SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "   ✓ SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "   ✓ SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "   ✓ SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "   ✓ SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "   ✓ SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "   ✓ SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "   ✓ SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "   ✓ SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "   ✓ SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "   ✓ SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "   ✓ SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "   ✓ SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "   ✓ SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "   ✓ SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "   ✓ SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "   ✓ SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "   ✓ SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "   ✓ SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "   ✓ SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "   ✓ SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "   ✓ SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "   ✓ SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "   ✓ SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "   ✓ SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "   ✓ SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "   ✓ SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "   ✓ SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "   ✓ SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "   ✓ SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "   ✓ SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "   ✓ SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "   ✓ SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "   ✓ SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "   ✓ SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "   ✓ SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "   ✓ SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "   ✓ SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "   ✓ SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "   ✓ SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "   ✓ SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "   ✓ SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "   ✓ SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "   ✓ SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "   ✓ SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "   ✓ SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "   ✓ SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "   ✓ SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "   ✓ SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "   ✓ SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "   ✓ SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "   ✓ SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "   ✓ SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "   ✓ SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "   ✓ SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "   ✓ SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "   ✓ SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "   ✓ SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "   ✓ SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "   ✓ SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "   ✓ SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "   ✓ SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "   ✓ SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "   ✓ SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "   ✓ SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "   ✓ SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "   ✓ SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "   ✓ SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "   ✓ SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "   ✓ SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "   ✓ SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "   ✓ SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "   ✓ SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "   ✓ SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "   ✓ SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "   ✓ SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "   ✓ SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "   ✓ SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "   ✓ SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY - XNurta 2024 Q3\n",
      "=======================================\n",
      "July files: 31\n",
      "August files: 31\n",
      "September files: 30\n",
      "Other files: 0\n",
      "Total files: 184\n",
      "Last run: 2025-08-25 17:33:05\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "class XNurtaDataProcessor2024:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"xnurta_file_metadata_2024.json\"\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "        \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "        Returns the first date (start date)\n",
    "        \"\"\"\n",
    "        pattern = r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx'\n",
    "        match = re.search(pattern, os.path.basename(filename))\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "        return None\n",
    "        \n",
    "    def clean_currency_column(self, column):\n",
    "        \"\"\"Remove $ symbol and convert to float\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN'], np.nan)\n",
    "            return pd.to_numeric(cleaned, errors='coerce')\n",
    "        return column\n",
    "        \n",
    "    def convert_to_float(self, column):\n",
    "        \"\"\"Convert object columns to float\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            return pd.to_numeric(cleaned, errors='coerce')\n",
    "        return column\n",
    "        \n",
    "    def convert_to_int(self, column):\n",
    "        \"\"\"Convert object columns to int\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        return column\n",
    "        \n",
    "    def extract_asin_from_portfolio(self, portfolio_str):\n",
    "        \"\"\"Extract ASIN from Portfolio string\"\"\"\n",
    "        if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "            return None\n",
    "        \n",
    "        portfolio_str = str(portfolio_str)\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str)\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: 10 alphanumeric characters starting with letter\n",
    "        pattern2 = r'[A-Z][A-Z0-9]{9}'\n",
    "        match2 = re.search(pattern2, portfolio_str)\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "        pattern3 = r'[A-Z0-9]{10}'\n",
    "        match3 = re.search(pattern3, portfolio_str)\n",
    "        if match3:\n",
    "            return match3.group()\n",
    "        \n",
    "        # Pattern 4: 10 alphanumeric with possible lowercase (convert to uppercase)\n",
    "        pattern4 = r'[A-Za-z0-9]{10}'\n",
    "        match4 = re.search(pattern4, portfolio_str)\n",
    "        if match4:\n",
    "            return match4.group().upper()\n",
    "        \n",
    "        # If no pattern matches, return first 10 characters as fallback\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        return portfolio_str[:10] if len(portfolio_str) >= 10 else portfolio_str\n",
    "        \n",
    "    def normalize_campaign_types(self, text):\n",
    "        \"\"\"Normalize campaign type keywords\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file according to specifications\"\"\"\n",
    "        try:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_extracted = self.extract_date_from_filename(file_path)\n",
    "            \n",
    "            # Drop specified columns if they exist\n",
    "            columns_to_drop = [\n",
    "                'Profile', \n",
    "                'Labels', \n",
    "                'Budget group',\n",
    "                'Status',\n",
    "                'Current Budget',\n",
    "                'SP Off-site Ads Strategy',\n",
    "                'Bidding Strategy',\n",
    "                'Sales Same SKU',\n",
    "                'Sales Other SKU',\n",
    "                'Orders Same SKU',\n",
    "                'Orders Other SKU',\n",
    "                'Units Same SKU',\n",
    "                'Units Other SKU'\n",
    "            ]\n",
    "            existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "            if existing_columns_to_drop:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"   🗑️ Dropped columns: {', '.join(existing_columns_to_drop)}\")\n",
    "            \n",
    "            # Add ASIN column as first column (extract ASIN from Portfolio)\n",
    "            if 'Portfolio' in df.columns:\n",
    "                df.insert(0, 'ASIN', df['Portfolio'].apply(self.extract_asin_from_portfolio))\n",
    "            \n",
    "            # Add Date column\n",
    "            df.insert(1, 'Date', date_extracted)\n",
    "            \n",
    "            # Normalize campaign types in Campaign Type column\n",
    "            if 'Campaign type' in df.columns:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(self.normalize_campaign_types)\n",
    "            \n",
    "            # Clean currency columns\n",
    "            currency_columns = ['Daily Budget']\n",
    "            for col in currency_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = self.clean_currency_column(df[col])\n",
    "            \n",
    "            # Convert specified columns to float\n",
    "            float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS']\n",
    "            for col in float_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = self.convert_to_float(df[col])\n",
    "            \n",
    "            # Note: Removed int_columns conversion since those columns are now dropped\n",
    "            \n",
    "            print(f\"✅ Successfully processed: {os.path.basename(file_path)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    def _is_h2_2024_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to Q3 2024 (July to September 2024)\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2024\n",
    "        \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all H2 2024 files\n",
    "        if is_initial_run:\n",
    "            if self._is_h2_2024_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing Q3 2024 file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "        \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "        \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"Main processing function for XNurta 2024 data\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing Q3 2024 XNurta files (Jul-Sep)\")\n",
    "            # Clear existing Q3 2024 metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = pd.to_datetime(metadata['date'])\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_h2_2024_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for Q3 2024 file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders (Tháng 7, Tháng 8, etc.)\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by Date and ASIN\n",
    "            if \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "            \n",
    "            # Reset index\n",
    "            master_df = master_df.reset_index(drop=True)\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear limited columns range (A to AZ) instead of entire sheet\n",
    "            self.worksheet.batch_clear(['A:AZ'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "            \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files by month\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        monthly_files = {\n",
    "            7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 'other': []\n",
    "        }\n",
    "        month_names = {\n",
    "            7: 'July', 8: 'August', 9: 'September', \n",
    "            10: 'October', 11: 'November', 12: 'December'\n",
    "        }\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = pd.to_datetime(metadata['date'])\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                monthly_files['other'].append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month in monthly_files:\n",
    "                monthly_files[file_date.month].append(file_name)\n",
    "            else:\n",
    "                monthly_files['other'].append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY - XNurta 2024 Q3\n",
    "=======================================\"\"\"\n",
    "        \n",
    "        for month_num in [7, 8, 9]:\n",
    "            count = len(monthly_files[month_num])\n",
    "            summary += f\"\\n{month_names[month_num]} files: {count}\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "Other files: {len(monthly_files['other'])}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration for XNurta 2024\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Xnurta 2024 (by day)\",  # Update path\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_XNurta_H2_2024\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = XNurtaDataProcessor2024(**config)\n",
    "    \n",
    "    # Choose run mode\n",
    "    print(\"Choose run mode for XNurta 2024 Q3 data:\")\n",
    "    print(\"1. Initial run (reprocess all Q3 2024 files: Jul-Sep)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78e365",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
