{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9af6b5b",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10542244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_UK\\Tháng 9\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 15 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_UK\\Tháng 9\n",
      "Processing: SA_Campaign_List_20250914_20250914_QPCbvK.xlsx\n",
      "  - Original shape: (14, 33)\n",
      "  - After cleaning: (14, 32)\n",
      "  - Extracted date: 2025-09-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (14, 20)\n",
      "  - Data preservation check: 14 rows maintained\n",
      "Processing: SA_Campaign_List_20250915_20250915_Hw7Hy5.xlsx\n",
      "  - Original shape: (14, 33)\n",
      "  - After cleaning: (14, 32)\n",
      "  - Extracted date: 2025-09-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (14, 20)\n",
      "  - Data preservation check: 14 rows maintained\n",
      "Processing: SA_Campaign_List_20250916_20250916_6up471.xlsx\n",
      "  - Original shape: (17, 33)\n",
      "  - After cleaning: (17, 32)\n",
      "  - Extracted date: 2025-09-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (17, 20)\n",
      "  - Data preservation check: 17 rows maintained\n",
      "Processing: SA_Campaign_List_20250917_20250917_00YGwl.xlsx\n",
      "  - Original shape: (17, 33)\n",
      "  - After cleaning: (17, 32)\n",
      "  - Extracted date: 2025-09-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (17, 20)\n",
      "  - Data preservation check: 17 rows maintained\n",
      "Processing: SA_Campaign_List_20250918_20250918_nLpgkt.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 20)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "Processing: SA_Campaign_List_20250919_20250919_FVfkHl.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 20)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "Processing: SA_Campaign_List_20250920_20250920_o5YUhE.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 20)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "Processing: SA_Campaign_List_20250921_20250921_kjBzDs.xlsx\n",
      "  - Original shape: (21, 33)\n",
      "  - After cleaning: (21, 32)\n",
      "  - Extracted date: 2025-09-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (21, 20)\n",
      "  - Data preservation check: 21 rows maintained\n",
      "Processing: SA_Campaign_List_20250922_20250922_3HUI4X.xlsx\n",
      "  - Original shape: (22, 33)\n",
      "  - After cleaning: (22, 32)\n",
      "  - Extracted date: 2025-09-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (22, 20)\n",
      "  - Data preservation check: 22 rows maintained\n",
      "Processing: SA_Campaign_List_20250923_20250923_Ebq1hG.xlsx\n",
      "  - Original shape: (25, 33)\n",
      "  - After cleaning: (25, 32)\n",
      "  - Extracted date: 2025-09-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (25, 20)\n",
      "  - Data preservation check: 25 rows maintained\n",
      "Processing: SA_Campaign_List_20250924_20250924_QnvriD.xlsx\n",
      "  - Original shape: (24, 33)\n",
      "  - After cleaning: (24, 32)\n",
      "  - Extracted date: 2025-09-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (24, 20)\n",
      "  - Data preservation check: 24 rows maintained\n",
      "Processing: SA_Campaign_List_20250925_20250925_oRVN14.xlsx\n",
      "  - Original shape: (22, 33)\n",
      "  - After cleaning: (22, 32)\n",
      "  - Extracted date: 2025-09-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (22, 20)\n",
      "  - Data preservation check: 22 rows maintained\n",
      "Processing: SA_Campaign_List_20250926_20250926_emFdkA.xlsx\n",
      "  - Original shape: (22, 33)\n",
      "  - After cleaning: (22, 32)\n",
      "  - Extracted date: 2025-09-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (22, 20)\n",
      "  - Data preservation check: 22 rows maintained\n",
      "Processing: SA_Campaign_List_20250927_20250927_PMmRGm.xlsx\n",
      "  - Original shape: (22, 33)\n",
      "  - After cleaning: (22, 32)\n",
      "  - Extracted date: 2025-09-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (22, 20)\n",
      "  - Data preservation check: 22 rows maintained\n",
      "Processing: SA_Campaign_List_20250928_20250928_ogrq2O.xlsx\n",
      "  - Original shape: (22, 33)\n",
      "  - After cleaning: (22, 32)\n",
      "  - Extracted date: 2025-09-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (22, 20)\n",
      "  - Data preservation check: 22 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 15 files\n",
      "  - Failed: 0 files\n",
      "Combined 15 files into 287 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 287\n",
      "Total columns: 20\n",
      "Date range: 2025-09-14 00:00:00 to 2025-09-28 00:00:00\n",
      "Unique ASINs: 13\n",
      "Data successfully saved to: Month9_Ads_Data_UK_20250929_101913.xlsx\n",
      "\n",
      "=== Uploading to Google Sheet ===\n",
      "Existing rows in sheet: 5086\n",
      "Appending 287 rows starting from row 5088\n",
      "Adding 287 new rows to sheet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5868\\2353300379.py:510: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"A{start_row}\", values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended 287 rows to Google Sheet\n",
      "Google Sheet update completed successfully!\n",
      "\n",
      "============================================================\n",
      "Month 9 processing completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group','ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_UK\", \"Tháng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing Tháng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(\n",
    "            ['Date', 'Sales'], \n",
    "            ascending=[True, False],   # Date ↑, Sales ↓\n",
    "            na_position='last'\n",
    "        ).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data_UK', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    with auto row expansion and safe datetime/NaN handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # Xác định row bắt đầu append\n",
    "        start_row = existing_rows + 2\n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Copy & xử lý dữ liệu\n",
    "        safe_df = new_df.copy()\n",
    "        \n",
    "        # Format cột datetime thành M/D/YYYY\n",
    "        datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "        for col in datetime_cols:\n",
    "            safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "        \n",
    "        # Thay NaN = \"\"\n",
    "        safe_df = safe_df.fillna(\"\")\n",
    "        \n",
    "        # Convert sang list of lists\n",
    "        values = safe_df.values.tolist()\n",
    "        \n",
    "        # 🔥 Đảm bảo sheet có đủ rows\n",
    "        needed_rows = start_row + len(values) - 1\n",
    "        if needed_rows > sheet.row_count:\n",
    "            add_count = needed_rows - sheet.row_count\n",
    "            print(f\"Adding {add_count} new rows to sheet...\")\n",
    "            sheet.add_rows(add_count)\n",
    "        \n",
    "        # Append dữ liệu\n",
    "        sheet.update(f\"A{start_row}\", values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            safe_df = new_df.copy()\n",
    "            datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "            for col in datetime_cols:\n",
    "                safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "            safe_df = safe_df.fillna(\"\")\n",
    "            \n",
    "            set_with_dataframe(sheet, safe_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_UK_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q3_2025_UK\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Promo', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "📋 Available columns: 16/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "📋 Available columns: 21/21\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "📋 Available columns: 17/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund сost']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Promo']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "📋 Available columns: 20/21\n",
      "⚠️ Missing columns: ['Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "📊 Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "📋 Available columns: 16/21\n",
      "⚠️ Missing columns: ['Refunds', '% Refunds', 'Refund сost', 'Sessions', 'Shipping']\n",
      "\n",
      "📈 Combining 70 dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_13136\\72051554.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined data shape: (1339, 21)\n",
      "📅 Date range: 2025-07-01 00:00:00 to 2025-09-08 00:00:00\n",
      "📤 Uploading to Google Sheets...\n",
      "✅ Successfully uploaded 1339 rows to Google Sheets\n",
      "🔗 Sheet: Raw_SB_H2_2025_UK\n",
      "📋 Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "🎉 Successfully processed 70 files:\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "   ✓ NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 8\n",
      "Total files: 70\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-09 15:37:41\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # Làm sạch tên cột\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Tạo mapping cho các tên cột có thể khác nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # Tìm cột khớp chính xác hoặc gần giống\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khớp chính xác\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khớp một phần cho một số trường hợp đặc biệt\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chỉ giữ lại các cột cần thiết\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # Thêm các cột thiếu với giá trị None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sắp xếp lại theo thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"📋 Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            print(f\"📋 Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_UK\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_UK\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618bef3",
   "metadata": {},
   "source": [
    "# SellerBoard Tháng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "📊 Current data rows in sheet: 1668\n",
      "➕ New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_28_09_2025-28_09_2025_(2025_09_29_08_16_989).xlsx\n",
      "📤 Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_17520\\397982148.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 13 rows to Google Sheets\n",
      "\n",
      "📊 SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 1\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-29 14:26:42\n",
      "\n",
      "September files processed:\n",
      "  • NewEleven_EU_Dashboard_Products_Group_by_ASIN_28_09_2025-28_09_2025_(2025_09_29_08_16_989).xlsx\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho tháng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # Thêm các cột thiếu\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sắp xếp đúng thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 9 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chỉ xử lý file tháng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Lấy tất cả giá trị trong cột A để đếm số dòng có dữ liệu\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trừ đi header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"📊 Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Đảm bảo cột theo đúng thứ tự chuẩn\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"ℹ️ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"📤 Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # để Sheets giữ trống\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"✅ Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 9 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  • {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_UK\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_UK\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"🗓️ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"❌ Operation cancelled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
