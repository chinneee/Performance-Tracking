{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9af6b5b",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10542244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_UK\\ThÃ¡ng 9\n",
      "\n",
      "=== Processing ThÃ¡ng 9 ===\n",
      "Found 4 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_UK\\ThÃ¡ng 9\n",
      "Processing: SA_Campaign_List_20250917_20250917_7aQz4p.xlsx\n",
      "  - Original shape: (17, 33)\n",
      "  - After cleaning: (17, 32)\n",
      "  - Extracted date: 2025-09-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (17, 29)\n",
      "  - Data preservation check: 17 rows maintained\n",
      "Processing: SA_Campaign_List_20250918_20250918_YqpGug.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 29)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "Processing: SA_Campaign_List_20250919_20250919_4GO64Q.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 29)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "Processing: SA_Campaign_List_20250920_20250920_oioj9s.xlsx\n",
      "  - Original shape: (15, 33)\n",
      "  - After cleaning: (15, 32)\n",
      "  - Extracted date: 2025-09-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Daily Budget - too many conversion failures, keeping original\n",
      "Warning: Current Budget - too many conversion failures, keeping original\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: CPA - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (15, 29)\n",
      "  - Data preservation check: 15 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 4 files\n",
      "  - Failed: 0 files\n",
      "Combined 4 files into 62 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 62\n",
      "Total columns: 29\n",
      "Date range: 2025-09-17 00:00:00 to 2025-09-20 00:00:00\n",
      "Unique ASINs: 7\n",
      "Data successfully saved to: Month9_Ads_Data_20250922_093157.xlsx\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      ASIN       Date Campaign type                                                                 Campaign     Status Country                                               Portfolio Daily Budget       Bidding Strategy  Top-of-search IS  Avg.time in Budget  Impressions  Clicks    CTR  Spend  CPC  Orders  Sales  Units  CVR    ACOS  ROAS   CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU Orders Other SKU  Units Same SKU Units Other SKU\n",
      "B08R8R2LQF 2025-09-17            SP SP_B08R8R2LQF_Tumbler 20_May The Forties Be With You Black_Auto_All_Down     Paused      UK B08R8R2LQF_TUMBLER 20_MAY THE FORTIES BE WITH YOU BLACK        Â£5.00 Dynamic bids down only            0.0000                 100           75       0 0.0000   0.00  NaN       0    0.0      0  NaN      --   NaN    --             0.0              0.0                0               --               0              --\n",
      "B08R8R2LQF 2025-09-17            SP       B08R8R2LQF_20oz_Forties black_40th birthday gifts for men ex_11h30     Paused      UK B08R8R2LQF_TUMBLER 20_MAY THE FORTIES BE WITH YOU BLACK        Â£8.00 Dynamic bids down only            0.0000                  70           24       0 0.0000   0.00  NaN       0    0.0      0  NaN      --   NaN    --             0.0              0.0                0               --               0              --\n",
      "B08R8R7T36 2025-09-17            SP                              B08R8R7T36_12oz_Bride_bride to be b,p_14h45 Delivering      UK                             B08R8R7T36_TUMBLER 12_BRIDE       Â£10.00  Dynamic bids and down            0.0512                 100          923       8 0.0087   7.04 0.88       2   28.3      2 0.25  0.2488  4.02  3.52            28.3              0.0                2               --               2              --\n",
      "\n",
      "=== Column List ===\n",
      " 1. ASIN\n",
      " 2. Date\n",
      " 3. Campaign type\n",
      " 4. Campaign\n",
      " 5. Status\n",
      " 6. Country\n",
      " 7. Portfolio\n",
      " 8. Daily Budget\n",
      " 9. Bidding Strategy\n",
      "10. Top-of-search IS\n",
      "11. Avg.time in Budget\n",
      "12. Impressions\n",
      "13. Clicks\n",
      "14. CTR\n",
      "15. Spend\n",
      "16. CPC\n",
      "17. Orders\n",
      "18. Sales\n",
      "19. Units\n",
      "20. CVR\n",
      "21. ACOS\n",
      "22. ROAS\n",
      "23. CPA\n",
      "24. Sales Same SKU\n",
      "25. Sales Other SKU\n",
      "26. Orders Same SKU\n",
      "27. Orders Other SKU\n",
      "28. Units Same SKU\n",
      "29. Units Other SKU\n",
      "\n",
      "=== Uploading to Google Sheet ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\1146510822.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\1146510822.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\1146510822.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\1146510822.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing rows in sheet: 1499\n",
      "Appending 62 rows starting from row 1501\n",
      "Updating range: A1501:]1562\n",
      "Error appending to Google Sheet: Object of type Timestamp is not JSON serializable\n",
      "Trying fallback method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\1146510822.py:509: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(range_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback append completed\n",
      "Google Sheet update completed successfully!\n",
      "\n",
      "============================================================\n",
      "Month 9 processing completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS', 'CPA', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU', 'Orders Same SKU', 'Units Same SKU', \n",
    "                      'Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_UK\", \"ThÃ¡ng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing ThÃ¡ng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(['ASIN', 'Date'], na_position='last')\n",
    "        df = df.reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def display_sample(df, rows=3):\n",
    "    \"\"\"Display sample data with proper formatting\"\"\"\n",
    "    try:\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 20)\n",
    "        print(df.head(rows).to_string(index=False))\n",
    "        pd.reset_option('display.max_columns')\n",
    "        pd.reset_option('display.width')\n",
    "        pd.reset_option('display.max_colwidth')\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying sample: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # Calculate the starting row for appending (existing_rows + 2 to account for header)\n",
    "        start_row = existing_rows + 2\n",
    "        \n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Convert DataFrame to list of lists for batch update\n",
    "        values = new_df.values.tolist()\n",
    "        \n",
    "        # Prepare the range for batch update\n",
    "        end_col_letter = chr(ord('A') + len(new_df.columns) - 1)  # Convert to column letter\n",
    "        range_name = f\"A{start_row}:{end_col_letter}{start_row + len(values) - 1}\"\n",
    "        \n",
    "        print(f\"Updating range: {range_name}\")\n",
    "        \n",
    "        # Batch update the sheet\n",
    "        sheet.update(range_name, values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        # Fallback: try using set_with_dataframe but with an offset\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            # This will overwrite, but we'll note the issue\n",
    "            set_with_dataframe(sheet, new_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            # Display sample\n",
    "            print(f\"\\nSample data (first 3 rows):\")\n",
    "            display_sample(result_df)\n",
    "            \n",
    "            print(f\"\\n=== Column List ===\")\n",
    "            for i, col in enumerate(result_df.columns, 1):\n",
    "                print(f\"{i:2d}. {col}\")\n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q3_2025_UK\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "ğŸš€ INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Promo', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "ğŸ“‹ Available columns: 16/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "ğŸ“‹ Available columns: 21/21\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "ğŸ“‹ Available columns: 17/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds', 'Refund Ñost']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Promo']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Refunds', 'Promo', '% Refunds']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "ğŸ“‹ Available columns: 20/21\n",
      "âš ï¸ Missing columns: ['Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "ğŸ“Š Processing: NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "ğŸ“‹ Available columns: 16/21\n",
      "âš ï¸ Missing columns: ['Refunds', '% Refunds', 'Refund Ñost', 'Sessions', 'Shipping']\n",
      "\n",
      "ğŸ“ˆ Combining 70 dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_13136\\72051554.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined data shape: (1339, 21)\n",
      "ğŸ“… Date range: 2025-07-01 00:00:00 to 2025-09-08 00:00:00\n",
      "ğŸ“¤ Uploading to Google Sheets...\n",
      "âœ… Successfully uploaded 1339 rows to Google Sheets\n",
      "ğŸ”— Sheet: Raw_SB_H2_2025_UK\n",
      "ğŸ“‹ Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund Ñost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "ğŸ‰ Successfully processed 70 files:\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(09_26_07_381).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(09_26_20_162).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(09_26_33_861).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(09_26_45_926).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(09_26_57_228).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(09_27_10_809).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(09_27_22_413).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(09_27_34_523).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(09_27_46_109).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(09_27_57_905).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(09_28_09_867).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(09_28_21_969).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(09_28_33_834).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(09_28_46_707).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(09_28_58_142).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(09_29_09_655).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(09_29_23_726).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(09_29_36_942).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(09_29_50_948).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(09_30_03_862).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(09_30_18_319).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(09_30_33_993).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(09_30_43_234).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(09_30_54_151).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(09_31_10_103).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(09_31_28_569).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(09_31_39_815).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(09_31_48_326).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(09_31_58_235).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(09_32_08_308).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(09_32_18_858).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(09_32_32_640).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(09_32_46_122).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(09_33_02_477).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(09_33_12_945).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(09_33_22_773).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(09_33_33_746).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(09_33_45_831).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(09_33_55_312).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(09_34_06_159).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(09_34_19_483).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(09_34_28_709).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(09_34_42_881).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(09_34_51_142).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(09_35_01_703).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(09_35_12_567).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(09_35_25_805).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(09_35_35_305).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(09_35_46_961).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(09_35_59_638).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(09_36_11_475).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(09_36_26_414).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(09_36_39_445).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(09_36_50_831).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(09_37_02_788).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(09_18_33_565).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(09_18_47_751).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(09_19_00_127).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(09_19_14_731).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(09_19_29_822).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(09_20_08_821).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(09_20_23_128).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(09_20_37_453).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(09_21_00_169).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_03_09_2025-03_09_2025_(09_21_16_292).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_04_09_2025-04_09_2025_(09_21_28_329).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_05_09_2025-05_09_2025_(09_21_39_278).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_06_09_2025-06_09_2025_(09_21_53_511).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_07_09_2025-07_09_2025_(09_22_07_795).xlsx\n",
      "   âœ“ NewEleven_EU_Dashboard Products Group by ASIN_08_09_2025-08_09_2025_(09_22_28_446).xlsx\n",
      "\n",
      "ğŸ“Š PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 8\n",
      "Total files: 70\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-09 15:37:41\n",
      "        \n",
      "\n",
      "ğŸ“‹ Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund Ñost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Äá»‹nh nghÄ©a thá»© tá»± cá»™t chuáº©n\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund Ñost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # LÃ m sáº¡ch tÃªn cá»™t\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Táº¡o mapping cho cÃ¡c tÃªn cá»™t cÃ³ thá»ƒ khÃ¡c nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # TÃ¬m cá»™t khá»›p chÃ­nh xÃ¡c hoáº·c gáº§n giá»‘ng\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khá»›p chÃ­nh xÃ¡c\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khá»›p má»™t pháº§n cho má»™t sá»‘ trÆ°á»ng há»£p Ä‘áº·c biá»‡t\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'Ñost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cáº§n thiáº¿t\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # ThÃªm cÃ¡c cá»™t thiáº¿u vá»›i giÃ¡ trá»‹ None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sáº¯p xáº¿p láº¡i theo thá»© tá»± chuáº©n\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"ğŸ“‹ Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"âš ï¸ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"ğŸ”„ Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"â• New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"ğŸ”„ Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"â­ï¸ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"ğŸš€ INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"ğŸ—‘ï¸ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"ğŸ”„ INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"ğŸ“Š Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\nğŸ“ˆ Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"âœ… Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"ğŸ“… Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\nğŸ‰ Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   âœ“ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ“¤ Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"âœ… Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"ğŸ”— Sheet: {self.worksheet_name}\")\n",
    "            print(f\"ğŸ“‹ Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ğŸ“Š PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_UK\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_UK\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\nğŸ“‹ Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618bef3",
   "metadata": {},
   "source": [
    "# SellerBoard ThÃ¡ng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—“ï¸ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "ğŸ—“ï¸ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "ğŸ“Š Current data rows in sheet: 1306\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_07_09_2025-07_09_2025_(03_23_20_306).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_08_09_2025-08_09_2025_(03_23_35_956).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_09_09_2025-09_09_2025_(03_23_49_508).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_10_09_2025-10_09_2025_(03_24_01_186).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_11_09_2025-11_09_2025_(03_24_15_778).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_12_09_2025-12_09_2025_(03_24_28_659).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_13_09_2025-13_09_2025_(03_24_42_468).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_14_09_2025-14_09_2025_(03_26_03_759).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_15_09_2025-15_09_2025_(03_26_19_244).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_16_09_2025-16_09_2025_(03_26_31_261).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_17_09_2025-17_09_2025_(03_26_46_707).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_18_09_2025-18_09_2025_(03_27_00_892).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_19_09_2025-19_09_2025_(03_27_24_107).xlsx\n",
      "â• New September file detected: NewEleven_EU_Dashboard_Products_Group_by_ASIN_20_09_2025-20_09_2025_(03_27_38_790).xlsx\n",
      "ğŸ“¤ Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\397982148.py:199: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_23976\\397982148.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully appended 245 rows to Google Sheets\n",
      "\n",
      "ğŸ“Š SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 14\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-22 09:29:43\n",
      "\n",
      "September files processed:\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_07_09_2025-07_09_2025_(03_23_20_306).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_08_09_2025-08_09_2025_(03_23_35_956).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_09_09_2025-09_09_2025_(03_23_49_508).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_10_09_2025-10_09_2025_(03_24_01_186).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_11_09_2025-11_09_2025_(03_24_15_778).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_12_09_2025-12_09_2025_(03_24_28_659).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_13_09_2025-13_09_2025_(03_24_42_468).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_14_09_2025-14_09_2025_(03_26_03_759).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_15_09_2025-15_09_2025_(03_26_19_244).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_16_09_2025-16_09_2025_(03_26_31_261).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_17_09_2025-17_09_2025_(03_26_46_707).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_18_09_2025-18_09_2025_(03_27_00_892).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_19_09_2025-19_09_2025_(03_27_24_107).xlsx\n",
      "  â€¢ NewEleven_EU_Dashboard_Products_Group_by_ASIN_20_09_2025-20_09_2025_(03_27_38_790).xlsx\n",
      "        \n",
      "\n",
      "ğŸ“‹ Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund Ñost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Äá»‹nh nghÄ©a thá»© tá»± cá»™t chuáº©n\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund Ñost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho thÃ¡ng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'Ñost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # ThÃªm cÃ¡c cá»™t thiáº¿u\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± chuáº©n\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 9 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chá»‰ xá»­ lÃ½ file thÃ¡ng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"â• New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"ğŸ”„ Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"â­ï¸ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Láº¥y táº¥t cáº£ giÃ¡ trá»‹ trong cá»™t A Ä‘á»ƒ Ä‘áº¿m sá»‘ dÃ²ng cÃ³ dá»¯ liá»‡u\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trá»« Ä‘i header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"ğŸ“Š Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ—“ï¸ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Äáº£m báº£o cá»™t theo Ä‘Ãºng thá»© tá»± chuáº©n\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"ğŸ“¤ Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # Ä‘á»ƒ Sheets giá»¯ trá»‘ng\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"âœ… Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 9 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ğŸ“Š SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  â€¢ {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_UK\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_UK\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"ğŸ—“ï¸ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\nğŸ“‹ Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"âŒ Operation cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eedbdb7",
   "metadata": {},
   "source": [
    "# XNurta H2 2024 (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f100f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode for XNurta 2024 Q3 data:\n",
      "1. Initial run (reprocess all Q3 2024 files: Jul-Sep)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "ğŸš€ INITIAL RUN: Processing Q3 2024 XNurta files (Jul-Sep)\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "============================================================\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "ğŸ”„ Initial run: Processing Q3 2024 file SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "ğŸ“Š Processing: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "   ğŸ—‘ï¸ Dropped columns: Profile, Labels, Budget group, Status, Current Budget, SP Off-site Ads Strategy, Bidding Strategy\n",
      "âœ… Successfully processed: SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "\n",
      "ğŸ“ˆ Combining 92 dataframes...\n",
      "âœ… Combined data shape: (41239, 62)\n",
      "ğŸ“… Date range: 2024-07-01 00:00:00 to 2024-09-30 00:00:00\n",
      "ğŸ“¤ Uploading to Google Sheets...\n",
      "âœ… Successfully uploaded 41239 rows to Google Sheets\n",
      "ğŸ”— Sheet: Raw_XNurta_H2_2024\n",
      "\n",
      "ğŸ‰ Successfully processed 92 files:\n",
      "   âœ“ SA_Campaign_List_20240701_20240701_38C2RG.xlsx\n",
      "   âœ“ SA_Campaign_List_20240702_20240702_9TkpbM.xlsx\n",
      "   âœ“ SA_Campaign_List_20240703_20240703_G80TRn.xlsx\n",
      "   âœ“ SA_Campaign_List_20240704_20240704_gZtCIL.xlsx\n",
      "   âœ“ SA_Campaign_List_20240705_20240705_A1i8Dn.xlsx\n",
      "   âœ“ SA_Campaign_List_20240706_20240706_ed4ep2.xlsx\n",
      "   âœ“ SA_Campaign_List_20240707_20240707_1nbAjc.xlsx\n",
      "   âœ“ SA_Campaign_List_20240708_20240708_nSsf7X.xlsx\n",
      "   âœ“ SA_Campaign_List_20240709_20240709_Sa3Pwo.xlsx\n",
      "   âœ“ SA_Campaign_List_20240710_20240710_NBUR0e.xlsx\n",
      "   âœ“ SA_Campaign_List_20240711_20240711_5zfUcS.xlsx\n",
      "   âœ“ SA_Campaign_List_20240712_20240712_Rd20hJ.xlsx\n",
      "   âœ“ SA_Campaign_List_20240713_20240713_z6wmov.xlsx\n",
      "   âœ“ SA_Campaign_List_20240714_20240714_gkwAAt.xlsx\n",
      "   âœ“ SA_Campaign_List_20240715_20240715_EB6eR5.xlsx\n",
      "   âœ“ SA_Campaign_List_20240716_20240716_X7QEk3.xlsx\n",
      "   âœ“ SA_Campaign_List_20240717_20240717_238iuR.xlsx\n",
      "   âœ“ SA_Campaign_List_20240718_20240718_jCLFfO.xlsx\n",
      "   âœ“ SA_Campaign_List_20240719_20240719_mgEWBW.xlsx\n",
      "   âœ“ SA_Campaign_List_20240720_20240720_D61x3E.xlsx\n",
      "   âœ“ SA_Campaign_List_20240721_20240721_Y9D7Br.xlsx\n",
      "   âœ“ SA_Campaign_List_20240722_20240722_WzjMZL.xlsx\n",
      "   âœ“ SA_Campaign_List_20240723_20240723_AQERzQ.xlsx\n",
      "   âœ“ SA_Campaign_List_20240724_20240724_vsoX3D.xlsx\n",
      "   âœ“ SA_Campaign_List_20240725_20240725_fdrCVt.xlsx\n",
      "   âœ“ SA_Campaign_List_20240726_20240726_yl0GwV.xlsx\n",
      "   âœ“ SA_Campaign_List_20240727_20240727_oUaGYh.xlsx\n",
      "   âœ“ SA_Campaign_List_20240728_20240728_JA2MmE.xlsx\n",
      "   âœ“ SA_Campaign_List_20240729_20240729_nccfO7.xlsx\n",
      "   âœ“ SA_Campaign_List_20240730_20240730_snFC0I.xlsx\n",
      "   âœ“ SA_Campaign_List_20240731_20240731_7gjPuv.xlsx\n",
      "   âœ“ SA_Campaign_List_20240801_20240801_THl8uh.xlsx\n",
      "   âœ“ SA_Campaign_List_20240802_20240802_Q1JEsN.xlsx\n",
      "   âœ“ SA_Campaign_List_20240803_20240803_syIZ3O.xlsx\n",
      "   âœ“ SA_Campaign_List_20240804_20240804_H2wN6Y.xlsx\n",
      "   âœ“ SA_Campaign_List_20240805_20240805_A3oC8p.xlsx\n",
      "   âœ“ SA_Campaign_List_20240806_20240806_tMyfQk.xlsx\n",
      "   âœ“ SA_Campaign_List_20240807_20240807_pvwPyI.xlsx\n",
      "   âœ“ SA_Campaign_List_20240808_20240808_iBVkTq (1).xlsx\n",
      "   âœ“ SA_Campaign_List_20240809_20240809_leqaeZ.xlsx\n",
      "   âœ“ SA_Campaign_List_20240810_20240810_yhgNkX.xlsx\n",
      "   âœ“ SA_Campaign_List_20240811_20240811_QtBpvM.xlsx\n",
      "   âœ“ SA_Campaign_List_20240812_20240812_JCmF4k.xlsx\n",
      "   âœ“ SA_Campaign_List_20240813_20240813_wEXK61.xlsx\n",
      "   âœ“ SA_Campaign_List_20240814_20240814_oXNFzb.xlsx\n",
      "   âœ“ SA_Campaign_List_20240815_20240815_jYtq9X.xlsx\n",
      "   âœ“ SA_Campaign_List_20240816_20240816_XV74Ue.xlsx\n",
      "   âœ“ SA_Campaign_List_20240817_20240817_dhbN8X.xlsx\n",
      "   âœ“ SA_Campaign_List_20240818_20240818_cVE3M2.xlsx\n",
      "   âœ“ SA_Campaign_List_20240819_20240819_xlZUuH.xlsx\n",
      "   âœ“ SA_Campaign_List_20240820_20240820_Cgxnji.xlsx\n",
      "   âœ“ SA_Campaign_List_20240821_20240821_X2DaaD.xlsx\n",
      "   âœ“ SA_Campaign_List_20240822_20240822_KNfyGC.xlsx\n",
      "   âœ“ SA_Campaign_List_20240823_20240823_xgc6an.xlsx\n",
      "   âœ“ SA_Campaign_List_20240824_20240824_zkQ8SX.xlsx\n",
      "   âœ“ SA_Campaign_List_20240825_20240825_JXZ5CR.xlsx\n",
      "   âœ“ SA_Campaign_List_20240826_20240826_j46ClA.xlsx\n",
      "   âœ“ SA_Campaign_List_20240827_20240827_yD1KBc.xlsx\n",
      "   âœ“ SA_Campaign_List_20240828_20240828_FKLiIq.xlsx\n",
      "   âœ“ SA_Campaign_List_20240829_20240829_qBu7M1.xlsx\n",
      "   âœ“ SA_Campaign_List_20240830_20240830_ZqG2zh.xlsx\n",
      "   âœ“ SA_Campaign_List_20240831_20240831_sBrJAN.xlsx\n",
      "   âœ“ SA_Campaign_List_20240901_20240901_3JsLtM.xlsx\n",
      "   âœ“ SA_Campaign_List_20240902_20240902_cG3CFE.xlsx\n",
      "   âœ“ SA_Campaign_List_20240903_20240903_GZDx0M.xlsx\n",
      "   âœ“ SA_Campaign_List_20240904_20240904_rIkwF3.xlsx\n",
      "   âœ“ SA_Campaign_List_20240905_20240905_Otkihv.xlsx\n",
      "   âœ“ SA_Campaign_List_20240906_20240906_6P7Fxn.xlsx\n",
      "   âœ“ SA_Campaign_List_20240907_20240907_SgpIZ2.xlsx\n",
      "   âœ“ SA_Campaign_List_20240908_20240908_74oHrV.xlsx\n",
      "   âœ“ SA_Campaign_List_20240909_20240909_5EBgFV.xlsx\n",
      "   âœ“ SA_Campaign_List_20240910_20240910_L1awWn.xlsx\n",
      "   âœ“ SA_Campaign_List_20240911_20240911_W5poXg.xlsx\n",
      "   âœ“ SA_Campaign_List_20240912_20240912_0qtkeO.xlsx\n",
      "   âœ“ SA_Campaign_List_20240913_20240913_E1WdFc.xlsx\n",
      "   âœ“ SA_Campaign_List_20240914_20240914_YxYNAn.xlsx\n",
      "   âœ“ SA_Campaign_List_20240915_20240915_W7CMIj.xlsx\n",
      "   âœ“ SA_Campaign_List_20240916_20240916_DOofR0.xlsx\n",
      "   âœ“ SA_Campaign_List_20240917_20240917_2vgqpt.xlsx\n",
      "   âœ“ SA_Campaign_List_20240918_20240918_J8z1kr.xlsx\n",
      "   âœ“ SA_Campaign_List_20240919_20240919_8jFTcp.xlsx\n",
      "   âœ“ SA_Campaign_List_20240920_20240920_4cmtsa.xlsx\n",
      "   âœ“ SA_Campaign_List_20240921_20240921_es3nT6.xlsx\n",
      "   âœ“ SA_Campaign_List_20240922_20240922_0AkxFc.xlsx\n",
      "   âœ“ SA_Campaign_List_20240923_20240923_wuLXAt.xlsx\n",
      "   âœ“ SA_Campaign_List_20240924_20240924_Bi9snd.xlsx\n",
      "   âœ“ SA_Campaign_List_20240925_20240925_OGRl2a.xlsx\n",
      "   âœ“ SA_Campaign_List_20240926_20240926_snbQG0.xlsx\n",
      "   âœ“ SA_Campaign_List_20240927_20240927_KJf43V.xlsx\n",
      "   âœ“ SA_Campaign_List_20240928_20240928_e1R8gN.xlsx\n",
      "   âœ“ SA_Campaign_List_20240929_20240929_f8Zkxo.xlsx\n",
      "   âœ“ SA_Campaign_List_20240930_20240930_GbPddt.xlsx\n",
      "\n",
      "ğŸ“Š PROCESSING SUMMARY - XNurta 2024 Q3\n",
      "=======================================\n",
      "July files: 31\n",
      "August files: 31\n",
      "September files: 30\n",
      "Other files: 0\n",
      "Total files: 184\n",
      "Last run: 2025-08-25 17:33:05\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "class XNurtaDataProcessor2024:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"xnurta_file_metadata_2024.json\"\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "        \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "        Returns the first date (start date)\n",
    "        \"\"\"\n",
    "        pattern = r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx'\n",
    "        match = re.search(pattern, os.path.basename(filename))\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "        return None\n",
    "        \n",
    "    def clean_currency_column(self, column):\n",
    "        \"\"\"Remove $ symbol and convert to float\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN'], np.nan)\n",
    "            return pd.to_numeric(cleaned, errors='coerce')\n",
    "        return column\n",
    "        \n",
    "    def convert_to_float(self, column):\n",
    "        \"\"\"Convert object columns to float\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            return pd.to_numeric(cleaned, errors='coerce')\n",
    "        return column\n",
    "        \n",
    "    def convert_to_int(self, column):\n",
    "        \"\"\"Convert object columns to int\"\"\"\n",
    "        if column.dtype == 'object':\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        return column\n",
    "        \n",
    "    def extract_asin_from_portfolio(self, portfolio_str):\n",
    "        \"\"\"Extract ASIN from Portfolio string\"\"\"\n",
    "        if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "            return None\n",
    "        \n",
    "        portfolio_str = str(portfolio_str)\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str)\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: 10 alphanumeric characters starting with letter\n",
    "        pattern2 = r'[A-Z][A-Z0-9]{9}'\n",
    "        match2 = re.search(pattern2, portfolio_str)\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "        pattern3 = r'[A-Z0-9]{10}'\n",
    "        match3 = re.search(pattern3, portfolio_str)\n",
    "        if match3:\n",
    "            return match3.group()\n",
    "        \n",
    "        # Pattern 4: 10 alphanumeric with possible lowercase (convert to uppercase)\n",
    "        pattern4 = r'[A-Za-z0-9]{10}'\n",
    "        match4 = re.search(pattern4, portfolio_str)\n",
    "        if match4:\n",
    "            return match4.group().upper()\n",
    "        \n",
    "        # If no pattern matches, return first 10 characters as fallback\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        return portfolio_str[:10] if len(portfolio_str) >= 10 else portfolio_str\n",
    "        \n",
    "    def normalize_campaign_types(self, text):\n",
    "        \"\"\"Normalize campaign type keywords\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file according to specifications\"\"\"\n",
    "        try:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_extracted = self.extract_date_from_filename(file_path)\n",
    "            \n",
    "            # Drop specified columns if they exist\n",
    "            columns_to_drop = [\n",
    "                'Profile', \n",
    "                'Labels', \n",
    "                'Budget group',\n",
    "                'Status',\n",
    "                'Current Budget',\n",
    "                'SP Off-site Ads Strategy',\n",
    "                'Bidding Strategy',\n",
    "                'Sales Same SKU',\n",
    "                'Sales Other SKU',\n",
    "                'Orders Same SKU',\n",
    "                'Orders Other SKU',\n",
    "                'Units Same SKU',\n",
    "                'Units Other SKU'\n",
    "            ]\n",
    "            existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "            if existing_columns_to_drop:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"   ğŸ—‘ï¸ Dropped columns: {', '.join(existing_columns_to_drop)}\")\n",
    "            \n",
    "            # Add ASIN column as first column (extract ASIN from Portfolio)\n",
    "            if 'Portfolio' in df.columns:\n",
    "                df.insert(0, 'ASIN', df['Portfolio'].apply(self.extract_asin_from_portfolio))\n",
    "            \n",
    "            # Add Date column\n",
    "            df.insert(1, 'Date', date_extracted)\n",
    "            \n",
    "            # Normalize campaign types in Campaign Type column\n",
    "            if 'Campaign type' in df.columns:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(self.normalize_campaign_types)\n",
    "            \n",
    "            # Clean currency columns\n",
    "            currency_columns = ['Daily Budget']\n",
    "            for col in currency_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = self.clean_currency_column(df[col])\n",
    "            \n",
    "            # Convert specified columns to float\n",
    "            float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS']\n",
    "            for col in float_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = self.convert_to_float(df[col])\n",
    "            \n",
    "            # Note: Removed int_columns conversion since those columns are now dropped\n",
    "            \n",
    "            print(f\"âœ… Successfully processed: {os.path.basename(file_path)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {file_path}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    def _is_h2_2024_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to Q3 2024 (July to September 2024)\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2024\n",
    "        \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all H2 2024 files\n",
    "        if is_initial_run:\n",
    "            if self._is_h2_2024_file(file_date):\n",
    "                print(f\"ğŸ”„ Initial run: Processing Q3 2024 file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"â• New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"ğŸ”„ Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"â­ï¸ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "        \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "        \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"Main processing function for XNurta 2024 data\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"ğŸš€ INITIAL RUN: Processing Q3 2024 XNurta files (Jul-Sep)\")\n",
    "            # Clear existing Q3 2024 metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = pd.to_datetime(metadata['date'])\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_h2_2024_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"ğŸ—‘ï¸ Cleared metadata for Q3 2024 file: {file_name}\")\n",
    "        else:\n",
    "            print(\"ğŸ”„ INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders (ThÃ¡ng 7, ThÃ¡ng 8, etc.)\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"ğŸ“Š Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\nğŸ“ˆ Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by Date and ASIN\n",
    "            if \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "            \n",
    "            # Reset index\n",
    "            master_df = master_df.reset_index(drop=True)\n",
    "            \n",
    "            print(f\"âœ… Combined data shape: {master_df.shape}\")\n",
    "            print(f\"ğŸ“… Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\nğŸ‰ Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   âœ“ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ“¤ Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear limited columns range (A to AZ) instead of entire sheet\n",
    "            self.worksheet.batch_clear(['A:AZ'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"âœ… Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"ğŸ”— Sheet: {self.worksheet_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error uploading to Google Sheets: {e}\")\n",
    "            \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files by month\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        monthly_files = {\n",
    "            7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 'other': []\n",
    "        }\n",
    "        month_names = {\n",
    "            7: 'July', 8: 'August', 9: 'September', \n",
    "            10: 'October', 11: 'November', 12: 'December'\n",
    "        }\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = pd.to_datetime(metadata['date'])\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                monthly_files['other'].append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month in monthly_files:\n",
    "                monthly_files[file_date.month].append(file_name)\n",
    "            else:\n",
    "                monthly_files['other'].append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ğŸ“Š PROCESSING SUMMARY - XNurta 2024 Q3\n",
    "=======================================\"\"\"\n",
    "        \n",
    "        for month_num in [7, 8, 9]:\n",
    "            count = len(monthly_files[month_num])\n",
    "            summary += f\"\\n{month_names[month_num]} files: {count}\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "Other files: {len(monthly_files['other'])}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration for XNurta 2024\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Xnurta 2024 (by day)\",  # Update path\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_XNurta_H2_2024\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = XNurtaDataProcessor2024(**config)\n",
    "    \n",
    "    # Choose run mode\n",
    "    print(\"Choose run mode for XNurta 2024 Q3 data:\")\n",
    "    print(\"1. Initial run (reprocess all Q3 2024 files: Jul-Sep)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78e365",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
