{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217392d9",
   "metadata": {},
   "source": [
    "# Monthly Performance (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903bddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.csv\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    pattern = r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.csv'\n",
    "    match = re.search(pattern, os.path.basename(filename))\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    return None\n",
    "\n",
    "def clean_currency_column(column):\n",
    "    \"\"\"\n",
    "    Remove $ symbol and convert to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Remove $ symbol and any other non-numeric characters except decimal point\n",
    "        cleaned = column.astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "        # Replace empty strings and 'nan' with NaN\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_float(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Replace empty strings and specific text with NaN\n",
    "        cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_int(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to int\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        # Convert to float first, then to int (handling NaN values)\n",
    "        float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "        return float_col.astype('Int64')  # Nullable integer type\n",
    "    return column\n",
    "\n",
    "def extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Extract ASIN from Portfolio string. ASIN is typically 10 characters:\n",
    "    - Pattern 1: B followed by 9 alphanumeric characters (e.g., B08XXXXXXX)\n",
    "    - Pattern 2: 10 alphanumeric characters starting with letters\n",
    "    - Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return None\n",
    "    \n",
    "    portfolio_str = str(portfolio_str)\n",
    "    \n",
    "    # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "    pattern1 = r'B[A-Z0-9]{9}'\n",
    "    match1 = re.search(pattern1, portfolio_str)\n",
    "    if match1:\n",
    "        return match1.group()\n",
    "    \n",
    "    # Pattern 2: 10 alphanumeric characters starting with letter\n",
    "    pattern2 = r'[A-Z][A-Z0-9]{9}'\n",
    "    match2 = re.search(pattern2, portfolio_str)\n",
    "    if match2:\n",
    "        return match2.group()\n",
    "    \n",
    "    # Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    pattern3 = r'[A-Z0-9]{10}'\n",
    "    match3 = re.search(pattern3, portfolio_str)\n",
    "    if match3:\n",
    "        return match3.group()\n",
    "    \n",
    "    # Pattern 4: 10 alphanumeric with possible lowercase (convert to uppercase)\n",
    "    pattern4 = r'[A-Za-z0-9]{10}'\n",
    "    match4 = re.search(pattern4, portfolio_str)\n",
    "    if match4:\n",
    "        return match4.group().upper()\n",
    "    \n",
    "    # If no pattern matches, return first 10 characters as fallback\n",
    "    clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "    if len(clean_str) >= 10:\n",
    "        return clean_str[:10].upper()\n",
    "    \n",
    "    return portfolio_str[:10] if len(portfolio_str) >= 10 else portfolio_str\n",
    "\n",
    "def normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Normalize campaign type keywords\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Define normalization mapping\n",
    "    normalizations = {\n",
    "        'sponsoredBrands': 'SB',\n",
    "        'sponsoredDisplay': 'SD', \n",
    "        'sponsoredProducts': 'SP',\n",
    "        'sponsoredbrands': 'SB',\n",
    "        'sponsoreddisplay': 'SD',\n",
    "        'sponsoredproducts': 'SP',\n",
    "        'Sponsored Brands': 'SB',\n",
    "        'Sponsored Display': 'SD',\n",
    "        'Sponsored Products': 'SP'\n",
    "    }\n",
    "    \n",
    "    # Apply normalizations\n",
    "    for original, normalized in normalizations.items():\n",
    "        text = text.replace(original, normalized)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_single_csv(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file according to specifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        \n",
    "        # Drop specified columns if they exist\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            df = df.drop(columns=existing_columns_to_drop)\n",
    "        \n",
    "        # Add ASIN column as first column (extract ASIN from Portfolio using smart detection)\n",
    "        if 'Portfolio' in df.columns:\n",
    "            df.insert(0, 'ASIN', df['Portfolio'].apply(extract_asin_from_portfolio))\n",
    "        \n",
    "        # Add Date column\n",
    "        df.insert(1, 'Date', date_extracted)\n",
    "        \n",
    "        # Normalize campaign types in Campaign Type column\n",
    "        if 'Campaign type' in df.columns:\n",
    "            df['Campaign type'] = df['Campaign type'].apply(normalize_campaign_types)\n",
    "        \n",
    "        # Clean currency columns (remove $ and convert to float)\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = clean_currency_column(df[col])\n",
    "        \n",
    "        # Convert specified columns to float\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_float(df[col])\n",
    "        \n",
    "        # Convert specified columns to int\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_int(df[col])\n",
    "        \n",
    "        print(f\"Successfully processed: {os.path.basename(file_path)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder\n",
    "    \"\"\"\n",
    "    # Find all CSV files in the folder\n",
    "    csv_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in sorted(csv_files):  # Sort to ensure consistent order\n",
    "        df = process_single_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        print(f\"Combined {len(dataframes)} files from {folder_path}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(f\"No valid data found in {folder_path}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45f2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Ads M7 ===\n",
      "Found 31 CSV files in /Users/thuytrinh/Desktop/Performance-Tracking/Ads-XNurta/Ads M7\n",
      "Successfully processed: SA_Campaign_List_20250701_20250701_K2WWz0.csv\n",
      "Successfully processed: SA_Campaign_List_20250702_20250702_Z3Xspn.csv\n",
      "Successfully processed: SA_Campaign_List_20250703_20250703_3QMabM.csv\n",
      "Successfully processed: SA_Campaign_List_20250704_20250704_VYujNl.csv\n",
      "Successfully processed: SA_Campaign_List_20250705_20250705_xZBIxL.csv\n",
      "Successfully processed: SA_Campaign_List_20250706_20250706_vxMjEt.csv\n",
      "Successfully processed: SA_Campaign_List_20250707_20250707_HLFsmI.csv\n",
      "Successfully processed: SA_Campaign_List_20250708_20250708_jykgJT.csv\n",
      "Successfully processed: SA_Campaign_List_20250709_20250709_gyLSeV.csv\n",
      "Successfully processed: SA_Campaign_List_20250710_20250710_EnigeZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250711_20250711_sVkMfE.csv\n",
      "Successfully processed: SA_Campaign_List_20250712_20250712_tRJ1lL.csv\n",
      "Successfully processed: SA_Campaign_List_20250713_20250713_6OFFvc.csv\n",
      "Successfully processed: SA_Campaign_List_20250714_20250714_BNE6rW.csv\n",
      "Successfully processed: SA_Campaign_List_20250715_20250715_srE3x1.csv\n",
      "Successfully processed: SA_Campaign_List_20250716_20250716_GGWtgv.csv\n",
      "Successfully processed: SA_Campaign_List_20250717_20250717_jARijM.csv\n",
      "Successfully processed: SA_Campaign_List_20250718_20250718_LbxCkC.csv\n",
      "Successfully processed: SA_Campaign_List_20250719_20250719_mRZZlj.csv\n",
      "Successfully processed: SA_Campaign_List_20250720_20250720_dvMDn5.csv\n",
      "Successfully processed: SA_Campaign_List_20250721_20250721_41xtsU.csv\n",
      "Successfully processed: SA_Campaign_List_20250722_20250722_NoROGg.csv\n",
      "Successfully processed: SA_Campaign_List_20250723_20250723_2q4lht.csv\n",
      "Successfully processed: SA_Campaign_List_20250724_20250724_I3dUwA.csv\n",
      "Successfully processed: SA_Campaign_List_20250725_20250725_K6XynZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250726_20250726_WqdjCY.csv\n",
      "Successfully processed: SA_Campaign_List_20250727_20250727_nnZh0B.csv\n",
      "Successfully processed: SA_Campaign_List_20250728_20250728_4tO2Vu.csv\n",
      "Successfully processed: SA_Campaign_List_20250729_20250729_FMvQKE.csv\n",
      "Successfully processed: SA_Campaign_List_20250730_20250730_FhUG8t.csv\n",
      "Successfully processed: SA_Campaign_List_20250731_20250731_LzKDYz.csv\n",
      "Combined 31 files from /Users/thuytrinh/Desktop/Performance-Tracking/Ads-XNurta/Ads M7\n",
      "\n",
      "=== Processing Ads M8 ===\n",
      "Found 23 CSV files in /Users/thuytrinh/Desktop/Performance-Tracking/Ads-XNurta/Ads M8\n",
      "Successfully processed: SA_Campaign_List_20250801_20250801_YntRcO.csv\n",
      "Successfully processed: SA_Campaign_List_20250802_20250802_5gPpW2.csv\n",
      "Successfully processed: SA_Campaign_List_20250803_20250803_TCbuRI.csv\n",
      "Successfully processed: SA_Campaign_List_20250804_20250804_E1v8cc.csv\n",
      "Successfully processed: SA_Campaign_List_20250805_20250805_bo2dYW.csv\n",
      "Successfully processed: SA_Campaign_List_20250806_20250806_gaLM11.csv\n",
      "Successfully processed: SA_Campaign_List_20250807_20250807_bWUkyz.csv\n",
      "Successfully processed: SA_Campaign_List_20250808_20250808_a356tt.csv\n",
      "Successfully processed: SA_Campaign_List_20250809_20250809_DRquad.csv\n",
      "Successfully processed: SA_Campaign_List_20250810_20250810_MyEjRs.csv\n",
      "Successfully processed: SA_Campaign_List_20250811_20250811_PLG4xZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250812_20250812_Ujasyj.csv\n",
      "Successfully processed: SA_Campaign_List_20250813_20250813_YvVtRK.csv\n",
      "Successfully processed: SA_Campaign_List_20250814_20250814_2PJhIJ.csv\n",
      "Successfully processed: SA_Campaign_List_20250815_20250815_osPFPs.csv\n",
      "Successfully processed: SA_Campaign_List_20250816_20250816_33rpjY.csv\n",
      "Successfully processed: SA_Campaign_List_20250817_20250817_IVD3hS.csv\n",
      "Successfully processed: SA_Campaign_List_20250818_20250818_JxUHc7.csv\n",
      "Successfully processed: SA_Campaign_List_20250819_20250819_mg7Asu.csv\n",
      "Successfully processed: SA_Campaign_List_20250820_20250820_igAA7U.csv\n",
      "Successfully processed: SA_Campaign_List_20250821_20250821_u7o1fK.csv\n",
      "Successfully processed: SA_Campaign_List_20250822_20250822_Fv7Vf5.csv\n",
      "Successfully processed: SA_Campaign_List_20250823_20250823_GuKROy.csv\n",
      "Combined 23 files from /Users/thuytrinh/Desktop/Performance-Tracking/Ads-XNurta/Ads M8\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 23713\n",
      "Date range: 2025-07-01 00:00:00 to 2025-08-23 00:00:00\n",
      "Columns: ['ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio', 'Target type', 'Daily Budget', 'Current Budget', 'Bidding Strategy', 'SP Off-site Ads Strategy', 'Top-of-search IS', 'Avg.time in Budget', 'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units', 'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "\n",
      "Data saved to: Combined_Ads_Data_20250824_142156.csv\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "         ASIN       Date Campaign type  \\\n",
      "0  B089QFYLFB 2025-07-01            SP   \n",
      "1  B089QFYLFB 2025-07-01            SP   \n",
      "2  B089QFYLFB 2025-07-01            SP   \n",
      "3  B089QFYLFB 2025-07-01            SP   \n",
      "4  B089QFYLFB 2025-07-01            SP   \n",
      "\n",
      "                                            Campaign  Status Country  \\\n",
      "0  B089QFYLFB_12oz_Fifty fabulous rose_birthday g...  Paused      US   \n",
      "1  B089QFYLFB_12oz_Fifty fabulous rose_asin exp 1...  Paused      US   \n",
      "2  B089QFYLFB_12oz_Fifty fabulous rose_50th birth...  Paused      US   \n",
      "3  B089QFYLFB_12oz_Fifty fabulous rose_women 50th...  Paused      US   \n",
      "4    B089QFYLFB_12oz_Fifty fabulous rose_all key 12h  Paused      US   \n",
      "\n",
      "                                           Portfolio Target type  \\\n",
      "0  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "1  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "2  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "3  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "4  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "\n",
      "   Daily Budget  Current Budget        Bidding Strategy  \\\n",
      "0          15.0             0.0   Dynamic bids and down   \n",
      "1           5.0             0.0   Dynamic bids and down   \n",
      "2          15.0             0.0  Dynamic bids down only   \n",
      "3          10.0             0.0   Dynamic bids and down   \n",
      "4           5.0             0.0  Dynamic bids down only   \n",
      "\n",
      "  SP Off-site Ads Strategy  Top-of-search IS  Avg.time in Budget  Impressions  \\\n",
      "0                  NOT_SET            0.0057                 100         1322   \n",
      "1           MINIMIZE_SPEND            0.0240                 100          430   \n",
      "2                  NOT_SET            0.0025                  60          792   \n",
      "3                  NOT_SET            0.0704                 100          553   \n",
      "4           MINIMIZE_SPEND            0.0008                  70          922   \n",
      "\n",
      "   Clicks     CTR  Spend   CPC  Orders  Sales  Units     CVR    ACOS  ROAS  \\\n",
      "0       5  0.0038  11.92  2.38       2  36.96      2  0.4000  0.3225  3.10   \n",
      "1       3  0.0070   7.20  2.40       0   0.00      0  0.0000     NaN  0.00   \n",
      "2       3  0.0038   6.75  2.25       3  59.94      3  1.0000  0.1126  8.88   \n",
      "3       2  0.0036   5.82  2.91       1  19.98      1  0.5000  0.2913  3.43   \n",
      "4       3  0.0033   5.19  1.73       1  19.98      1  0.3333  0.2598  3.85   \n",
      "\n",
      "    CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU  Orders Other SKU  \\\n",
      "0  5.96           19.98            16.98                1                 1   \n",
      "1  0.00            0.00             0.00                0              <NA>   \n",
      "2  2.25            0.00            59.94                0                 3   \n",
      "3  5.82           19.98             0.00                1              <NA>   \n",
      "4  5.19           19.98             0.00                1              <NA>   \n",
      "\n",
      "   Units Same SKU  Units Other SKU  \n",
      "0               1                1  \n",
      "1               0             <NA>  \n",
      "2               0                3  \n",
      "3               1             <NA>  \n",
      "4               1             <NA>  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process Ads M7 and M8 folders\n",
    "    \"\"\"\n",
    "    # Define folder paths\n",
    "    base_path = \"/Users/thuytrinh/Desktop/Performance-Tracking\"  # Adjust this path as needed\n",
    "    ads_m7_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M7\")\n",
    "    ads_m8_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M8\")\n",
    "    \n",
    "    # Check if folders exist\n",
    "    folders_to_process = []\n",
    "    if os.path.exists(ads_m7_path):\n",
    "        folders_to_process.append((\"Ads M7\", ads_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(ads_m8_path):\n",
    "        folders_to_process.append((\"Ads M8\", ads_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m8_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Combine all data from both folders\n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Sort by Date and ASIN for better organization\n",
    "        final_df = final_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        \n",
    "        # Reset index\n",
    "        final_df = final_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n=== Final Results ===\")\n",
    "        print(f\"Total rows: {len(final_df)}\")\n",
    "        print(f\"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}\")\n",
    "        print(f\"Columns: {list(final_df.columns)}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        output_filename = f\"Combined_Ads_Data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData saved to: {output_filename}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSample data (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        print(final_df.head())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Additional utility functions for ongoing updates\n",
    "def update_with_new_file(existing_df, new_file_path):\n",
    "    \"\"\"\n",
    "    Add new file data to existing DataFrame\n",
    "    \"\"\"\n",
    "    new_df = process_single_csv(new_file_path)\n",
    "    if new_df is not None and not new_df.empty:\n",
    "        # Combine with existing data\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True, sort=False)\n",
    "        # Remove duplicates based on Date and ASIN\n",
    "        updated_df = updated_df.drop_duplicates(subset=['Date', 'ASIN'], keep='last')\n",
    "        # Sort by Date and ASIN\n",
    "        updated_df = updated_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        updated_df = updated_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Successfully added data from {os.path.basename(new_file_path)}\")\n",
    "        return updated_df\n",
    "    else:\n",
    "        print(f\"Failed to process new file: {new_file_path}\")\n",
    "        return existing_df\n",
    "\n",
    "def daily_update(base_df_path, new_file_path):\n",
    "    \"\"\"\n",
    "    Daily update function for adding new data\n",
    "    \"\"\"\n",
    "    # Load existing data\n",
    "    if os.path.exists(base_df_path):\n",
    "        existing_df = pd.read_csv(base_df_path)\n",
    "        existing_df['Date'] = pd.to_datetime(existing_df['Date'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "    \n",
    "    # Add new file data\n",
    "    updated_df = update_with_new_file(existing_df, new_file_path)\n",
    "    \n",
    "    # Save updated data\n",
    "    updated_df.to_csv(base_df_path, index=False)\n",
    "    print(f\"Updated data saved to: {base_df_path}\")\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    result_df = main()\n",
    "    \n",
    "    # Example of how to use daily update:\n",
    "    # daily_update(\"Combined_Ads_Data_20241201_120000.csv\", \"path/to/new/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"/Users/thuytrinh/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_XNurta_H2\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing all July-August files\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_23_24_828).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_23_42_117).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_24_00_897).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_24_20_518).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(03_47_02_350).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(01_24_41_300).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(01_44_34_899).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(01_47_54_443).xlsx\n",
      "🗑️ Cleared metadata for July/August file: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(20_49_05_525).xlsx\n",
      "============================================================\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_17_38_754).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_17_38_754).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_17_11_354).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_17_11_354).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "\n",
      "📈 Combining 54 dataframes...\n",
      "✅ Combined data shape: (12990, 27)\n",
      "📅 Date range: 2025-07-01 00:00:00 to 2025-08-23 00:00:00\n",
      "📤 Uploading to Google Sheets...\n",
      "✅ Successfully uploaded 12990 rows to Google Sheets\n",
      "🔗 Sheet: Raw_SellerBoard_H2\n",
      "\n",
      "🎉 Successfully processed 54 files:\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_17_38_754).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_17_11_354).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 23\n",
      "Other files: 0\n",
      "Total files: 54\n",
      "\n",
      "Last run: 2025-08-24 14:23:03\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            df.columns = [c.strip() for c in df.columns]\n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Move Date column after ASIN\n",
    "            if \"ASIN\" in df.columns and \"Date\" in df.columns:\n",
    "                asin_idx = df.columns.get_loc(\"ASIN\")\n",
    "                cols = list(df.columns)\n",
    "                cols.insert(asin_idx + 1, cols.pop(cols.index(\"Date\")))\n",
    "                df = df[cols]\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date\n",
    "            if \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            # ===== THÊM BƯỚC SẮP XẾP SALES Ở ĐÂY =====\n",
    "            # Sắp xếp Sales giảm dần theo từng ngày\n",
    "            if \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # ===== THÊM BƯỚC CLEAR GIỚI HẠN CỘT Ở ĐÂY =====\n",
    "            # Clear chỉ từ cột A đến AA thay vì clear toàn bộ\n",
    "            self.worksheet.batch_clear(['A:AA'])  # Hoặc có thể dùng range cụ thể như 'A1:AA10000'\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"/Users/thuytrinh/Desktop/Performance-Tracking/Agg-SB\",\n",
    "        'credentials_path': \"/Users/thuytrinh/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SellerBoard_H2\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
