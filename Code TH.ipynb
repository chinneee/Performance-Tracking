{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217392d9",
   "metadata": {},
   "source": [
    "# Monthly Performance (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903bddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44cce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.csv\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    pattern = r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.csv'\n",
    "    match = re.search(pattern, os.path.basename(filename))\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    return None\n",
    "\n",
    "def clean_currency_column(column):\n",
    "    \"\"\"\n",
    "    Remove $ symbol and convert to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Remove $ symbol and any other non-numeric characters except decimal point\n",
    "        cleaned = column.astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "        # Replace empty strings and 'nan' with NaN\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_float(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Replace empty strings and specific text with NaN\n",
    "        cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_int(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to int\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        # Convert to float first, then to int (handling NaN values)\n",
    "        float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "        return float_col.astype('Int64')  # Nullable integer type\n",
    "    return column\n",
    "\n",
    "def extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Extract ASIN from Portfolio string. ASIN is typically 10 characters:\n",
    "    - Pattern 1: B followed by 9 alphanumeric characters (e.g., B08XXXXXXX)\n",
    "    - Pattern 2: 10 alphanumeric characters starting with letters\n",
    "    - Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return None\n",
    "    \n",
    "    portfolio_str = str(portfolio_str)\n",
    "    \n",
    "    # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "    pattern1 = r'B[A-Z0-9]{9}'\n",
    "    match1 = re.search(pattern1, portfolio_str)\n",
    "    if match1:\n",
    "        return match1.group()\n",
    "    \n",
    "    # Pattern 2: 10 alphanumeric characters starting with letter\n",
    "    pattern2 = r'[A-Z][A-Z0-9]{9}'\n",
    "    match2 = re.search(pattern2, portfolio_str)\n",
    "    if match2:\n",
    "        return match2.group()\n",
    "    \n",
    "    # Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    pattern3 = r'[A-Z0-9]{10}'\n",
    "    match3 = re.search(pattern3, portfolio_str)\n",
    "    if match3:\n",
    "        return match3.group()\n",
    "    \n",
    "    # Pattern 4: 10 alphanumeric with possible lowercase (convert to uppercase)\n",
    "    pattern4 = r'[A-Za-z0-9]{10}'\n",
    "    match4 = re.search(pattern4, portfolio_str)\n",
    "    if match4:\n",
    "        return match4.group().upper()\n",
    "    \n",
    "    # If no pattern matches, return first 10 characters as fallback\n",
    "    clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "    if len(clean_str) >= 10:\n",
    "        return clean_str[:10].upper()\n",
    "    \n",
    "    return portfolio_str[:10] if len(portfolio_str) >= 10 else portfolio_str\n",
    "\n",
    "def normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Normalize campaign type keywords\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Define normalization mapping\n",
    "    normalizations = {\n",
    "        'sponsoredBrands': 'SB',\n",
    "        'sponsoredDisplay': 'SD', \n",
    "        'sponsoredProducts': 'SP',\n",
    "        'sponsoredbrands': 'SB',\n",
    "        'sponsoreddisplay': 'SD',\n",
    "        'sponsoredproducts': 'SP',\n",
    "        'Sponsored Brands': 'SB',\n",
    "        'Sponsored Display': 'SD',\n",
    "        'Sponsored Products': 'SP'\n",
    "    }\n",
    "    \n",
    "    # Apply normalizations\n",
    "    for original, normalized in normalizations.items():\n",
    "        text = text.replace(original, normalized)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_single_csv(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file according to specifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        \n",
    "        # Drop specified columns if they exist\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            df = df.drop(columns=existing_columns_to_drop)\n",
    "        \n",
    "        # Add ASIN column as first column (extract ASIN from Portfolio using smart detection)\n",
    "        if 'Portfolio' in df.columns:\n",
    "            df.insert(0, 'ASIN', df['Portfolio'].apply(extract_asin_from_portfolio))\n",
    "        \n",
    "        # Add Date column\n",
    "        df.insert(1, 'Date', date_extracted)\n",
    "        \n",
    "        # Normalize campaign types in Campaign Type column\n",
    "        if 'Campaign type' in df.columns:\n",
    "            df['Campaign type'] = df['Campaign type'].apply(normalize_campaign_types)\n",
    "        \n",
    "        # Clean currency columns (remove $ and convert to float)\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = clean_currency_column(df[col])\n",
    "        \n",
    "        # Convert specified columns to float\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_float(df[col])\n",
    "        \n",
    "        # Convert specified columns to int\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_int(df[col])\n",
    "        \n",
    "        print(f\"Successfully processed: {os.path.basename(file_path)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder\n",
    "    \"\"\"\n",
    "    # Find all CSV files in the folder\n",
    "    csv_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in sorted(csv_files):  # Sort to ensure consistent order\n",
    "        df = process_single_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        print(f\"Combined {len(dataframes)} files from {folder_path}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(f\"No valid data found in {folder_path}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c45f2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Ads M7 ===\n",
      "Found 31 CSV files in /Users/ppcintern/Desktop/Performance-Tracking/Ads-XNurta/Ads M7\n",
      "Successfully processed: SA_Campaign_List_20250701_20250701_K2WWz0.csv\n",
      "Successfully processed: SA_Campaign_List_20250702_20250702_Z3Xspn.csv\n",
      "Successfully processed: SA_Campaign_List_20250703_20250703_3QMabM.csv\n",
      "Successfully processed: SA_Campaign_List_20250704_20250704_VYujNl.csv\n",
      "Successfully processed: SA_Campaign_List_20250705_20250705_xZBIxL.csv\n",
      "Successfully processed: SA_Campaign_List_20250706_20250706_vxMjEt.csv\n",
      "Successfully processed: SA_Campaign_List_20250707_20250707_HLFsmI.csv\n",
      "Successfully processed: SA_Campaign_List_20250708_20250708_jykgJT.csv\n",
      "Successfully processed: SA_Campaign_List_20250709_20250709_gyLSeV.csv\n",
      "Successfully processed: SA_Campaign_List_20250710_20250710_EnigeZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250711_20250711_sVkMfE.csv\n",
      "Successfully processed: SA_Campaign_List_20250712_20250712_tRJ1lL.csv\n",
      "Successfully processed: SA_Campaign_List_20250713_20250713_6OFFvc.csv\n",
      "Successfully processed: SA_Campaign_List_20250714_20250714_BNE6rW.csv\n",
      "Successfully processed: SA_Campaign_List_20250715_20250715_srE3x1.csv\n",
      "Successfully processed: SA_Campaign_List_20250716_20250716_GGWtgv.csv\n",
      "Successfully processed: SA_Campaign_List_20250717_20250717_jARijM.csv\n",
      "Successfully processed: SA_Campaign_List_20250718_20250718_LbxCkC.csv\n",
      "Successfully processed: SA_Campaign_List_20250719_20250719_mRZZlj.csv\n",
      "Successfully processed: SA_Campaign_List_20250720_20250720_dvMDn5.csv\n",
      "Successfully processed: SA_Campaign_List_20250721_20250721_41xtsU.csv\n",
      "Successfully processed: SA_Campaign_List_20250722_20250722_NoROGg.csv\n",
      "Successfully processed: SA_Campaign_List_20250723_20250723_2q4lht.csv\n",
      "Successfully processed: SA_Campaign_List_20250724_20250724_I3dUwA.csv\n",
      "Successfully processed: SA_Campaign_List_20250725_20250725_K6XynZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250726_20250726_WqdjCY.csv\n",
      "Successfully processed: SA_Campaign_List_20250727_20250727_nnZh0B.csv\n",
      "Successfully processed: SA_Campaign_List_20250728_20250728_4tO2Vu.csv\n",
      "Successfully processed: SA_Campaign_List_20250729_20250729_FMvQKE.csv\n",
      "Successfully processed: SA_Campaign_List_20250730_20250730_FhUG8t.csv\n",
      "Successfully processed: SA_Campaign_List_20250731_20250731_LzKDYz.csv\n",
      "Combined 31 files from /Users/ppcintern/Desktop/Performance-Tracking/Ads-XNurta/Ads M7\n",
      "\n",
      "=== Processing Ads M8 ===\n",
      "Found 11 CSV files in /Users/ppcintern/Desktop/Performance-Tracking/Ads-XNurta/Ads M8\n",
      "Successfully processed: SA_Campaign_List_20250801_20250801_YntRcO.csv\n",
      "Successfully processed: SA_Campaign_List_20250802_20250802_5gPpW2.csv\n",
      "Successfully processed: SA_Campaign_List_20250803_20250803_bXqImD.csv\n",
      "Successfully processed: SA_Campaign_List_20250804_20250804_dG57bn.csv\n",
      "Successfully processed: SA_Campaign_List_20250805_20250805_Pq8M11.csv\n",
      "Successfully processed: SA_Campaign_List_20250806_20250806_G8AdE4.csv\n",
      "Successfully processed: SA_Campaign_List_20250807_20250807_jafvdX.csv\n",
      "Successfully processed: SA_Campaign_List_20250808_20250808_GH7nqJ.csv\n",
      "Successfully processed: SA_Campaign_List_20250809_20250809_wgWWUR.csv\n",
      "Successfully processed: SA_Campaign_List_20250810_20250810_nq9Nbz.csv\n",
      "Successfully processed: SA_Campaign_List_20250811_20250811_5mFUAa.csv\n",
      "Combined 11 files from /Users/ppcintern/Desktop/Performance-Tracking/Ads-XNurta/Ads M8\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 19409\n",
      "Date range: 2025-07-01 00:00:00 to 2025-08-11 00:00:00\n",
      "Columns: ['ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio', 'Target type', 'Daily Budget', 'Current Budget', 'Bidding Strategy', 'SP Off-site Ads Strategy', 'Top-of-search IS', 'Avg.time in Budget', 'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units', 'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "\n",
      "Data saved to: Combined_Ads_Data_20250813_144427.csv\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "         ASIN       Date Campaign type  \\\n",
      "0  B089QFYLFB 2025-07-01            SP   \n",
      "1  B089QFYLFB 2025-07-01            SP   \n",
      "2  B089QFYLFB 2025-07-01            SP   \n",
      "3  B089QFYLFB 2025-07-01            SP   \n",
      "4  B089QFYLFB 2025-07-01            SP   \n",
      "\n",
      "                                            Campaign  Status Country  \\\n",
      "0  B089QFYLFB_12oz_Fifty fabulous rose_birthday g...  Paused      US   \n",
      "1  B089QFYLFB_12oz_Fifty fabulous rose_asin exp 1...  Paused      US   \n",
      "2  B089QFYLFB_12oz_Fifty fabulous rose_50th birth...  Paused      US   \n",
      "3  B089QFYLFB_12oz_Fifty fabulous rose_women 50th...  Paused      US   \n",
      "4    B089QFYLFB_12oz_Fifty fabulous rose_all key 12h  Paused      US   \n",
      "\n",
      "                                           Portfolio Target type  \\\n",
      "0  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "1  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "2  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "3  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "4  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "\n",
      "   Daily Budget  Current Budget        Bidding Strategy  \\\n",
      "0          15.0             0.0   Dynamic bids and down   \n",
      "1           5.0             0.0   Dynamic bids and down   \n",
      "2          15.0             0.0  Dynamic bids down only   \n",
      "3          10.0             0.0   Dynamic bids and down   \n",
      "4           5.0             0.0  Dynamic bids down only   \n",
      "\n",
      "  SP Off-site Ads Strategy  Top-of-search IS  Avg.time in Budget  Impressions  \\\n",
      "0                  NOT_SET            0.0057                 100         1322   \n",
      "1           MINIMIZE_SPEND            0.0240                 100          430   \n",
      "2                  NOT_SET            0.0025                  60          792   \n",
      "3                  NOT_SET            0.0704                 100          553   \n",
      "4           MINIMIZE_SPEND            0.0008                  70          922   \n",
      "\n",
      "   Clicks     CTR  Spend   CPC  Orders  Sales  Units     CVR    ACOS  ROAS  \\\n",
      "0       5  0.0038  11.92  2.38       2  36.96      2  0.4000  0.3225  3.10   \n",
      "1       3  0.0070   7.20  2.40       0   0.00      0  0.0000     NaN  0.00   \n",
      "2       3  0.0038   6.75  2.25       3  59.94      3  1.0000  0.1126  8.88   \n",
      "3       2  0.0036   5.82  2.91       1  19.98      1  0.5000  0.2913  3.43   \n",
      "4       3  0.0033   5.19  1.73       1  19.98      1  0.3333  0.2598  3.85   \n",
      "\n",
      "    CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU  Orders Other SKU  \\\n",
      "0  5.96           19.98            16.98                1                 1   \n",
      "1  0.00            0.00             0.00                0              <NA>   \n",
      "2  2.25            0.00            59.94                0                 3   \n",
      "3  5.82           19.98             0.00                1              <NA>   \n",
      "4  5.19           19.98             0.00                1              <NA>   \n",
      "\n",
      "   Units Same SKU  Units Other SKU  \n",
      "0               1                1  \n",
      "1               0             <NA>  \n",
      "2               0                3  \n",
      "3               1             <NA>  \n",
      "4               1             <NA>  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process Ads M7 and M8 folders\n",
    "    \"\"\"\n",
    "    # Define folder paths\n",
    "    base_path = \"/Users/ppcintern/Desktop/Performance-Tracking\"  # Adjust this path as needed\n",
    "    ads_m7_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M7\")\n",
    "    ads_m8_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M8\")\n",
    "    \n",
    "    # Check if folders exist\n",
    "    folders_to_process = []\n",
    "    if os.path.exists(ads_m7_path):\n",
    "        folders_to_process.append((\"Ads M7\", ads_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(ads_m8_path):\n",
    "        folders_to_process.append((\"Ads M8\", ads_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m8_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Combine all data from both folders\n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Sort by Date and ASIN for better organization\n",
    "        final_df = final_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        \n",
    "        # Reset index\n",
    "        final_df = final_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n=== Final Results ===\")\n",
    "        print(f\"Total rows: {len(final_df)}\")\n",
    "        print(f\"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}\")\n",
    "        print(f\"Columns: {list(final_df.columns)}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        output_filename = f\"Combined_Ads_Data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData saved to: {output_filename}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSample data (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        print(final_df.head())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Additional utility functions for ongoing updates\n",
    "def update_with_new_file(existing_df, new_file_path):\n",
    "    \"\"\"\n",
    "    Add new file data to existing DataFrame\n",
    "    \"\"\"\n",
    "    new_df = process_single_csv(new_file_path)\n",
    "    if new_df is not None and not new_df.empty:\n",
    "        # Combine with existing data\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True, sort=False)\n",
    "        # Remove duplicates based on Date and ASIN\n",
    "        updated_df = updated_df.drop_duplicates(subset=['Date', 'ASIN'], keep='last')\n",
    "        # Sort by Date and ASIN\n",
    "        updated_df = updated_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        updated_df = updated_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Successfully added data from {os.path.basename(new_file_path)}\")\n",
    "        return updated_df\n",
    "    else:\n",
    "        print(f\"Failed to process new file: {new_file_path}\")\n",
    "        return existing_df\n",
    "\n",
    "def daily_update(base_df_path, new_file_path):\n",
    "    \"\"\"\n",
    "    Daily update function for adding new data\n",
    "    \"\"\"\n",
    "    # Load existing data\n",
    "    if os.path.exists(base_df_path):\n",
    "        existing_df = pd.read_csv(base_df_path)\n",
    "        existing_df['Date'] = pd.to_datetime(existing_df['Date'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "    \n",
    "    # Add new file data\n",
    "    updated_df = update_with_new_file(existing_df, new_file_path)\n",
    "    \n",
    "    # Save updated data\n",
    "    updated_df.to_csv(base_df_path, index=False)\n",
    "    print(f\"Updated data saved to: {base_df_path}\")\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    result_df = main()\n",
    "    \n",
    "    # Example of how to use daily update:\n",
    "    # daily_update(\"Combined_Ads_Data_20241201_120000.csv\", \"path/to/new/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be59826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nChỉnh sửa định dạng các cột: Daily Budget và Current Budget bỏ kí tự đặc biệt $ và chuyển thành float; \\nChuyển từ object thành float các cột: Avg.time in Budget, Top-of-search IS, CPC, CVR, ACOS, ROAS, \\nChuyển từ object thành int các cột: Orders Other SKU, Units Other SKU\\nThêm cột ASIN làm cột đầu tiên lấy 10 kí tự từ Porfolio\\nThêm cột Date detect từ tên file, định dạng là datetime, biết rằng tên file là SA_Campaign_List_YYYYMMDD_YYYYMMDD_mãhash.csv\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bỏ cột: Profile, Labels, Budget group\n",
    "''' \n",
    "Chỉnh sửa định dạng các cột: Daily Budget và Current Budget bỏ kí tự đặc biệt $ và chuyển thành float; \n",
    "Chuyển từ object thành float các cột: Avg.time in Budget, Top-of-search IS, CPC, CVR, ACOS, ROAS, \n",
    "Chuyển từ object thành int các cột: Orders Other SKU, Units Other SKU\n",
    "Thêm cột ASIN làm cột đầu tiên lấy 10 kí tự từ Porfolio\n",
    "Thêm cột Date detect từ tên file, định dạng là datetime, biết rằng tên file là SA_Campaign_List_YYYYMMDD_YYYYMMDD_mãhash.csv\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"/Users/ppcintern/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_XNurta\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "def39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cead930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/ppcintern/Desktop/Performance-Tracking/Agg-SB/SB M7/NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(22_23_12_947).csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b643cda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>ASIN</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Units</th>\n",
       "      <th>Refunds</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Promo</th>\n",
       "      <th>Ads</th>\n",
       "      <th>Sponsored products (PPC)</th>\n",
       "      <th>Sponsored Display</th>\n",
       "      <th>Sponsored brands (HSA)</th>\n",
       "      <th>Sponsored Brands Video</th>\n",
       "      <th>Google ads</th>\n",
       "      <th>Facebook ads</th>\n",
       "      <th>% Refunds</th>\n",
       "      <th>Sellable Quota</th>\n",
       "      <th>Refund сost</th>\n",
       "      <th>Amazon fees</th>\n",
       "      <th>Cost of Goods</th>\n",
       "      <th>Shipping</th>\n",
       "      <th>Gross profit</th>\n",
       "      <th>Net profit</th>\n",
       "      <th>Estimated payout</th>\n",
       "      <th>Expenses</th>\n",
       "      <th>Margin</th>\n",
       "      <th>ROI</th>\n",
       "      <th>BSR</th>\n",
       "      <th>Real ACOS</th>\n",
       "      <th>Sessions</th>\n",
       "      <th>Unit Session Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewEleven 50th Birthday Gifts For Men Women - ...</td>\n",
       "      <td>B0DH7YGD9Q</td>\n",
       "      <td>EC-NYJH-0YZA</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1 198,8</td>\n",
       "      <td>-2,99</td>\n",
       "      <td>-367,57</td>\n",
       "      <td>-364,86</td>\n",
       "      <td>-2,71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,67</td>\n",
       "      <td>66,67</td>\n",
       "      <td>-63,34</td>\n",
       "      <td>-446,51</td>\n",
       "      <td>-197,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120,99</td>\n",
       "      <td>120,99</td>\n",
       "      <td>311,81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10,09</td>\n",
       "      <td>61,29</td>\n",
       "      <td>2 630</td>\n",
       "      <td>30,66</td>\n",
       "      <td>193.0</td>\n",
       "      <td>31,09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewEleven Retirement Gift For Woman, Men 2025 ...</td>\n",
       "      <td>B0F18PVLJ1</td>\n",
       "      <td>WD-3UXP-TSPV</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1 194,96</td>\n",
       "      <td>-4,6</td>\n",
       "      <td>-333,51</td>\n",
       "      <td>-333,51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,85</td>\n",
       "      <td>100</td>\n",
       "      <td>-38,47</td>\n",
       "      <td>-433,39</td>\n",
       "      <td>-182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202,99</td>\n",
       "      <td>202,99</td>\n",
       "      <td>381,49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16,99</td>\n",
       "      <td>111,53</td>\n",
       "      <td>9 814</td>\n",
       "      <td>27,91</td>\n",
       "      <td>191.0</td>\n",
       "      <td>27,23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewEleven Retirement Gift For Woman 2025 - Coo...</td>\n",
       "      <td>B0DH88L44J</td>\n",
       "      <td>3E-J0TV-6W9S</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>620,46</td>\n",
       "      <td>-4,6</td>\n",
       "      <td>-232,94</td>\n",
       "      <td>-232,94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10,34</td>\n",
       "      <td>50</td>\n",
       "      <td>-59,35</td>\n",
       "      <td>-280,52</td>\n",
       "      <td>-129,92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-86,87</td>\n",
       "      <td>-86,87</td>\n",
       "      <td>38,57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14,00</td>\n",
       "      <td>-66,86</td>\n",
       "      <td>7 329</td>\n",
       "      <td>37,54</td>\n",
       "      <td>177.0</td>\n",
       "      <td>16,38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product        ASIN  \\\n",
       "0  NewEleven 50th Birthday Gifts For Men Women - ...  B0DH7YGD9Q   \n",
       "1  NewEleven Retirement Gift For Woman, Men 2025 ...  B0F18PVLJ1   \n",
       "2  NewEleven Retirement Gift For Woman 2025 - Coo...  B0DH88L44J   \n",
       "\n",
       "            SKU  Units  Refunds     Sales  Promo      Ads  \\\n",
       "0  EC-NYJH-0YZA   60.0      4.0   1 198,8  -2,99  -367,57   \n",
       "1  WD-3UXP-TSPV   52.0      2.0  1 194,96   -4,6  -333,51   \n",
       "2  3E-J0TV-6W9S   29.0      3.0    620,46   -4,6  -232,94   \n",
       "\n",
       "  Sponsored products (PPC) Sponsored Display  Sponsored brands (HSA)  \\\n",
       "0                  -364,86             -2,71                     NaN   \n",
       "1                  -333,51               NaN                     NaN   \n",
       "2                  -232,94               NaN                     NaN   \n",
       "\n",
       "   Sponsored Brands Video  Google ads  Facebook ads % Refunds Sellable Quota  \\\n",
       "0                     NaN         NaN           NaN      6,67          66,67   \n",
       "1                     NaN         NaN           NaN      3,85            100   \n",
       "2                     NaN         NaN           NaN     10,34             50   \n",
       "\n",
       "  Refund сost Amazon fees Cost of Goods  Shipping Gross profit Net profit  \\\n",
       "0      -63,34     -446,51        -197,4       NaN       120,99     120,99   \n",
       "1      -38,47     -433,39          -182       NaN       202,99     202,99   \n",
       "2      -59,35     -280,52       -129,92       NaN       -86,87     -86,87   \n",
       "\n",
       "  Estimated payout  Expenses  Margin     ROI    BSR Real ACOS  Sessions  \\\n",
       "0           311,81       NaN   10,09   61,29  2 630     30,66     193.0   \n",
       "1           381,49       NaN   16,99  111,53  9 814     27,91     191.0   \n",
       "2            38,57       NaN  -14,00  -66,86  7 329     37,54     177.0   \n",
       "\n",
       "  Unit Session Percentage  \n",
       "0                   31,09  \n",
       "1                   27,23  \n",
       "2                   16,38  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fd5edbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChuyển đổi định dạng các cột từ object sang float: Sales, Promo, Ads, Sponsored products (PPC), Sponsored Display, % Refunds, Sellable Quota, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Margin, ROI, BSR, Real ACOS, Unit Session Percentage\\nChuyển đổi định dạng các cột từ object sang int: Units, Refunds\\nThêm cột Date từ tên file vào sau cột ASIN, định dạng là datetime, biết rằng tên file là NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(22_23_12_947).csv\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Chuyển đổi định dạng các cột từ object sang float: Sales, Promo, Ads, Sponsored products (PPC), Sponsored Display, % Refunds, Sellable Quota, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Margin, ROI, BSR, Real ACOS, Unit Session Percentage\n",
    "Chuyển đổi định dạng các cột từ object sang int: Units, Refunds\n",
    "Thêm cột Date từ tên file vào sau cột ASIN, định dạng là datetime, biết rằng tên file là NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(22_23_12_947).csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "466069ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 30 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Product                   239 non-null    object \n",
      " 1   ASIN                      239 non-null    object \n",
      " 2   SKU                       232 non-null    object \n",
      " 3   Units                     131 non-null    float64\n",
      " 4   Refunds                   27 non-null     float64\n",
      " 5   Sales                     131 non-null    object \n",
      " 6   Promo                     31 non-null     object \n",
      " 7   Ads                       86 non-null     object \n",
      " 8   Sponsored products (PPC)  82 non-null     object \n",
      " 9   Sponsored Display         26 non-null     object \n",
      " 10  Sponsored brands (HSA)    0 non-null      float64\n",
      " 11  Sponsored Brands Video    0 non-null      float64\n",
      " 12  Google ads                0 non-null      float64\n",
      " 13  Facebook ads              0 non-null      float64\n",
      " 14  % Refunds                 27 non-null     object \n",
      " 15  Sellable Quota            237 non-null    object \n",
      " 16  Refund сost               40 non-null     object \n",
      " 17  Amazon fees               138 non-null    object \n",
      " 18  Cost of Goods             134 non-null    object \n",
      " 19  Shipping                  0 non-null      float64\n",
      " 20  Gross profit              148 non-null    object \n",
      " 21  Net profit                148 non-null    object \n",
      " 22  Estimated payout          147 non-null    object \n",
      " 23  Expenses                  0 non-null      float64\n",
      " 24  Margin                    131 non-null    object \n",
      " 25  ROI                       134 non-null    object \n",
      " 26  BSR                       238 non-null    object \n",
      " 27  Real ACOS                 79 non-null     object \n",
      " 28  Sessions                  232 non-null    float64\n",
      " 29  Unit Session Percentage   127 non-null    object \n",
      "dtypes: float64(9), object(21)\n",
      "memory usage: 56.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06d30e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: NewEleven_Dashboard Products Group by ASIN_DD_MM_YYYY-DD_MM_YYYY_(HH_MM_SS_MS).csv\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    # Pattern for DD_MM_YYYY-DD_MM_YYYY\n",
    "    pattern = r'NewEleven_Dashboard Products Group by ASIN_(\\d{2}_\\d{2}_\\d{4})-\\d{2}_\\d{2}_\\d{4}_.*\\.csv'\n",
    "    match = re.search(pattern, os.path.basename(filename))\n",
    "    if match:\n",
    "        date_str = match.group(1)  # Get first date DD_MM_YYYY\n",
    "        # Convert DD_MM_YYYY to YYYY-MM-DD format\n",
    "        day, month, year = date_str.split('_')\n",
    "        formatted_date = f\"{year}-{month}-{day}\"\n",
    "        return pd.to_datetime(formatted_date, format='%Y-%m-%d')\n",
    "    return None\n",
    "\n",
    "def clean_percentage_and_currency(column):\n",
    "    \"\"\"\n",
    "    Remove % symbol, $ symbol, commas and convert to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Remove %, $, commas and any other non-numeric characters except decimal point and minus\n",
    "        cleaned = column.astype(str).str.replace(r'[%$,]', '', regex=True)\n",
    "        # Replace empty strings and 'nan' with NaN\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_float(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Remove commas and other non-numeric characters except decimal point and minus\n",
    "        cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_int(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to int\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        # Convert to float first, then to int (handling NaN values)\n",
    "        float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "        return float_col.astype('Int64')  # Nullable integer type\n",
    "    return column\n",
    "\n",
    "def process_single_csv(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file according to Agg-SB specifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try different delimiters and handle malformed CSV\n",
    "        try:\n",
    "            # First try with semicolon delimiter\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', delimiter=';')\n",
    "        except:\n",
    "            try:\n",
    "                # If semicolon fails, try comma\n",
    "                df = pd.read_csv(file_path, encoding='utf-8', delimiter=',')\n",
    "            except:\n",
    "                # If both fail, try with error handling\n",
    "                df = pd.read_csv(file_path, encoding='utf-8', delimiter=';', \n",
    "                               on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        # Remove any completely empty columns\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Clean column names - remove quotes if present\n",
    "        df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        \n",
    "        # Add Date column after ASIN column\n",
    "        if 'ASIN' in df.columns:\n",
    "            asin_index = df.columns.get_loc('ASIN')\n",
    "            df.insert(asin_index + 1, 'Date', date_extracted)\n",
    "        else:\n",
    "            # If no ASIN column, add Date as second column\n",
    "            df.insert(1, 'Date', date_extracted)\n",
    "        \n",
    "        # Define columns to convert to float\n",
    "        float_columns = [\n",
    "            'Sales', 'Promo', 'Ads', 'Sponsored products (PPC)', 'Sponsored Display', \n",
    "            '% Refunds', 'Sellable Quota', 'Refund сost', 'Amazon fees', 'Cost of Goods', \n",
    "            'Gross profit', 'Net profit', 'Estimated payout', 'Margin', 'ROI', 'BSR', \n",
    "            'Real ACOS', 'Unit Session Percentage'\n",
    "        ]\n",
    "        \n",
    "        # Convert float columns\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                if col in ['% Refunds', 'Margin', 'ROI', 'Real ACOS', 'Unit Session Percentage']:\n",
    "                    # These might have % symbol\n",
    "                    df[col] = clean_percentage_and_currency(df[col])\n",
    "                else:\n",
    "                    # Regular float conversion\n",
    "                    df[col] = convert_to_float(df[col])\n",
    "        \n",
    "        # Define columns to convert to int\n",
    "        int_columns = ['Units', 'Refunds']\n",
    "        \n",
    "        # Convert int columns\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_int(df[col])\n",
    "        \n",
    "        print(f\"Successfully processed: {os.path.basename(file_path)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder\n",
    "    \"\"\"\n",
    "    # Find all CSV files in the folder\n",
    "    csv_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in sorted(csv_files):  # Sort to ensure consistent order\n",
    "        df = process_single_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        print(f\"Combined {len(dataframes)} files from {folder_path}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(f\"No valid data found in {folder_path}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "338660b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing SB M7 ===\n",
      "Found 31 CSV files in /Users/ppcintern/Desktop/Performance-Tracking/Agg-SB/SB M7\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(22_23_12_947).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(22_23_40_311).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(22_24_14_109).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(22_24_35_753).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(22_24_58_988).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(22_25_20_814).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(22_25_41_441).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(22_26_06_991).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(22_26_25_578).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(22_26_46_670).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(22_27_03_414).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(22_27_28_356).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(22_27_53_837).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(22_28_12_130).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(22_28_33_788).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(22_29_10_160).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(22_29_33_990).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(22_32_01_252).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(22_32_22_119).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(22_32_46_216).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(22_33_07_235).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(22_33_38_141).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(22_33_57_972).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(22_34_22_123).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(22_34_42_654).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(22_34_59_999).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(22_35_15_996).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(22_35_35_744).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(22_35_54_532).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(22_36_15_634).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(22_36_30_756).csv\n",
      "Combined 31 files from /Users/ppcintern/Desktop/Performance-Tracking/Agg-SB/SB M7\n",
      "\n",
      "=== Processing SB M8 ===\n",
      "Found 11 CSV files in /Users/ppcintern/Desktop/Performance-Tracking/Agg-SB/SB M8\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(22_37_08_847).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(22_37_28_529).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(22_37_49_712).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(22_38_04_520).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(22_38_28_623).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(22_38_47_713).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(22_39_10_511).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(22_39_28_885).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(22_39_46_317).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(22_40_05_867).csv\n",
      "Successfully processed: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(22_40_20_921).csv\n",
      "Combined 11 files from /Users/ppcintern/Desktop/Performance-Tracking/Agg-SB/SB M8\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 10214\n",
      "Date range: 2025-07-01 00:00:00 to 2025-08-11 00:00:00\n",
      "Columns: ['Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', 'Promo', 'Ads', 'Sponsored products (PPC)', 'Sponsored Display', '% Refunds', 'Sellable Quota', 'Refund сost', 'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', 'Estimated payout', 'Margin', 'ROI', 'BSR', 'Real ACOS', 'Sessions', 'Unit Session Percentage']\n",
      "\n",
      "Data saved to: Combined_AggSB_Data_20250813_152444.csv\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                                             Product        ASIN       Date  \\\n",
      "0  NewEleven Gifts For Mom - Gifts For Mother, Wo...  B07Y7ZQ265 2025-07-01   \n",
      "1  NewEleven Fathers Day Gift For Dad From Daught...  B088GX79HT 2025-07-01   \n",
      "2  NewEleven 40th Birthday Gifts For Women - 40th...  B089QFK8NV 2025-07-01   \n",
      "3  NewEleven 50th Birthday Gifts For Women, Her -...  B089QFYLFB 2025-07-01   \n",
      "4  NewEleven Gifts For Men, Dad, Husband, Him - A...  B08CDFWN1Q 2025-07-01   \n",
      "\n",
      "            SKU  Units  Refunds    Sales   Promo     Ads  \\\n",
      "0  F1-10S9-NASK    NaN      NaN      NaN     NaN     NaN   \n",
      "1           NaN    NaN      NaN      NaN     NaN     NaN   \n",
      "2  6O-P3K0-CFHN    1.0      NaN   1499.0     NaN     NaN   \n",
      "3  XH-ELQT-PEJ2   19.0      NaN  37962.0 -1998.0 -5117.0   \n",
      "4  T7-POTQ-PD13   10.0      1.0   1598.0     NaN     NaN   \n",
      "\n",
      "   Sponsored products (PPC)  Sponsored Display  % Refunds  Sellable Quota  \\\n",
      "0                       NaN                NaN        NaN             NaN   \n",
      "1                       NaN                NaN        NaN             NaN   \n",
      "2                       NaN                NaN        NaN             NaN   \n",
      "3                    -506.0              -57.0        NaN             NaN   \n",
      "4                       NaN                NaN     1000.0             NaN   \n",
      "\n",
      "   Refund сost  Amazon fees  Cost of Goods  Gross profit  Net profit  \\\n",
      "0          NaN          NaN            NaN           NaN         NaN   \n",
      "1          NaN          NaN            NaN           NaN         NaN   \n",
      "2          NaN       -663.0         -387.0         449.0       449.0   \n",
      "3          NaN     -13825.0        -6441.0       10581.0     10581.0   \n",
      "4      -1465.0       -637.0         -392.0        4225.0      4225.0   \n",
      "\n",
      "   Estimated payout  Margin      ROI  BSR  Real ACOS  Sessions  \\\n",
      "0               NaN     NaN      NaN  NaN        NaN       9.0   \n",
      "1               NaN     NaN      NaN  NaN        NaN       6.0   \n",
      "2             836.0  2995.0  11602.0  NaN        NaN      18.0   \n",
      "3           17022.0  2787.0  16428.0  NaN     1348.0      40.0   \n",
      "4            8145.0  2644.0  10778.0  NaN        NaN      42.0   \n",
      "\n",
      "   Unit Session Percentage  \n",
      "0                      NaN  \n",
      "1                      NaN  \n",
      "2                    556.0  \n",
      "3                   4750.0  \n",
      "4                   2381.0  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process SB M7 and SB M8 folders in Agg-SB\n",
    "    \"\"\"\n",
    "    # Define folder paths\n",
    "    base_path = \"/Users/ppcintern/Desktop/Performance-Tracking\"  # Adjust this path as needed\n",
    "    agg_sb_path = os.path.join(base_path, \"Agg-SB\")\n",
    "    sb_m7_path = os.path.join(agg_sb_path, \"SB M7\")\n",
    "    sb_m8_path = os.path.join(agg_sb_path, \"SB M8\")\n",
    "    \n",
    "    # Check if folders exist\n",
    "    folders_to_process = []\n",
    "    if os.path.exists(sb_m7_path):\n",
    "        folders_to_process.append((\"SB M7\", sb_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {sb_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(sb_m8_path):\n",
    "        folders_to_process.append((\"SB M8\", sb_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {sb_m8_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Combine all data from both folders\n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Sort by Date and ASIN for better organization\n",
    "        if 'ASIN' in final_df.columns:\n",
    "            final_df = final_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        else:\n",
    "            final_df = final_df.sort_values(['Date'], na_position='last')\n",
    "        \n",
    "        # Reset index\n",
    "        final_df = final_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n=== Final Results ===\")\n",
    "        print(f\"Total rows: {len(final_df)}\")\n",
    "        print(f\"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}\")\n",
    "        print(f\"Columns: {list(final_df.columns)}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        output_filename = f\"Combined_AggSB_Data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData saved to: {output_filename}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSample data (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        print(final_df.head())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Additional utility functions for ongoing updates\n",
    "def update_with_new_file(existing_df, new_file_path):\n",
    "    \"\"\"\n",
    "    Add new file data to existing DataFrame\n",
    "    \"\"\"\n",
    "    new_df = process_single_csv(new_file_path)\n",
    "    if new_df is not None and not new_df.empty:\n",
    "        # Combine with existing data\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True, sort=False)\n",
    "        # Remove duplicates based on Date and ASIN (if ASIN column exists)\n",
    "        if 'ASIN' in updated_df.columns:\n",
    "            updated_df = updated_df.drop_duplicates(subset=['Date', 'ASIN'], keep='last')\n",
    "            # Sort by Date and ASIN\n",
    "            updated_df = updated_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        else:\n",
    "            # If no ASIN, just sort by Date\n",
    "            updated_df = updated_df.sort_values(['Date'], na_position='last')\n",
    "        \n",
    "        updated_df = updated_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Successfully added data from {os.path.basename(new_file_path)}\")\n",
    "        return updated_df\n",
    "    else:\n",
    "        print(f\"Failed to process new file: {new_file_path}\")\n",
    "        return existing_df\n",
    "\n",
    "def daily_update(base_df_path, new_file_path):\n",
    "    \"\"\"\n",
    "    Daily update function for adding new data\n",
    "    \"\"\"\n",
    "    # Load existing data\n",
    "    if os.path.exists(base_df_path):\n",
    "        existing_df = pd.read_csv(base_df_path)\n",
    "        existing_df['Date'] = pd.to_datetime(existing_df['Date'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "    \n",
    "    # Add new file data\n",
    "    updated_df = update_with_new_file(existing_df, new_file_path)\n",
    "    \n",
    "    # Save updated data\n",
    "    updated_df.to_csv(base_df_path, index=False)\n",
    "    print(f\"Updated data saved to: {base_df_path}\")\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    result_df = main()\n",
    "    \n",
    "    # Example of how to use daily update:\n",
    "    # daily_update(\"Combined_AggSB_Data_20241201_120000.csv\", \"path/to/new/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "363fe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"/Users/ppcintern/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_SellerBoard\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
