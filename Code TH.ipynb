{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217392d9",
   "metadata": {},
   "source": [
    "# Monthly Performance (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903bddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.csv\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    pattern = r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.csv'\n",
    "    match = re.search(pattern, os.path.basename(filename))\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    return None\n",
    "\n",
    "def clean_currency_column(column):\n",
    "    \"\"\"\n",
    "    Remove $ symbol and convert to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Remove $ symbol and any other non-numeric characters except decimal point\n",
    "        cleaned = column.astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "        # Replace empty strings and 'nan' with NaN\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_float(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to float\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        # Replace empty strings and specific text with NaN\n",
    "        cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    return column\n",
    "\n",
    "def convert_to_int(column):\n",
    "    \"\"\"\n",
    "    Convert object columns to int\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "        cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "        # Convert to float first, then to int (handling NaN values)\n",
    "        float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "        return float_col.astype('Int64')  # Nullable integer type\n",
    "    return column\n",
    "\n",
    "def extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Extract ASIN from Portfolio string. ASIN is typically 10 characters:\n",
    "    - Pattern 1: B followed by 9 alphanumeric characters (e.g., B08XXXXXXX)\n",
    "    - Pattern 2: 10 alphanumeric characters starting with letters\n",
    "    - Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return None\n",
    "    \n",
    "    portfolio_str = str(portfolio_str)\n",
    "    \n",
    "    # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "    pattern1 = r'B[A-Z0-9]{9}'\n",
    "    match1 = re.search(pattern1, portfolio_str)\n",
    "    if match1:\n",
    "        return match1.group()\n",
    "    \n",
    "    # Pattern 2: 10 alphanumeric characters starting with letter\n",
    "    pattern2 = r'[A-Z][A-Z0-9]{9}'\n",
    "    match2 = re.search(pattern2, portfolio_str)\n",
    "    if match2:\n",
    "        return match2.group()\n",
    "    \n",
    "    # Pattern 3: Any 10 consecutive alphanumeric characters\n",
    "    pattern3 = r'[A-Z0-9]{10}'\n",
    "    match3 = re.search(pattern3, portfolio_str)\n",
    "    if match3:\n",
    "        return match3.group()\n",
    "    \n",
    "    # Pattern 4: 10 alphanumeric with possible lowercase (convert to uppercase)\n",
    "    pattern4 = r'[A-Za-z0-9]{10}'\n",
    "    match4 = re.search(pattern4, portfolio_str)\n",
    "    if match4:\n",
    "        return match4.group().upper()\n",
    "    \n",
    "    # If no pattern matches, return first 10 characters as fallback\n",
    "    clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "    if len(clean_str) >= 10:\n",
    "        return clean_str[:10].upper()\n",
    "    \n",
    "    return portfolio_str[:10] if len(portfolio_str) >= 10 else portfolio_str\n",
    "\n",
    "def normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Normalize campaign type keywords\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Define normalization mapping\n",
    "    normalizations = {\n",
    "        'sponsoredBrands': 'SB',\n",
    "        'sponsoredDisplay': 'SD', \n",
    "        'sponsoredProducts': 'SP',\n",
    "        'sponsoredbrands': 'SB',\n",
    "        'sponsoreddisplay': 'SD',\n",
    "        'sponsoredproducts': 'SP',\n",
    "        'Sponsored Brands': 'SB',\n",
    "        'Sponsored Display': 'SD',\n",
    "        'Sponsored Products': 'SP'\n",
    "    }\n",
    "    \n",
    "    # Apply normalizations\n",
    "    for original, normalized in normalizations.items():\n",
    "        text = text.replace(original, normalized)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_single_csv(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file according to specifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        \n",
    "        # Drop specified columns if they exist\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            df = df.drop(columns=existing_columns_to_drop)\n",
    "        \n",
    "        # Add ASIN column as first column (extract ASIN from Portfolio using smart detection)\n",
    "        if 'Portfolio' in df.columns:\n",
    "            df.insert(0, 'ASIN', df['Portfolio'].apply(extract_asin_from_portfolio))\n",
    "        \n",
    "        # Add Date column\n",
    "        df.insert(1, 'Date', date_extracted)\n",
    "        \n",
    "        # Normalize campaign types in Campaign Type column\n",
    "        if 'Campaign type' in df.columns:\n",
    "            df['Campaign type'] = df['Campaign type'].apply(normalize_campaign_types)\n",
    "        \n",
    "        # Clean currency columns (remove $ and convert to float)\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = clean_currency_column(df[col])\n",
    "        \n",
    "        # Convert specified columns to float\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_float(df[col])\n",
    "        \n",
    "        # Convert specified columns to int\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = convert_to_int(df[col])\n",
    "        \n",
    "        print(f\"Successfully processed: {os.path.basename(file_path)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder\n",
    "    \"\"\"\n",
    "    # Find all CSV files in the folder\n",
    "    csv_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in sorted(csv_files):  # Sort to ensure consistent order\n",
    "        df = process_single_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        print(f\"Combined {len(dataframes)} files from {folder_path}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(f\"No valid data found in {folder_path}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45f2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Ads M7 ===\n",
      "Found 31 CSV files in C:/Users/admin1/Desktop/Performance-Tracking\\Ads-XNurta\\Ads M7\n",
      "Successfully processed: SA_Campaign_List_20250701_20250701_K2WWz0.csv\n",
      "Successfully processed: SA_Campaign_List_20250702_20250702_Z3Xspn.csv\n",
      "Successfully processed: SA_Campaign_List_20250703_20250703_3QMabM.csv\n",
      "Successfully processed: SA_Campaign_List_20250704_20250704_VYujNl.csv\n",
      "Successfully processed: SA_Campaign_List_20250705_20250705_xZBIxL.csv\n",
      "Successfully processed: SA_Campaign_List_20250706_20250706_vxMjEt.csv\n",
      "Successfully processed: SA_Campaign_List_20250707_20250707_HLFsmI.csv\n",
      "Successfully processed: SA_Campaign_List_20250708_20250708_jykgJT.csv\n",
      "Successfully processed: SA_Campaign_List_20250709_20250709_gyLSeV.csv\n",
      "Successfully processed: SA_Campaign_List_20250710_20250710_EnigeZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250711_20250711_sVkMfE.csv\n",
      "Successfully processed: SA_Campaign_List_20250712_20250712_tRJ1lL.csv\n",
      "Successfully processed: SA_Campaign_List_20250713_20250713_6OFFvc.csv\n",
      "Successfully processed: SA_Campaign_List_20250714_20250714_BNE6rW.csv\n",
      "Successfully processed: SA_Campaign_List_20250715_20250715_srE3x1.csv\n",
      "Successfully processed: SA_Campaign_List_20250716_20250716_GGWtgv.csv\n",
      "Successfully processed: SA_Campaign_List_20250717_20250717_jARijM.csv\n",
      "Successfully processed: SA_Campaign_List_20250718_20250718_LbxCkC.csv\n",
      "Successfully processed: SA_Campaign_List_20250719_20250719_mRZZlj.csv\n",
      "Successfully processed: SA_Campaign_List_20250720_20250720_dvMDn5.csv\n",
      "Successfully processed: SA_Campaign_List_20250721_20250721_41xtsU.csv\n",
      "Successfully processed: SA_Campaign_List_20250722_20250722_NoROGg.csv\n",
      "Successfully processed: SA_Campaign_List_20250723_20250723_2q4lht.csv\n",
      "Successfully processed: SA_Campaign_List_20250724_20250724_I3dUwA.csv\n",
      "Successfully processed: SA_Campaign_List_20250725_20250725_K6XynZ.csv\n",
      "Successfully processed: SA_Campaign_List_20250726_20250726_WqdjCY.csv\n",
      "Successfully processed: SA_Campaign_List_20250727_20250727_nnZh0B.csv\n",
      "Successfully processed: SA_Campaign_List_20250728_20250728_4tO2Vu.csv\n",
      "Successfully processed: SA_Campaign_List_20250729_20250729_FMvQKE.csv\n",
      "Successfully processed: SA_Campaign_List_20250730_20250730_FhUG8t.csv\n",
      "Successfully processed: SA_Campaign_List_20250731_20250731_LzKDYz.csv\n",
      "Combined 31 files from C:/Users/admin1/Desktop/Performance-Tracking\\Ads-XNurta\\Ads M7\n",
      "\n",
      "=== Processing Ads M8 ===\n",
      "Found 17 CSV files in C:/Users/admin1/Desktop/Performance-Tracking\\Ads-XNurta\\Ads M8\n",
      "Successfully processed: SA_Campaign_List_20250801_20250801_YntRcO.csv\n",
      "Successfully processed: SA_Campaign_List_20250802_20250802_5gPpW2.csv\n",
      "Successfully processed: SA_Campaign_List_20250803_20250803_Bq7OPW.csv\n",
      "Successfully processed: SA_Campaign_List_20250804_20250804_C5g72G.csv\n",
      "Successfully processed: SA_Campaign_List_20250805_20250805_A99uE9.csv\n",
      "Successfully processed: SA_Campaign_List_20250806_20250806_vTfZqc.csv\n",
      "Successfully processed: SA_Campaign_List_20250807_20250807_c3cNiQ.csv\n",
      "Successfully processed: SA_Campaign_List_20250808_20250808_WC818t.csv\n",
      "Successfully processed: SA_Campaign_List_20250809_20250809_1bZpam.csv\n",
      "Successfully processed: SA_Campaign_List_20250810_20250810_DY9i9h.csv\n",
      "Successfully processed: SA_Campaign_List_20250811_20250811_55NyLA.csv\n",
      "Successfully processed: SA_Campaign_List_20250812_20250812_kpYvfG.csv\n",
      "Successfully processed: SA_Campaign_List_20250813_20250813_rfswbp.csv\n",
      "Successfully processed: SA_Campaign_List_20250814_20250814_PjSX3Y.csv\n",
      "Successfully processed: SA_Campaign_List_20250815_20250815_9V2okf.csv\n",
      "Successfully processed: SA_Campaign_List_20250816_20250816_lShl92.csv\n",
      "Successfully processed: SA_Campaign_List_20250817_20250817_SAobBy.csv\n",
      "Combined 17 files from C:/Users/admin1/Desktop/Performance-Tracking\\Ads-XNurta\\Ads M8\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 21699\n",
      "Date range: 2025-07-01 00:00:00 to 2025-08-17 00:00:00\n",
      "Columns: ['ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio', 'Target type', 'Daily Budget', 'Current Budget', 'Bidding Strategy', 'SP Off-site Ads Strategy', 'Top-of-search IS', 'Avg.time in Budget', 'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units', 'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "\n",
      "Data saved to: Combined_Ads_Data_20250819_143905.csv\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "         ASIN       Date Campaign type  \\\n",
      "0  B089QFYLFB 2025-07-01            SP   \n",
      "1  B089QFYLFB 2025-07-01            SP   \n",
      "2  B089QFYLFB 2025-07-01            SP   \n",
      "3  B089QFYLFB 2025-07-01            SP   \n",
      "4  B089QFYLFB 2025-07-01            SP   \n",
      "\n",
      "                                            Campaign  Status Country  \\\n",
      "0  B089QFYLFB_12oz_Fifty fabulous rose_birthday g...  Paused      US   \n",
      "1  B089QFYLFB_12oz_Fifty fabulous rose_asin exp 1...  Paused      US   \n",
      "2  B089QFYLFB_12oz_Fifty fabulous rose_50th birth...  Paused      US   \n",
      "3  B089QFYLFB_12oz_Fifty fabulous rose_women 50th...  Paused      US   \n",
      "4    B089QFYLFB_12oz_Fifty fabulous rose_all key 12h  Paused      US   \n",
      "\n",
      "                                           Portfolio Target type  \\\n",
      "0  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "1  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "2  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "3  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "4  B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD...      manual   \n",
      "\n",
      "   Daily Budget  Current Budget        Bidding Strategy  \\\n",
      "0          15.0             0.0   Dynamic bids and down   \n",
      "1           5.0             0.0   Dynamic bids and down   \n",
      "2          15.0             0.0  Dynamic bids down only   \n",
      "3          10.0             0.0   Dynamic bids and down   \n",
      "4           5.0             0.0  Dynamic bids down only   \n",
      "\n",
      "  SP Off-site Ads Strategy  Top-of-search IS  Avg.time in Budget  Impressions  \\\n",
      "0                  NOT_SET            0.0057               100.0         1322   \n",
      "1           MINIMIZE_SPEND            0.0240               100.0          430   \n",
      "2                  NOT_SET            0.0025                60.0          792   \n",
      "3                  NOT_SET            0.0704               100.0          553   \n",
      "4           MINIMIZE_SPEND            0.0008                70.0          922   \n",
      "\n",
      "   Clicks     CTR  Spend   CPC  Orders  Sales  Units     CVR    ACOS  ROAS  \\\n",
      "0       5  0.0038  11.92  2.38       2  36.96      2  0.4000  0.3225  3.10   \n",
      "1       3  0.0070   7.20  2.40       0   0.00      0  0.0000     NaN  0.00   \n",
      "2       3  0.0038   6.75  2.25       3  59.94      3  1.0000  0.1126  8.88   \n",
      "3       2  0.0036   5.82  2.91       1  19.98      1  0.5000  0.2913  3.43   \n",
      "4       3  0.0033   5.19  1.73       1  19.98      1  0.3333  0.2598  3.85   \n",
      "\n",
      "    CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU  Orders Other SKU  \\\n",
      "0  5.96           19.98            16.98                1                 1   \n",
      "1  0.00            0.00             0.00                0              <NA>   \n",
      "2  2.25            0.00            59.94                0                 3   \n",
      "3  5.82           19.98             0.00                1              <NA>   \n",
      "4  5.19           19.98             0.00                1              <NA>   \n",
      "\n",
      "   Units Same SKU  Units Other SKU  \n",
      "0               1                1  \n",
      "1               0             <NA>  \n",
      "2               0                3  \n",
      "3               1             <NA>  \n",
      "4               1             <NA>  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process Ads M7 and M8 folders\n",
    "    \"\"\"\n",
    "    # Define folder paths\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking\"  # Adjust this path as needed\n",
    "    ads_m7_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M7\")\n",
    "    ads_m8_path = os.path.join(base_path, \"Ads-XNurta\", \"Ads M8\")\n",
    "    \n",
    "    # Check if folders exist\n",
    "    folders_to_process = []\n",
    "    if os.path.exists(ads_m7_path):\n",
    "        folders_to_process.append((\"Ads M7\", ads_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(ads_m8_path):\n",
    "        folders_to_process.append((\"Ads M8\", ads_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m8_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Combine all data from both folders\n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Sort by Date and ASIN for better organization\n",
    "        final_df = final_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        \n",
    "        # Reset index\n",
    "        final_df = final_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n=== Final Results ===\")\n",
    "        print(f\"Total rows: {len(final_df)}\")\n",
    "        print(f\"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}\")\n",
    "        print(f\"Columns: {list(final_df.columns)}\")\n",
    "        \n",
    "        # Save combined data\n",
    "        output_filename = f\"Combined_Ads_Data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nData saved to: {output_filename}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSample data (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        print(final_df.head())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Additional utility functions for ongoing updates\n",
    "def update_with_new_file(existing_df, new_file_path):\n",
    "    \"\"\"\n",
    "    Add new file data to existing DataFrame\n",
    "    \"\"\"\n",
    "    new_df = process_single_csv(new_file_path)\n",
    "    if new_df is not None and not new_df.empty:\n",
    "        # Combine with existing data\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True, sort=False)\n",
    "        # Remove duplicates based on Date and ASIN\n",
    "        updated_df = updated_df.drop_duplicates(subset=['Date', 'ASIN'], keep='last')\n",
    "        # Sort by Date and ASIN\n",
    "        updated_df = updated_df.sort_values(['Date', 'ASIN'], na_position='last')\n",
    "        updated_df = updated_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Successfully added data from {os.path.basename(new_file_path)}\")\n",
    "        return updated_df\n",
    "    else:\n",
    "        print(f\"Failed to process new file: {new_file_path}\")\n",
    "        return existing_df\n",
    "\n",
    "def daily_update(base_df_path, new_file_path):\n",
    "    \"\"\"\n",
    "    Daily update function for adding new data\n",
    "    \"\"\"\n",
    "    # Load existing data\n",
    "    if os.path.exists(base_df_path):\n",
    "        existing_df = pd.read_csv(base_df_path)\n",
    "        existing_df['Date'] = pd.to_datetime(existing_df['Date'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "    \n",
    "    # Add new file data\n",
    "    updated_df = update_with_new_file(existing_df, new_file_path)\n",
    "    \n",
    "    # Save updated data\n",
    "    updated_df.to_csv(base_df_path, index=False)\n",
    "    print(f\"Updated data saved to: {base_df_path}\")\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    result_df = main()\n",
    "    \n",
    "    # Example of how to use daily update:\n",
    "    # daily_update(\"Combined_Ads_Data_20241201_120000.csv\", \"path/to/new/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_XNurta_H2\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx, already in master\n",
      "⏭️ Skipping NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx, already in master\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(00_21_23_975).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(00_21_43_173).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_23_24_828).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_23_42_117).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_24_00_897).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_24_20_518).xlsx\n",
      "➕ Adding NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_24_36_871).xlsx\n",
      "✅ Master file updated: AggSB_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "    match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "    return None\n",
    "\n",
    "def process_single_excel(file_path):\n",
    "    \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df.dropna(axis=1, how=\"all\")  \n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "        # Extract date from filename\n",
    "        date_val = extract_date_from_filename(os.path.basename(file_path))\n",
    "        if date_val:\n",
    "            df[\"Date\"] = pd.to_datetime(date_val)\n",
    "\n",
    "        # Move Date column after ASIN\n",
    "        if \"ASIN\" in df.columns and \"Date\" in df.columns:\n",
    "            asin_idx = df.columns.get_loc(\"ASIN\")\n",
    "            cols = list(df.columns)\n",
    "            cols.insert(asin_idx + 1, cols.pop(cols.index(\"Date\")))\n",
    "            df = df[cols]\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def append_new_files(base_folder, master_csv_path=\"AggSB_master.csv\"):\n",
    "    \"\"\"\n",
    "    Process ALL subfolders inside Agg-SB\n",
    "    Append new Excel files into one master DataFrame\n",
    "    \"\"\"\n",
    "    # Load existing master\n",
    "    if os.path.exists(master_csv_path):\n",
    "        master_df = pd.read_csv(master_csv_path)\n",
    "        if \"Date\" in master_df.columns:\n",
    "            master_df[\"Date\"] = pd.to_datetime(master_df[\"Date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        master_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through subfolders\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_date = extract_date_from_filename(file)\n",
    "\n",
    "                # Skip if this date already exists\n",
    "                if file_date and \"Date\" in master_df.columns:\n",
    "                    if file_date in master_df[\"Date\"].dt.date.values:\n",
    "                        print(f\"⏭️ Skipping {file}, already in master\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"➕ Adding {file}\")\n",
    "                new_df = process_single_excel(file_path)\n",
    "                if not new_df.empty:\n",
    "                    master_df = pd.concat([master_df, new_df], ignore_index=True, sort=False)\n",
    "\n",
    "    # Save updated master\n",
    "    master_df.to_csv(master_csv_path, index=False)\n",
    "    print(f\"✅ Master file updated: {master_csv_path}\")\n",
    "    return master_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agg_sb_folder = \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB\"\n",
    "    updated_master = append_new_files(agg_sb_folder, \"AggSB_master.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363fe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_SellerBoard_H2\")\n",
    "\n",
    "set_with_dataframe(sheet1, updated_master)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
