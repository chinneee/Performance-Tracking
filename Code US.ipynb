{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd56f6",
   "metadata": {},
   "source": [
    "# Append Selected Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f02d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\ThÃ¡ng 9\n",
      "\n",
      "=== Processing ThÃ¡ng 9 ===\n",
      "Found 12 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\ThÃ¡ng 9\n",
      "Processing: SA_Campaign_List_20251001_20251001_HQ8lrj.xlsx\n",
      "  - Original shape: (433, 33)\n",
      "  - After cleaning: (433, 32)\n",
      "  - Extracted date: 2025-10-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (433, 20)\n",
      "  - Data preservation check: 433 rows maintained\n",
      "Processing: SA_Campaign_List_20251002_20251002_gCGHfj.xlsx\n",
      "  - Original shape: (454, 33)\n",
      "  - After cleaning: (454, 32)\n",
      "  - Extracted date: 2025-10-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (454, 20)\n",
      "  - Data preservation check: 454 rows maintained\n",
      "Processing: SA_Campaign_List_20251003_20251003_Pc2xy3.xlsx\n",
      "  - Original shape: (476, 33)\n",
      "  - After cleaning: (476, 32)\n",
      "  - Extracted date: 2025-10-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (476, 20)\n",
      "  - Data preservation check: 476 rows maintained\n",
      "Processing: SA_Campaign_List_20251004_20251004_pYcP3r.xlsx\n",
      "  - Original shape: (481, 33)\n",
      "  - After cleaning: (481, 32)\n",
      "  - Extracted date: 2025-10-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (481, 20)\n",
      "  - Data preservation check: 481 rows maintained\n",
      "Processing: SA_Campaign_List_20251005_20251005_XaJcQP.xlsx\n",
      "  - Original shape: (498, 33)\n",
      "  - After cleaning: (498, 32)\n",
      "  - Extracted date: 2025-10-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (498, 20)\n",
      "  - Data preservation check: 498 rows maintained\n",
      "Processing: SA_Campaign_List_20251006_20251006_DHZXQY.xlsx\n",
      "  - Original shape: (503, 33)\n",
      "  - After cleaning: (503, 32)\n",
      "  - Extracted date: 2025-10-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (503, 20)\n",
      "  - Data preservation check: 503 rows maintained\n",
      "Processing: SA_Campaign_List_20251007_20251007_uvQDzb.xlsx\n",
      "  - Original shape: (563, 33)\n",
      "  - After cleaning: (563, 32)\n",
      "  - Extracted date: 2025-10-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (563, 20)\n",
      "  - Data preservation check: 563 rows maintained\n",
      "Processing: SA_Campaign_List_20251008_20251008_UUxbCE.xlsx\n",
      "  - Original shape: (552, 33)\n",
      "  - After cleaning: (552, 32)\n",
      "  - Extracted date: 2025-10-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (552, 20)\n",
      "  - Data preservation check: 552 rows maintained\n",
      "Processing: SA_Campaign_List_20251009_20251009_pk9j0O.xlsx\n",
      "  - Original shape: (538, 33)\n",
      "  - After cleaning: (538, 32)\n",
      "  - Extracted date: 2025-10-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (538, 20)\n",
      "  - Data preservation check: 538 rows maintained\n",
      "Processing: SA_Campaign_List_20251010_20251010_ewxras.xlsx\n",
      "  - Original shape: (539, 33)\n",
      "  - After cleaning: (539, 32)\n",
      "  - Extracted date: 2025-10-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (539, 20)\n",
      "  - Data preservation check: 539 rows maintained\n",
      "Processing: SA_Campaign_List_20251011_20251011_ijjq5d.xlsx\n",
      "  - Original shape: (538, 33)\n",
      "  - After cleaning: (538, 32)\n",
      "  - Extracted date: 2025-10-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (538, 20)\n",
      "  - Data preservation check: 538 rows maintained\n",
      "Processing: SA_Campaign_List_20251012_20251012_0fUEzW.xlsx\n",
      "  - Original shape: (532, 33)\n",
      "  - After cleaning: (532, 32)\n",
      "  - Extracted date: 2025-10-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (532, 20)\n",
      "  - Data preservation check: 532 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 12 files\n",
      "  - Failed: 0 files\n",
      "Combined 12 files into 6107 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 6107\n",
      "Total columns: 20\n",
      "Date range: 2025-10-01 00:00:00 to 2025-10-12 00:00:00\n",
      "Unique ASINs: 92\n",
      "Data successfully saved to: Month9_Ads_Data_US_20251013_091213.xlsx\n",
      "\n",
      "=== Uploading to Google Sheet ===\n",
      "Existing rows in sheet: 1174\n",
      "Appending 6107 rows starting from row 1176\n",
      "Adding 6107 new rows to sheet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5524\\2300337423.py:510: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"A{start_row}\", values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended 6107 rows to Google Sheet\n",
      "Google Sheet update completed successfully!\n",
      "\n",
      "============================================================\n",
      "Month 9 processing completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group','ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_US\", \"ThÃ¡ng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing ThÃ¡ng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(\n",
    "            ['Date', 'Sales'], \n",
    "            ascending=[True, False],   # Date â†‘, Sales â†“\n",
    "            na_position='last'\n",
    "        ).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data_US', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    with auto row expansion and safe datetime/NaN handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # XÃ¡c Ä‘á»‹nh row báº¯t Ä‘áº§u append\n",
    "        start_row = existing_rows + 2\n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Copy & xá»­ lÃ½ dá»¯ liá»‡u\n",
    "        safe_df = new_df.copy()\n",
    "        \n",
    "        # Format cá»™t datetime thÃ nh M/D/YYYY\n",
    "        datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "        for col in datetime_cols:\n",
    "            safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "        \n",
    "        # Thay NaN = \"\"\n",
    "        safe_df = safe_df.fillna(\"\")\n",
    "        \n",
    "        # Convert sang list of lists\n",
    "        values = safe_df.values.tolist()\n",
    "        \n",
    "        # ğŸ”¥ Äáº£m báº£o sheet cÃ³ Ä‘á»§ rows\n",
    "        needed_rows = start_row + len(values) - 1\n",
    "        if needed_rows > sheet.row_count:\n",
    "            add_count = needed_rows - sheet.row_count\n",
    "            print(f\"Adding {add_count} new rows to sheet...\")\n",
    "            sheet.add_rows(add_count)\n",
    "        \n",
    "        # Append dá»¯ liá»‡u\n",
    "        sheet.update(f\"A{start_row}\", values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            safe_df = new_df.copy()\n",
    "            datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "            for col in datetime_cols:\n",
    "                safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "            safe_df = safe_df.fillna(\"\")\n",
    "            \n",
    "            set_with_dataframe(sheet, safe_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_US_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"C:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1GpPsWt_fWCfHnEdFQJIsNBebhqFnIiExsHA8SjNUhFk\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q4_2025_US\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "ğŸš€ INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "ğŸ“‹ Available columns: 19/21\n",
      "âš ï¸ Missing columns: ['VAT', 'Shipping']\n",
      "ğŸ”„ Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "ğŸ“Š Processing: NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "ğŸ“‹ Available columns: 18/21\n",
      "âš ï¸ Missing columns: ['Sessions', 'VAT', 'Shipping']\n",
      "\n",
      "ğŸ“ˆ Combining 64 dataframes...\n",
      "âœ… Combined data shape: (15148, 21)\n",
      "ğŸ“… Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "ğŸ“¤ Uploading to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_21800\\935954928.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully uploaded 15148 rows to Google Sheets\n",
      "ğŸ”— Sheet: Raw_SB_H2_2025_US\n",
      "ğŸ“‹ Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund Ñost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "ğŸ‰ Successfully processed 64 files:\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "   âœ“ NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "\n",
      "ğŸ“Š PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 2\n",
      "Total files: 64\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-03 11:28:27\n",
      "        \n",
      "\n",
      "ğŸ“‹ Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund Ñost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Äá»‹nh nghÄ©a thá»© tá»± cá»™t chuáº©n\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund Ñost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # LÃ m sáº¡ch tÃªn cá»™t\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Táº¡o mapping cho cÃ¡c tÃªn cá»™t cÃ³ thá»ƒ khÃ¡c nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # TÃ¬m cá»™t khá»›p chÃ­nh xÃ¡c hoáº·c gáº§n giá»‘ng\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khá»›p chÃ­nh xÃ¡c\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khá»›p má»™t pháº§n cho má»™t sá»‘ trÆ°á»ng há»£p Ä‘áº·c biá»‡t\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'Ñost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cáº§n thiáº¿t\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # ThÃªm cÃ¡c cá»™t thiáº¿u vá»›i giÃ¡ trá»‹ None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sáº¯p xáº¿p láº¡i theo thá»© tá»± chuáº©n\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"ğŸ“‹ Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"âš ï¸ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"ğŸ”„ Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"â• New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"ğŸ”„ Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"â­ï¸ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"ğŸš€ INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"ğŸ—‘ï¸ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"ğŸ”„ INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"ğŸ“Š Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\nğŸ“ˆ Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"âœ… Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"ğŸ“… Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\nğŸ‰ Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   âœ“ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ“¤ Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"âœ… Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"ğŸ”— Sheet: {self.worksheet_name}\")\n",
    "            print(f\"ğŸ“‹ Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ğŸ“Š PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\nğŸ“‹ Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SellerBoard ThÃ¡ng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—“ï¸ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "ğŸ—“ï¸ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "ğŸ“Š Current data rows in sheet: 645\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_01_10_2025-01_10_2025_(2025_10_12_22_29_814).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_02_10_2025-02_10_2025_(2025_10_12_22_30_803).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_03_10_2025-03_10_2025_(2025_10_12_22_30_662).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_04_10_2025-04_10_2025_(2025_10_12_22_31_491).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_05_10_2025-05_10_2025_(2025_10_12_22_31_970).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_06_10_2025-06_10_2025_(2025_10_12_22_32_913).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_07_10_2025-07_10_2025_(2025_10_12_22_32_462).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_08_10_2025-08_10_2025_(2025_10_12_22_32_414).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_09_10_2025-09_10_2025_(2025_10_12_22_32_985).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_10_10_2025-10_10_2025_(2025_10_12_22_33_257).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_11_10_2025-11_10_2025_(2025_10_12_22_33_301).xlsx\n",
      "â• New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_12_22_33_140).xlsx\n",
      "ğŸ“¤ Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5524\\2196521134.py:199: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5524\\2196521134.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully appended 2874 rows to Google Sheets\n",
      "\n",
      "ğŸ“Š SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 12\n",
      "Standard columns: 21\n",
      "Last run: 2025-10-13 12:35:25\n",
      "\n",
      "September files processed:\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_01_10_2025-01_10_2025_(2025_10_12_22_29_814).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_02_10_2025-02_10_2025_(2025_10_12_22_30_803).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_03_10_2025-03_10_2025_(2025_10_12_22_30_662).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_04_10_2025-04_10_2025_(2025_10_12_22_31_491).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_05_10_2025-05_10_2025_(2025_10_12_22_31_970).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_06_10_2025-06_10_2025_(2025_10_12_22_32_913).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_07_10_2025-07_10_2025_(2025_10_12_22_32_462).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_08_10_2025-08_10_2025_(2025_10_12_22_32_414).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_09_10_2025-09_10_2025_(2025_10_12_22_32_985).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_10_10_2025-10_10_2025_(2025_10_12_22_33_257).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_11_10_2025-11_10_2025_(2025_10_12_22_33_301).xlsx\n",
      "  â€¢ NewEleven_Dashboard_Products_Group_by_ASIN_12_10_2025-12_10_2025_(2025_10_12_22_33_140).xlsx\n",
      "        \n",
      "\n",
      "ğŸ“‹ Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund Ñost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Äá»‹nh nghÄ©a thá»© tá»± cá»™t chuáº©n\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund Ñost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho thÃ¡ng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'Ñost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # ThÃªm cÃ¡c cá»™t thiáº¿u\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± chuáº©n\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 10 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chá»‰ xá»­ lÃ½ file thÃ¡ng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"â• New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"ğŸ”„ Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"â­ï¸ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Láº¥y táº¥t cáº£ giÃ¡ trá»‹ trong cá»™t A Ä‘á»ƒ Ä‘áº¿m sá»‘ dÃ²ng cÃ³ dá»¯ liá»‡u\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trá»« Ä‘i header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"ğŸ“Š Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ—“ï¸ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Äáº£m báº£o cá»™t theo Ä‘Ãºng thá»© tá»± chuáº©n\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"ğŸ“¤ Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # Ä‘á»ƒ Sheets giá»¯ trá»‘ng\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"âœ… Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 10 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ğŸ“Š SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  â€¢ {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1GpPsWt_fWCfHnEdFQJIsNBebhqFnIiExsHA8SjNUhFk\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"ğŸ—“ï¸ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\nğŸ“‹ Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"âŒ Operation cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e9972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_excel(\"Month9_SB_Data_US.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
