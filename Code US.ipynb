{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217392d9",
   "metadata": {},
   "source": [
    "# Monthly Performance (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b14709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing with Data Preservation...\n",
      "============================================================\n",
      "Found 3 folders to process\n",
      "\n",
      "=== Processing Tháng 7 ===\n",
      "Found 31 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 7\n",
      "Processing: SA_Campaign_List_20250701_20250701_CNW4yt.xlsx\n",
      "  - Original shape: (675, 33)\n",
      "  - After cleaning: (675, 32)\n",
      "  - Extracted date: 2025-07-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (675, 32)\n",
      "  - Data preservation check: 675 rows maintained\n",
      "Processing: SA_Campaign_List_20250702_20250702_2akRmQ.xlsx\n",
      "  - Original shape: (551, 33)\n",
      "  - After cleaning: (551, 32)\n",
      "  - Extracted date: 2025-07-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (551, 32)\n",
      "  - Data preservation check: 551 rows maintained\n",
      "Processing: SA_Campaign_List_20250703_20250703_W0ZhJk.xlsx\n",
      "  - Original shape: (532, 33)\n",
      "  - After cleaning: (532, 32)\n",
      "  - Extracted date: 2025-07-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (532, 32)\n",
      "  - Data preservation check: 532 rows maintained\n",
      "Processing: SA_Campaign_List_20250704_20250704_08dyuo.xlsx\n",
      "  - Original shape: (506, 33)\n",
      "  - After cleaning: (506, 32)\n",
      "  - Extracted date: 2025-07-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (506, 32)\n",
      "  - Data preservation check: 506 rows maintained\n",
      "Processing: SA_Campaign_List_20250705_20250705_pKy2B5.xlsx\n",
      "  - Original shape: (504, 33)\n",
      "  - After cleaning: (504, 32)\n",
      "  - Extracted date: 2025-07-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (504, 32)\n",
      "  - Data preservation check: 504 rows maintained\n",
      "Processing: SA_Campaign_List_20250706_20250706_JjWjI1.xlsx\n",
      "  - Original shape: (510, 33)\n",
      "  - After cleaning: (510, 32)\n",
      "  - Extracted date: 2025-07-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (510, 32)\n",
      "  - Data preservation check: 510 rows maintained\n",
      "Processing: SA_Campaign_List_20250707_20250707_c0sH3M.xlsx\n",
      "  - Original shape: (472, 33)\n",
      "  - After cleaning: (472, 32)\n",
      "  - Extracted date: 2025-07-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (472, 32)\n",
      "  - Data preservation check: 472 rows maintained\n",
      "Processing: SA_Campaign_List_20250708_20250708_FI3Lhd.xlsx\n",
      "  - Original shape: (496, 33)\n",
      "  - After cleaning: (496, 32)\n",
      "  - Extracted date: 2025-07-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (496, 32)\n",
      "  - Data preservation check: 496 rows maintained\n",
      "Processing: SA_Campaign_List_20250709_20250709_vtQQFP.xlsx\n",
      "  - Original shape: (475, 33)\n",
      "  - After cleaning: (475, 32)\n",
      "  - Extracted date: 2025-07-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (475, 32)\n",
      "  - Data preservation check: 475 rows maintained\n",
      "Processing: SA_Campaign_List_20250710_20250710_6lHFO5.xlsx\n",
      "  - Original shape: (440, 33)\n",
      "  - After cleaning: (440, 32)\n",
      "  - Extracted date: 2025-07-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (440, 32)\n",
      "  - Data preservation check: 440 rows maintained\n",
      "Processing: SA_Campaign_List_20250711_20250711_mfaJlo.xlsx\n",
      "  - Original shape: (403, 33)\n",
      "  - After cleaning: (403, 32)\n",
      "  - Extracted date: 2025-07-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (403, 32)\n",
      "  - Data preservation check: 403 rows maintained\n",
      "Processing: SA_Campaign_List_20250712_20250712_OPyVWF.xlsx\n",
      "  - Original shape: (379, 33)\n",
      "  - After cleaning: (379, 32)\n",
      "  - Extracted date: 2025-07-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (379, 32)\n",
      "  - Data preservation check: 379 rows maintained\n",
      "Processing: SA_Campaign_List_20250713_20250713_louFD6.xlsx\n",
      "  - Original shape: (398, 33)\n",
      "  - After cleaning: (398, 32)\n",
      "  - Extracted date: 2025-07-13 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (398, 32)\n",
      "  - Data preservation check: 398 rows maintained\n",
      "Processing: SA_Campaign_List_20250714_20250714_aC6bHD.xlsx\n",
      "  - Original shape: (412, 33)\n",
      "  - After cleaning: (412, 32)\n",
      "  - Extracted date: 2025-07-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (412, 32)\n",
      "  - Data preservation check: 412 rows maintained\n",
      "Processing: SA_Campaign_List_20250715_20250715_vPt2jW.xlsx\n",
      "  - Original shape: (538, 33)\n",
      "  - After cleaning: (538, 32)\n",
      "  - Extracted date: 2025-07-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (538, 32)\n",
      "  - Data preservation check: 538 rows maintained\n",
      "Processing: SA_Campaign_List_20250716_20250716_jkzgrH.xlsx\n",
      "  - Original shape: (580, 33)\n",
      "  - After cleaning: (580, 32)\n",
      "  - Extracted date: 2025-07-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (580, 32)\n",
      "  - Data preservation check: 580 rows maintained\n",
      "Processing: SA_Campaign_List_20250717_20250717_zjFW3K.xlsx\n",
      "  - Original shape: (579, 33)\n",
      "  - After cleaning: (579, 32)\n",
      "  - Extracted date: 2025-07-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (579, 32)\n",
      "  - Data preservation check: 579 rows maintained\n",
      "Processing: SA_Campaign_List_20250718_20250718_MVGJ8X.xlsx\n",
      "  - Original shape: (534, 33)\n",
      "  - After cleaning: (534, 32)\n",
      "  - Extracted date: 2025-07-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (534, 32)\n",
      "  - Data preservation check: 534 rows maintained\n",
      "Processing: SA_Campaign_List_20250719_20250719_xKHNWY.xlsx\n",
      "  - Original shape: (521, 33)\n",
      "  - After cleaning: (521, 32)\n",
      "  - Extracted date: 2025-07-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (521, 32)\n",
      "  - Data preservation check: 521 rows maintained\n",
      "Processing: SA_Campaign_List_20250720_20250720_BkGmc5.xlsx\n",
      "  - Original shape: (508, 33)\n",
      "  - After cleaning: (508, 32)\n",
      "  - Extracted date: 2025-07-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (508, 32)\n",
      "  - Data preservation check: 508 rows maintained\n",
      "Processing: SA_Campaign_List_20250721_20250721_vLBvsQ.xlsx\n",
      "  - Original shape: (549, 33)\n",
      "  - After cleaning: (549, 32)\n",
      "  - Extracted date: 2025-07-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (549, 32)\n",
      "  - Data preservation check: 549 rows maintained\n",
      "Processing: SA_Campaign_List_20250722_20250722_LBZesQ.xlsx\n",
      "  - Original shape: (500, 33)\n",
      "  - After cleaning: (500, 32)\n",
      "  - Extracted date: 2025-07-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (500, 32)\n",
      "  - Data preservation check: 500 rows maintained\n",
      "Processing: SA_Campaign_List_20250723_20250723_WXbWly.xlsx\n",
      "  - Original shape: (509, 33)\n",
      "  - After cleaning: (509, 32)\n",
      "  - Extracted date: 2025-07-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (509, 32)\n",
      "  - Data preservation check: 509 rows maintained\n",
      "Processing: SA_Campaign_List_20250724_20250724_1d8wGi.xlsx\n",
      "  - Original shape: (503, 33)\n",
      "  - After cleaning: (503, 32)\n",
      "  - Extracted date: 2025-07-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (503, 32)\n",
      "  - Data preservation check: 503 rows maintained\n",
      "Processing: SA_Campaign_List_20250725_20250725_K1xXeF.xlsx\n",
      "  - Original shape: (486, 33)\n",
      "  - After cleaning: (486, 32)\n",
      "  - Extracted date: 2025-07-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (486, 32)\n",
      "  - Data preservation check: 486 rows maintained\n",
      "Processing: SA_Campaign_List_20250726_20250726_zhI5fA.xlsx\n",
      "  - Original shape: (464, 33)\n",
      "  - After cleaning: (464, 32)\n",
      "  - Extracted date: 2025-07-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (464, 32)\n",
      "  - Data preservation check: 464 rows maintained\n",
      "Processing: SA_Campaign_List_20250727_20250727_sOnDfC.xlsx\n",
      "  - Original shape: (467, 33)\n",
      "  - After cleaning: (467, 32)\n",
      "  - Extracted date: 2025-07-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (467, 32)\n",
      "  - Data preservation check: 467 rows maintained\n",
      "Processing: SA_Campaign_List_20250728_20250728_MKABca.xlsx\n",
      "  - Original shape: (448, 33)\n",
      "  - After cleaning: (448, 32)\n",
      "  - Extracted date: 2025-07-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (448, 32)\n",
      "  - Data preservation check: 448 rows maintained\n",
      "Processing: SA_Campaign_List_20250729_20250729_8hvkPD.xlsx\n",
      "  - Original shape: (439, 33)\n",
      "  - After cleaning: (439, 32)\n",
      "  - Extracted date: 2025-07-29 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (439, 32)\n",
      "  - Data preservation check: 439 rows maintained\n",
      "Processing: SA_Campaign_List_20250730_20250730_xkV24m.xlsx\n",
      "  - Original shape: (428, 33)\n",
      "  - After cleaning: (428, 32)\n",
      "  - Extracted date: 2025-07-30 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (428, 32)\n",
      "  - Data preservation check: 428 rows maintained\n",
      "Processing: SA_Campaign_List_20250731_20250731_k4waFa.xlsx\n",
      "  - Original shape: (413, 33)\n",
      "  - After cleaning: (413, 32)\n",
      "  - Extracted date: 2025-07-31 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (413, 32)\n",
      "  - Data preservation check: 413 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 31 files\n",
      "  - Failed: 0 files\n",
      "Combined 31 files into 15219 total rows\n",
      "\n",
      "=== Processing Tháng 8 ===\n",
      "Found 31 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 8\n",
      "Processing: SA_Campaign_List_20250801_20250801_ODVMwJ.xlsx\n",
      "  - Original shape: (420, 33)\n",
      "  - After cleaning: (420, 32)\n",
      "  - Extracted date: 2025-08-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (420, 32)\n",
      "  - Data preservation check: 420 rows maintained\n",
      "Processing: SA_Campaign_List_20250802_20250802_pdqbqo.xlsx\n",
      "  - Original shape: (422, 33)\n",
      "  - After cleaning: (422, 32)\n",
      "  - Extracted date: 2025-08-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (422, 32)\n",
      "  - Data preservation check: 422 rows maintained\n",
      "Processing: SA_Campaign_List_20250803_20250803_757eu7.xlsx\n",
      "  - Original shape: (423, 33)\n",
      "  - After cleaning: (423, 32)\n",
      "  - Extracted date: 2025-08-03 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (423, 32)\n",
      "  - Data preservation check: 423 rows maintained\n",
      "Processing: SA_Campaign_List_20250804_20250804_v9LHCg.xlsx\n",
      "  - Original shape: (383, 33)\n",
      "  - After cleaning: (383, 32)\n",
      "  - Extracted date: 2025-08-04 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (383, 32)\n",
      "  - Data preservation check: 383 rows maintained\n",
      "Processing: SA_Campaign_List_20250805_20250805_UTTEFl.xlsx\n",
      "  - Original shape: (379, 33)\n",
      "  - After cleaning: (379, 32)\n",
      "  - Extracted date: 2025-08-05 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (379, 32)\n",
      "  - Data preservation check: 379 rows maintained\n",
      "Processing: SA_Campaign_List_20250806_20250806_tt1UAD.xlsx\n",
      "  - Original shape: (359, 33)\n",
      "  - After cleaning: (359, 32)\n",
      "  - Extracted date: 2025-08-06 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (359, 32)\n",
      "  - Data preservation check: 359 rows maintained\n",
      "Processing: SA_Campaign_List_20250807_20250807_83DLWZ.xlsx\n",
      "  - Original shape: (354, 33)\n",
      "  - After cleaning: (354, 32)\n",
      "  - Extracted date: 2025-08-07 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (354, 32)\n",
      "  - Data preservation check: 354 rows maintained\n",
      "Processing: SA_Campaign_List_20250808_20250808_7jZ5uD.xlsx\n",
      "  - Original shape: (350, 33)\n",
      "  - After cleaning: (350, 32)\n",
      "  - Extracted date: 2025-08-08 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (350, 32)\n",
      "  - Data preservation check: 350 rows maintained\n",
      "Processing: SA_Campaign_List_20250809_20250809_V6OrOz.xlsx\n",
      "  - Original shape: (350, 33)\n",
      "  - After cleaning: (350, 32)\n",
      "  - Extracted date: 2025-08-09 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (350, 32)\n",
      "  - Data preservation check: 350 rows maintained\n",
      "Processing: SA_Campaign_List_20250810_20250810_ziMZFP.xlsx\n",
      "  - Original shape: (375, 33)\n",
      "  - After cleaning: (375, 32)\n",
      "  - Extracted date: 2025-08-10 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (375, 32)\n",
      "  - Data preservation check: 375 rows maintained\n",
      "Processing: SA_Campaign_List_20250811_20250811_H6LBYK.xlsx\n",
      "  - Original shape: (375, 33)\n",
      "  - After cleaning: (375, 32)\n",
      "  - Extracted date: 2025-08-11 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (375, 32)\n",
      "  - Data preservation check: 375 rows maintained\n",
      "Processing: SA_Campaign_List_20250812_20250812_pqVwu9.xlsx\n",
      "  - Original shape: (389, 33)\n",
      "  - After cleaning: (389, 32)\n",
      "  - Extracted date: 2025-08-12 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (389, 32)\n",
      "  - Data preservation check: 389 rows maintained\n",
      "Processing: SA_Campaign_List_20250813_20250813_83tOqn.xlsx\n",
      "  - Original shape: (369, 33)\n",
      "  - After cleaning: (369, 32)\n",
      "  - Extracted date: 2025-08-13 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (369, 32)\n",
      "  - Data preservation check: 369 rows maintained\n",
      "Processing: SA_Campaign_List_20250814_20250814_MTGB0G.xlsx\n",
      "  - Original shape: (387, 33)\n",
      "  - After cleaning: (387, 32)\n",
      "  - Extracted date: 2025-08-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (387, 32)\n",
      "  - Data preservation check: 387 rows maintained\n",
      "Processing: SA_Campaign_List_20250815_20250815_MEnGN8.xlsx\n",
      "  - Original shape: (386, 33)\n",
      "  - After cleaning: (386, 32)\n",
      "  - Extracted date: 2025-08-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (386, 32)\n",
      "  - Data preservation check: 386 rows maintained\n",
      "Processing: SA_Campaign_List_20250816_20250816_xNyU1X.xlsx\n",
      "  - Original shape: (376, 33)\n",
      "  - After cleaning: (376, 32)\n",
      "  - Extracted date: 2025-08-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (376, 32)\n",
      "  - Data preservation check: 376 rows maintained\n",
      "Processing: SA_Campaign_List_20250817_20250817_LSkH2D.xlsx\n",
      "  - Original shape: (383, 33)\n",
      "  - After cleaning: (383, 32)\n",
      "  - Extracted date: 2025-08-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (383, 32)\n",
      "  - Data preservation check: 383 rows maintained\n",
      "Processing: SA_Campaign_List_20250818_20250818_KJOIni.xlsx\n",
      "  - Original shape: (343, 33)\n",
      "  - After cleaning: (343, 32)\n",
      "  - Extracted date: 2025-08-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (343, 32)\n",
      "  - Data preservation check: 343 rows maintained\n",
      "Processing: SA_Campaign_List_20250819_20250819_BjSKl0.xlsx\n",
      "  - Original shape: (344, 33)\n",
      "  - After cleaning: (344, 32)\n",
      "  - Extracted date: 2025-08-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (344, 32)\n",
      "  - Data preservation check: 344 rows maintained\n",
      "Processing: SA_Campaign_List_20250820_20250820_bUIZ5d.xlsx\n",
      "  - Original shape: (331, 33)\n",
      "  - After cleaning: (331, 32)\n",
      "  - Extracted date: 2025-08-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (331, 32)\n",
      "  - Data preservation check: 331 rows maintained\n",
      "Processing: SA_Campaign_List_20250821_20250821_VAhWXA.xlsx\n",
      "  - Original shape: (339, 33)\n",
      "  - After cleaning: (339, 32)\n",
      "  - Extracted date: 2025-08-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (339, 32)\n",
      "  - Data preservation check: 339 rows maintained\n",
      "Processing: SA_Campaign_List_20250822_20250822_xbWhJE.xlsx\n",
      "  - Original shape: (331, 33)\n",
      "  - After cleaning: (331, 32)\n",
      "  - Extracted date: 2025-08-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (331, 32)\n",
      "  - Data preservation check: 331 rows maintained\n",
      "Processing: SA_Campaign_List_20250823_20250823_3Sr0fy.xlsx\n",
      "  - Original shape: (326, 33)\n",
      "  - After cleaning: (326, 32)\n",
      "  - Extracted date: 2025-08-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (326, 32)\n",
      "  - Data preservation check: 326 rows maintained\n",
      "Processing: SA_Campaign_List_20250824_20250824_lko04t.xlsx\n",
      "  - Original shape: (325, 33)\n",
      "  - After cleaning: (325, 32)\n",
      "  - Extracted date: 2025-08-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (325, 32)\n",
      "  - Data preservation check: 325 rows maintained\n",
      "Processing: SA_Campaign_List_20250825_20250825_pbuxXB.xlsx\n",
      "  - Original shape: (318, 33)\n",
      "  - After cleaning: (318, 32)\n",
      "  - Extracted date: 2025-08-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (318, 32)\n",
      "  - Data preservation check: 318 rows maintained\n",
      "Processing: SA_Campaign_List_20250826_20250826_oz83uE.xlsx\n",
      "  - Original shape: (317, 33)\n",
      "  - After cleaning: (317, 32)\n",
      "  - Extracted date: 2025-08-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (317, 32)\n",
      "  - Data preservation check: 317 rows maintained\n",
      "Processing: SA_Campaign_List_20250827_20250827_6nNdV8.xlsx\n",
      "  - Original shape: (308, 33)\n",
      "  - After cleaning: (308, 32)\n",
      "  - Extracted date: 2025-08-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (308, 32)\n",
      "  - Data preservation check: 308 rows maintained\n",
      "Processing: SA_Campaign_List_20250828_20250828_WQrZWL.xlsx\n",
      "  - Original shape: (299, 33)\n",
      "  - After cleaning: (299, 32)\n",
      "  - Extracted date: 2025-08-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (299, 32)\n",
      "  - Data preservation check: 299 rows maintained\n",
      "Processing: SA_Campaign_List_20250829_20250829_KDJ6s3.xlsx\n",
      "  - Original shape: (274, 33)\n",
      "  - After cleaning: (274, 32)\n",
      "  - Extracted date: 2025-08-29 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (274, 32)\n",
      "  - Data preservation check: 274 rows maintained\n",
      "Processing: SA_Campaign_List_20250830_20250830_oOIRl4.xlsx\n",
      "  - Original shape: (270, 33)\n",
      "  - After cleaning: (270, 32)\n",
      "  - Extracted date: 2025-08-30 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (270, 32)\n",
      "  - Data preservation check: 270 rows maintained\n",
      "Processing: SA_Campaign_List_20250831_20250831_bJrF9h.xlsx\n",
      "  - Original shape: (268, 33)\n",
      "  - After cleaning: (268, 32)\n",
      "  - Extracted date: 2025-08-31 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (268, 32)\n",
      "  - Data preservation check: 268 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 31 files\n",
      "  - Failed: 0 files\n",
      "Combined 31 files into 10873 total rows\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 2 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 9\n",
      "Processing: SA_Campaign_List_20250901_20250901_Cugr13.xlsx\n",
      "  - Original shape: (272, 33)\n",
      "  - After cleaning: (272, 32)\n",
      "  - Extracted date: 2025-09-01 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (272, 32)\n",
      "  - Data preservation check: 272 rows maintained\n",
      "Processing: SA_Campaign_List_20250902_20250902_i4tLnT.xlsx\n",
      "  - Original shape: (273, 33)\n",
      "  - After cleaning: (273, 32)\n",
      "  - Extracted date: 2025-09-02 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels']\n",
      "Warning: ACOS - too many conversion failures, keeping original\n",
      "Warning: Orders Other SKU - too many conversion failures, keeping original\n",
      "Warning: Units Other SKU - too many conversion failures, keeping original\n",
      "  - Preserved extra column as: Extra_Target type\n",
      "  - Preserved extra column as: Extra_Current Budget\n",
      "  - Preserved extra column as: Extra_SP Off-site Ads Strategy\n",
      "  - Final shape: (273, 32)\n",
      "  - Data preservation check: 273 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 2 files\n",
      "  - Failed: 0 files\n",
      "Combined 2 files into 545 total rows\n",
      "\n",
      "=== Combining Data from All Folders ===\n",
      "Successfully combined data from 3 folders\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Final Results ===\n",
      "Total rows: 26637\n",
      "Total columns: 32\n",
      "  - Tháng 7: 15219 rows\n",
      "  - Tháng 8: 10873 rows\n",
      "  - Tháng 9: 545 rows\n",
      "Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "Unique ASINs: 143\n",
      "Data successfully saved to: Combined_Ads_Data_Safe_20250903_121643.xlsx\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      ASIN       Date Campaign type                                                                Campaign Status Country                                           Portfolio  Daily Budget      Bidding Strategy  Top-of-search IS  Avg.time in Budget  Impressions  Clicks    CTR  Spend  CPC  Orders  Sales  Units  CVR    ACOS  ROAS  CPA  Sales Same SKU  Sales Other SKU  Orders Same SKU Orders Other SKU  Units Same SKU Units Other SKU Extra_Target type  Extra_Current Budget Extra_SP Off-site Ads Strategy\n",
      "B089QFYLFB 2025-07-01            SP       B089QFYLFB_12oz_Fifty fabulous rose_women 50th birthday gifts_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN          10.0 Dynamic bids and down            0.0704                 100          553       2 0.0036   5.82 2.91       1  19.98      1  0.5  0.2913  3.43 5.82           19.98              0.0                1               --               1              --            manual                   0.0                        NOT_SET\n",
      "B089QFYLFB 2025-07-01            SP B089QFYLFB_12oz_Fifty fabulous rose_happy 50th birthday decorations_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN           5.0 Dynamic bids and down            0.0117                 100           25       1 0.0400   2.20 2.20       0   0.00      0  0.0      --  0.00 0.00            0.00              0.0                0               --               0              --            manual                   0.0                        NOT_SET\n",
      "B089QFYLFB 2025-07-01            SP  B089QFYLFB_12oz_Fifty fabulous rose_fifty birthday gifts for women_b,p Paused      US B089QFYLFB_TUMBLER 12_FIFTY FABULOUS ROSE GOLD_NGAN           5.0 Dynamic bids and down            0.0882                 100            6       0 0.0000   0.00  NaN       0   0.00      0  NaN      --   NaN 0.00            0.00              0.0                0               --               0              --            manual                   0.0                        NOT_SET\n",
      "\n",
      "=== Column List ===\n",
      " 1. ASIN\n",
      " 2. Date\n",
      " 3. Campaign type\n",
      " 4. Campaign\n",
      " 5. Status\n",
      " 6. Country\n",
      " 7. Portfolio\n",
      " 8. Daily Budget\n",
      " 9. Bidding Strategy\n",
      "10. Top-of-search IS\n",
      "11. Avg.time in Budget\n",
      "12. Impressions\n",
      "13. Clicks\n",
      "14. CTR\n",
      "15. Spend\n",
      "16. CPC\n",
      "17. Orders\n",
      "18. Sales\n",
      "19. Units\n",
      "20. CVR\n",
      "21. ACOS\n",
      "22. ROAS\n",
      "23. CPA\n",
      "24. Sales Same SKU\n",
      "25. Sales Other SKU\n",
      "26. Orders Same SKU\n",
      "27. Orders Other SKU\n",
      "28. Units Same SKU\n",
      "29. Units Other SKU\n",
      "30. Extra_Target type\n",
      "31. Extra_Current Budget\n",
      "32. Extra_SP Off-site Ads Strategy\n",
      "\n",
      "============================================================\n",
      "Processing completed with data preservation safeguards!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'ACOS', 'ROAS', 'CPA', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Orders Other SKU', 'Units Other SKU', 'Orders Same SKU', 'Units Same SKU', \n",
    "                      'Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU'\n",
    "        ]\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # Add any additional columns that weren't in the required list (preserve extra data)\n",
    "        extra_columns = [col for col in df.columns if col not in required_columns and col not in ['ASIN', 'Date']]\n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process Ads folders with data preservation\n",
    "    \"\"\"\n",
    "    # Define folder paths - FIXED: Variable naming issue\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m7_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 7\")\n",
    "    ads_m8_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 8\")\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 9\")  # FIXED: Was overwriting ads_m8_path\n",
    "    \n",
    "    # Check if folders exist and prepare processing list\n",
    "    folders_to_process = []\n",
    "    \n",
    "    if os.path.exists(ads_m7_path):\n",
    "        folders_to_process.append((\"Tháng 7\", ads_m7_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m7_path} not found\")\n",
    "    \n",
    "    if os.path.exists(ads_m8_path):\n",
    "        folders_to_process.append((\"Tháng 8\", ads_m8_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m8_path} not found\")\n",
    "        \n",
    "    if os.path.exists(ads_m9_path):\n",
    "        folders_to_process.append((\"Tháng 9\", ads_m9_path))\n",
    "    else:\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "    \n",
    "    if not folders_to_process:\n",
    "        print(\"No valid folders found. Please check your paths.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(folders_to_process)} folders to process\")\n",
    "    \n",
    "    # Process each folder\n",
    "    all_dataframes = []\n",
    "    processing_summary = []\n",
    "    \n",
    "    for folder_name, folder_path in folders_to_process:\n",
    "        print(f\"\\n=== Processing {folder_name} ===\")\n",
    "        df = process_folder(folder_path)\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "            processing_summary.append(f\"{folder_name}: {len(df)} rows\")\n",
    "        else:\n",
    "            processing_summary.append(f\"{folder_name}: No data\")\n",
    "    \n",
    "    # Combine all data from folders\n",
    "    if all_dataframes:\n",
    "        print(f\"\\n=== Combining Data from All Folders ===\")\n",
    "        try:\n",
    "            final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully combined data from {len(all_dataframes)} folders\")\n",
    "            \n",
    "            # Safe duplicate removal (only if key columns exist)\n",
    "            if 'ASIN' in final_df.columns and 'Campaign' in final_df.columns and 'Date' in final_df.columns:\n",
    "                original_rows = len(final_df)\n",
    "                final_df = final_df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "                removed_duplicates = original_rows - len(final_df)\n",
    "                print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "            \n",
    "            # Safe sorting\n",
    "            try:\n",
    "                final_df = final_df.sort_values(['ASIN', 'Date'], na_position='last')\n",
    "                final_df = final_df.reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error sorting data: {e}\")\n",
    "            \n",
    "            print(f\"\\n=== Final Results ===\")\n",
    "            print(f\"Total rows: {len(final_df)}\")\n",
    "            print(f\"Total columns: {len(final_df.columns)}\")\n",
    "            \n",
    "            for summary in processing_summary:\n",
    "                print(f\"  - {summary}\")\n",
    "            \n",
    "            if 'Date' in final_df.columns:\n",
    "                try:\n",
    "                    date_min = final_df['Date'].min()\n",
    "                    date_max = final_df['Date'].max()\n",
    "                    print(f\"Date range: {date_min} to {date_max}\")\n",
    "                except:\n",
    "                    print(\"Date range: Unable to determine\")\n",
    "            \n",
    "            if 'ASIN' in final_df.columns:\n",
    "                try:\n",
    "                    unique_asins = final_df['ASIN'].nunique()\n",
    "                    print(f\"Unique ASINs: {unique_asins}\")\n",
    "                except:\n",
    "                    print(\"Unique ASINs: Unable to determine\")\n",
    "            \n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error combining data: {e}\")\n",
    "            # Return the largest DataFrame as fallback\n",
    "            if all_dataframes:\n",
    "                largest_df = max(all_dataframes, key=len)\n",
    "                print(f\"Returning largest DataFrame with {len(largest_df)} rows as fallback\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(\"No data to process.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Keep all the other utility functions unchanged\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Combined_Ads_Data', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def display_sample(df, rows=3):\n",
    "    \"\"\"Display sample data with proper formatting\"\"\"\n",
    "    try:\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 20)\n",
    "        print(df.head(rows).to_string(index=False))\n",
    "        pd.reset_option('display.max_columns')\n",
    "        pd.reset_option('display.width')\n",
    "        pd.reset_option('display.max_colwidth')\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying sample: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main processing\n",
    "    print(\"Starting Amazon Ads Data Processing with Data Preservation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result_df = main()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save combined data\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Combined_Ads_Data_Safe_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            # Display sample\n",
    "            print(f\"\\nSample data (first 3 rows):\")\n",
    "            display_sample(result_df)\n",
    "            \n",
    "            print(f\"\\n=== Column List ===\")\n",
    "            for i, col in enumerate(result_df.columns, 1):\n",
    "                print(f\"{i:2d}. {col}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Processing completed with data preservation safeguards!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "          \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_file(\"c:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Mở Google Sheet\n",
    "sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "\n",
    "# Mở file Google Sheet (Spreadsheet object)\n",
    "spreadsheet = client.open_by_key(sheet_id)\n",
    "sheet1 = client.open_by_key(sheet_id).worksheet(\"Raw_XN_Q3_2025_US\")\n",
    "\n",
    "set_with_dataframe(sheet1, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd56f6",
   "metadata": {},
   "source": [
    "# Append Selected Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f02d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amazon Ads Data Processing - Month 9 Only with Append...\n",
      "============================================================\n",
      "Processing only Month 9 data from: C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 9\n",
      "\n",
      "=== Processing Tháng 9 ===\n",
      "Found 15 Excel files in C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\\H2_2025_US\\Tháng 9\n",
      "Processing: SA_Campaign_List_20250914_20250914_2PaT2O.xlsx\n",
      "  - Original shape: (423, 33)\n",
      "  - After cleaning: (423, 32)\n",
      "  - Extracted date: 2025-09-14 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (423, 20)\n",
      "  - Data preservation check: 423 rows maintained\n",
      "Processing: SA_Campaign_List_20250915_20250915_74tLfO.xlsx\n",
      "  - Original shape: (421, 33)\n",
      "  - After cleaning: (421, 32)\n",
      "  - Extracted date: 2025-09-15 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (421, 20)\n",
      "  - Data preservation check: 421 rows maintained\n",
      "Processing: SA_Campaign_List_20250916_20250916_hLinnD.xlsx\n",
      "  - Original shape: (416, 33)\n",
      "  - After cleaning: (416, 32)\n",
      "  - Extracted date: 2025-09-16 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (416, 20)\n",
      "  - Data preservation check: 416 rows maintained\n",
      "Processing: SA_Campaign_List_20250917_20250917_hV1ZNY.xlsx\n",
      "  - Original shape: (432, 33)\n",
      "  - After cleaning: (432, 32)\n",
      "  - Extracted date: 2025-09-17 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (432, 20)\n",
      "  - Data preservation check: 432 rows maintained\n",
      "Processing: SA_Campaign_List_20250918_20250918_VU29D4.xlsx\n",
      "  - Original shape: (447, 33)\n",
      "  - After cleaning: (447, 32)\n",
      "  - Extracted date: 2025-09-18 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (447, 20)\n",
      "  - Data preservation check: 447 rows maintained\n",
      "Processing: SA_Campaign_List_20250919_20250919_8WFz5v.xlsx\n",
      "  - Original shape: (408, 33)\n",
      "  - After cleaning: (408, 32)\n",
      "  - Extracted date: 2025-09-19 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (408, 20)\n",
      "  - Data preservation check: 408 rows maintained\n",
      "Processing: SA_Campaign_List_20250920_20250920_bRfW8H.xlsx\n",
      "  - Original shape: (381, 33)\n",
      "  - After cleaning: (381, 32)\n",
      "  - Extracted date: 2025-09-20 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (381, 20)\n",
      "  - Data preservation check: 381 rows maintained\n",
      "Processing: SA_Campaign_List_20250921_20250921_iaYOi0.xlsx\n",
      "  - Original shape: (384, 33)\n",
      "  - After cleaning: (384, 32)\n",
      "  - Extracted date: 2025-09-21 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (384, 20)\n",
      "  - Data preservation check: 384 rows maintained\n",
      "Processing: SA_Campaign_List_20250922_20250922_9TtELO.xlsx\n",
      "  - Original shape: (381, 33)\n",
      "  - After cleaning: (381, 32)\n",
      "  - Extracted date: 2025-09-22 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (381, 20)\n",
      "  - Data preservation check: 381 rows maintained\n",
      "Processing: SA_Campaign_List_20250923_20250923_rCq7O9.xlsx\n",
      "  - Original shape: (381, 33)\n",
      "  - After cleaning: (381, 32)\n",
      "  - Extracted date: 2025-09-23 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (381, 20)\n",
      "  - Data preservation check: 381 rows maintained\n",
      "Processing: SA_Campaign_List_20250924_20250924_ygPvqp.xlsx\n",
      "  - Original shape: (402, 33)\n",
      "  - After cleaning: (402, 32)\n",
      "  - Extracted date: 2025-09-24 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (402, 20)\n",
      "  - Data preservation check: 402 rows maintained\n",
      "Processing: SA_Campaign_List_20250925_20250925_q6uOCM.xlsx\n",
      "  - Original shape: (360, 33)\n",
      "  - After cleaning: (360, 32)\n",
      "  - Extracted date: 2025-09-25 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (360, 20)\n",
      "  - Data preservation check: 360 rows maintained\n",
      "Processing: SA_Campaign_List_20250926_20250926_rSTZOt.xlsx\n",
      "  - Original shape: (344, 33)\n",
      "  - After cleaning: (344, 32)\n",
      "  - Extracted date: 2025-09-26 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (344, 20)\n",
      "  - Data preservation check: 344 rows maintained\n",
      "Processing: SA_Campaign_List_20250927_20250927_NleFCI.xlsx\n",
      "  - Original shape: (334, 33)\n",
      "  - After cleaning: (334, 32)\n",
      "  - Extracted date: 2025-09-27 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (334, 20)\n",
      "  - Data preservation check: 334 rows maintained\n",
      "Processing: SA_Campaign_List_20250928_20250928_HJWnYR.xlsx\n",
      "  - Original shape: (358, 33)\n",
      "  - After cleaning: (358, 32)\n",
      "  - Extracted date: 2025-09-28 00:00:00\n",
      "  - Dropped columns: ['Profile', 'Labels', 'ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU', 'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
      "  - Excluded extra columns: ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
      "  - Final shape: (358, 20)\n",
      "  - Data preservation check: 358 rows maintained\n",
      "\n",
      "Processing Summary:\n",
      "  - Successful: 15 files\n",
      "  - Failed: 0 files\n",
      "Combined 15 files into 5872 total rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "=== Month 9 Results ===\n",
      "Total rows: 5872\n",
      "Total columns: 20\n",
      "Date range: 2025-09-14 00:00:00 to 2025-09-28 00:00:00\n",
      "Unique ASINs: 77\n",
      "Data successfully saved to: Month9_Ads_Data_US_20250929_143653.xlsx\n",
      "\n",
      "=== Uploading to Google Sheet ===\n",
      "Existing rows in sheet: 30752\n",
      "Appending 5872 rows starting from row 30754\n",
      "Adding 5862 new rows to sheet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5944\\136178942.py:510: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"A{start_row}\", values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended 5872 rows to Google Sheet\n",
      "Google Sheet update completed successfully!\n",
      "\n",
      "============================================================\n",
      "Month 9 processing completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date from filename pattern: SA_Campaign_List_YYYYMMDD_YYYYMMDD_hash.xlsx\n",
    "    Returns the first date (start date)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'SA_Campaign_List_(\\d{8})_\\d{8}_.*\\.xlsx',  # Original pattern\n",
    "        r'(\\d{8})',  # Any 8-digit date\n",
    "        r'(\\d{2}_\\d{2}_\\d{4})',  # DD_MM_YYYY format\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD format\n",
    "    ]\n",
    "    \n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, basename)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                if '_' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%d_%m_%Y')\n",
    "                elif '-' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                else:\n",
    "                    return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: use file modification date\n",
    "    mod_time = os.path.getmtime(filename)\n",
    "    return pd.to_datetime(datetime.fromtimestamp(mod_time).date())\n",
    "\n",
    "def safe_clean_currency_column(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely remove $ symbol and convert to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[C$,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning currency column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_float(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to float, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[%,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            result = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = result.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting float column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_convert_to_int(column, original_column_name):\n",
    "    \"\"\"\n",
    "    Safely convert object columns to int, preserve original on error\n",
    "    \"\"\"\n",
    "    if column.dtype == 'object':\n",
    "        try:\n",
    "            cleaned = column.astype(str).str.replace(r'[,]', '', regex=True)\n",
    "            cleaned = cleaned.replace(['', 'nan', 'NaN', '--', 'N/A'], np.nan)\n",
    "            cleaned = cleaned.infer_objects(copy=False)\n",
    "            \n",
    "            # Convert to float first, then to int (handling NaN values)\n",
    "            float_col = pd.to_numeric(cleaned, errors='coerce')\n",
    "            \n",
    "            # Check if too many values became NaN (more than 50% loss)\n",
    "            valid_original = column.notna().sum()\n",
    "            valid_converted = float_col.notna().sum()\n",
    "            \n",
    "            if valid_original > 0 and (valid_converted / valid_original) < 0.5:\n",
    "                print(f\"Warning: {original_column_name} - too many conversion failures, keeping original\")\n",
    "                return column\n",
    "            \n",
    "            return float_col.astype('Int64')  # Nullable integer type\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error converting int column {original_column_name}: {e}\")\n",
    "            return column\n",
    "    return column\n",
    "\n",
    "def safe_extract_asin_from_portfolio(portfolio_str):\n",
    "    \"\"\"\n",
    "    Safely extract ASIN from Portfolio string, return original if extraction fails\n",
    "    \"\"\"\n",
    "    if pd.isna(portfolio_str) or portfolio_str == '':\n",
    "        return portfolio_str  # Keep original NaN or empty\n",
    "    \n",
    "    try:\n",
    "        portfolio_str = str(portfolio_str).strip()\n",
    "        \n",
    "        # Pattern 1: B + 9 alphanumeric (most common ASIN format)\n",
    "        pattern1 = r'B[A-Z0-9]{9}'\n",
    "        match1 = re.search(pattern1, portfolio_str.upper())\n",
    "        if match1:\n",
    "            return match1.group()\n",
    "        \n",
    "        # Pattern 2: Any 10 consecutive alphanumeric characters\n",
    "        pattern2 = r'[A-Z0-9]{10}'\n",
    "        match2 = re.search(pattern2, portfolio_str.upper())\n",
    "        if match2:\n",
    "            return match2.group()\n",
    "        \n",
    "        # Pattern 3: Extract first 10 alphanumeric characters\n",
    "        clean_str = re.sub(r'[^A-Za-z0-9]', '', portfolio_str)\n",
    "        if len(clean_str) >= 10:\n",
    "            return clean_str[:10].upper()\n",
    "        \n",
    "        # If no valid ASIN found, return original value\n",
    "        return portfolio_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting ASIN from '{portfolio_str}': {e}\")\n",
    "        return portfolio_str\n",
    "\n",
    "def safe_normalize_campaign_types(text):\n",
    "    \"\"\"\n",
    "    Safely normalize campaign type keywords, preserve original on error\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        text = str(text)\n",
    "        \n",
    "        normalizations = {\n",
    "            'sponsoredBrands': 'SB',\n",
    "            'sponsoredDisplay': 'SD', \n",
    "            'sponsoredProducts': 'SP',\n",
    "            'sponsoredbrands': 'SB',\n",
    "            'sponsoreddisplay': 'SD',\n",
    "            'sponsoredproducts': 'SP',\n",
    "            'Sponsored Brands': 'SB',\n",
    "            'Sponsored Display': 'SD',\n",
    "            'Sponsored Products': 'SP'\n",
    "        }\n",
    "        \n",
    "        for original, normalized in normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing campaign type '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "def process_single_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Process a single XLSX file with data preservation safeguards - MODIFIED to exclude specific extra columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        original_shape = df.shape\n",
    "        print(f\"  - Original shape: {original_shape}\")\n",
    "        \n",
    "        # Remove completely empty rows and columns (but be conservative)\n",
    "        df_cleaned = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        \n",
    "        # Check if we lost too many rows (safety check)\n",
    "        if len(df_cleaned) < len(df) * 0.9:  # If we lose more than 10% of rows\n",
    "            print(f\"  Warning: Potential data loss in row cleaning, using original data\")\n",
    "            df_cleaned = df\n",
    "        \n",
    "        df = df_cleaned\n",
    "        print(f\"  - After cleaning: {df.shape}\")\n",
    "        \n",
    "        # Clean column names safely\n",
    "        original_columns = df.columns.tolist()\n",
    "        try:\n",
    "            df.columns = [str(col).strip() for col in df.columns]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error cleaning column names: {e}\")\n",
    "            df.columns = original_columns\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date_extracted = extract_date_from_filename(file_path)\n",
    "        print(f\"  - Extracted date: {date_extracted}\")\n",
    "        \n",
    "        # Drop specified columns if they exist (but safely)\n",
    "        columns_to_drop = ['Profile', 'Labels', 'Budget group','ACOS', 'ROAS', 'CPA', 'Sales Same SKU', 'Sales Other SKU',\n",
    "            'Orders Same SKU', 'Orders Other SKU', 'Units Same SKU', 'Units Other SKU']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns_to_drop:\n",
    "            try:\n",
    "                df = df.drop(columns=existing_columns_to_drop)\n",
    "                print(f\"  - Dropped columns: {existing_columns_to_drop}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error dropping columns: {e}\")\n",
    "        \n",
    "        # Create ASIN column from Portfolio (safely)\n",
    "        asin_values = None\n",
    "        if 'Portfolio' in df.columns:\n",
    "            try:\n",
    "                asin_values = df['Portfolio'].apply(safe_extract_asin_from_portfolio)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error creating ASIN column: {e}\")\n",
    "                asin_values = df['Portfolio']  # Use original Portfolio column\n",
    "        else:\n",
    "            # If no Portfolio column, create empty ASIN column\n",
    "            asin_values = [None] * len(df)\n",
    "        \n",
    "        # Create Date column\n",
    "        date_values = [date_extracted] * len(df)\n",
    "        \n",
    "        # Normalize campaign types safely\n",
    "        if 'Campaign type' in df.columns:\n",
    "            try:\n",
    "                df['Campaign type'] = df['Campaign type'].apply(safe_normalize_campaign_types)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error normalizing campaign types: {e}\")\n",
    "        \n",
    "        # Clean currency columns safely\n",
    "        currency_columns = ['Daily Budget', 'Current Budget']\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_clean_currency_column(df[col], col)\n",
    "        \n",
    "        # Convert float columns safely\n",
    "        float_columns = ['Avg.time in Budget', 'Top-of-search IS', 'CPC', 'CVR', 'CTR']\n",
    "        for col in float_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_float(df[col], col)\n",
    "        \n",
    "        # Convert int columns safely\n",
    "        int_columns = ['Impressions', 'Clicks', 'Orders', 'Units']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = safe_convert_to_int(df[col], col)\n",
    "        \n",
    "        # Define exact required columns only\n",
    "        required_columns = [\n",
    "            'ASIN', 'Date', 'Campaign type', 'Campaign', 'Status', 'Country', 'Portfolio',\n",
    "            'Daily Budget', 'Bidding Strategy', 'Top-of-search IS', 'Avg.time in Budget',\n",
    "            'Impressions', 'Clicks', 'CTR', 'Spend', 'CPC', 'Orders', 'Sales', 'Units',\n",
    "            'CVR'\n",
    "        ]\n",
    "        \n",
    "        # MODIFIED: Define columns to exclude from extra columns\n",
    "        excluded_extra_columns = ['Target type', 'Current Budget', 'SP Off-site Ads Strategy']\n",
    "        \n",
    "        # Create new DataFrame with required columns (preserve all data)\n",
    "        ordered_df = pd.DataFrame()\n",
    "        \n",
    "        # Add ASIN as first column\n",
    "        ordered_df['ASIN'] = asin_values\n",
    "        \n",
    "        # Add Date as second column\n",
    "        ordered_df['Date'] = date_values\n",
    "        \n",
    "        # Add remaining required columns in specified order\n",
    "        for col in required_columns[2:]:  # Skip ASIN and Date since already added\n",
    "            if col in df.columns:\n",
    "                ordered_df[col] = df[col]\n",
    "            else:\n",
    "                ordered_df[col] = np.nan  # Add missing columns with NaN\n",
    "                print(f\"  - Missing column filled with NaN: {col}\")\n",
    "        \n",
    "        # MODIFIED: Add any additional columns that weren't in the required list (preserve extra data, but exclude specified columns)\n",
    "        extra_columns = [col for col in df.columns \n",
    "                        if col not in required_columns \n",
    "                        and col not in ['ASIN', 'Date'] \n",
    "                        and col not in excluded_extra_columns]  # NEW: Exclude unwanted columns\n",
    "        \n",
    "        for col in extra_columns:\n",
    "            new_col_name = f\"Extra_{col}\"  # Prefix to identify extra columns\n",
    "            ordered_df[new_col_name] = df[col]\n",
    "            print(f\"  - Preserved extra column as: {new_col_name}\")\n",
    "        \n",
    "        # Print excluded columns for transparency\n",
    "        excluded_found = [col for col in excluded_extra_columns if col in df.columns]\n",
    "        if excluded_found:\n",
    "            print(f\"  - Excluded extra columns: {excluded_found}\")\n",
    "        \n",
    "        final_shape = ordered_df.shape\n",
    "        print(f\"  - Final shape: {final_shape}\")\n",
    "        print(f\"  - Data preservation check: {len(ordered_df)} rows maintained\")\n",
    "        \n",
    "        return ordered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        print(\"Attempting to return minimal processed data to avoid total loss...\")\n",
    "        \n",
    "        # Last resort: return basic DataFrame with original data\n",
    "        try:\n",
    "            basic_df = pd.read_excel(file_path)\n",
    "            # Just add Date column and return\n",
    "            basic_df['Date'] = extract_date_from_filename(file_path)\n",
    "            return basic_df\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XLSX files in a folder with data preservation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find all XLSX files\n",
    "    xlsx_pattern = os.path.join(folder_path, \"*.xlsx\")\n",
    "    xlsx_files = glob.glob(xlsx_pattern)\n",
    "    \n",
    "    # Filter out temporary Excel files\n",
    "    xlsx_files = [f for f in xlsx_files if not os.path.basename(f).startswith('~')]\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No Excel files found in {folder_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} Excel files in {folder_path}\")\n",
    "    \n",
    "    # Process each file and collect DataFrames\n",
    "    dataframes = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in sorted(xlsx_files):  # Sort for consistent order\n",
    "        df = process_single_xlsx(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "            successful_files.append(os.path.basename(file_path))\n",
    "        else:\n",
    "            failed_files.append(os.path.basename(file_path))\n",
    "    \n",
    "    # Report processing results\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  - Successful: {len(successful_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"  - Failed files: {failed_files}\")\n",
    "    \n",
    "    # Combine all DataFrames safely\n",
    "    if dataframes:\n",
    "        try:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            print(f\"Combined {len(successful_files)} files into {len(combined_df)} total rows\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining DataFrames: {e}\")\n",
    "            # Try to return the largest DataFrame as fallback\n",
    "            if dataframes:\n",
    "                largest_df = max(dataframes, key=len)\n",
    "                print(f\"Returning largest individual DataFrame with {len(largest_df)} rows\")\n",
    "                return largest_df\n",
    "    \n",
    "    print(f\"No valid data found in {folder_path}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main_month9_only():\n",
    "    \"\"\"\n",
    "    MODIFIED: Main function to process ONLY Month 9 data\n",
    "    \"\"\"\n",
    "    # Define folder paths - ONLY Month 9\n",
    "    base_path = \"C:/Users/admin1/Desktop/Performance-Tracking/Ads-XNurta\"\n",
    "    ads_m9_path = os.path.join(base_path, \"H2_2025_US\", \"Tháng 9\")\n",
    "    \n",
    "    # Check if Month 9 folder exists\n",
    "    if not os.path.exists(ads_m9_path):\n",
    "        print(f\"Warning: {ads_m9_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Processing only Month 9 data from: {ads_m9_path}\")\n",
    "    \n",
    "    # Process Month 9 folder only\n",
    "    print(f\"\\n=== Processing Tháng 9 ===\")\n",
    "    df = process_folder(ads_m9_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found for Month 9\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Safe duplicate removal (only if key columns exist)\n",
    "    if 'ASIN' in df.columns and 'Campaign' in df.columns and 'Date' in df.columns:\n",
    "        original_rows = len(df)\n",
    "        df = df.drop_duplicates(subset=['ASIN', 'Date', 'Campaign'], keep='last')\n",
    "        removed_duplicates = original_rows - len(df)\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # Safe sorting\n",
    "    try:\n",
    "        df = df.sort_values(\n",
    "            ['Date', 'Sales'], \n",
    "            ascending=[True, False],   # Date ↑, Sales ↓\n",
    "            na_position='last'\n",
    "        ).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error sorting data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Month 9 Results ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        try:\n",
    "            date_min = df['Date'].min()\n",
    "            date_max = df['Date'].max()\n",
    "            print(f\"Date range: {date_min} to {date_max}\")\n",
    "        except:\n",
    "            print(\"Date range: Unable to determine\")\n",
    "    \n",
    "    if 'ASIN' in df.columns:\n",
    "        try:\n",
    "            unique_asins = df['ASIN'].nunique()\n",
    "            print(f\"Unique ASINs: {unique_asins}\")\n",
    "        except:\n",
    "            print(\"Unique ASINs: Unable to determine\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, filename):\n",
    "    \"\"\"Save DataFrame to Excel with proper formatting\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Month9_Ads_Data_US', index=False)\n",
    "            print(f\"Data successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def get_existing_row_count(sheet):\n",
    "    \"\"\"\n",
    "    Get the number of existing rows in the Google Sheet (excluding header)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_values = sheet.get_all_values()\n",
    "        # Return count minus header row (assuming first row is header)\n",
    "        return len(all_values) - 1 if len(all_values) > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting row count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def append_to_google_sheet(sheet, new_df, existing_rows):\n",
    "    \"\"\"\n",
    "    Append new data to Google Sheet starting from the next available row\n",
    "    with auto row expansion and safe datetime/NaN handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if new_df.empty:\n",
    "            print(\"No data to append\")\n",
    "            return\n",
    "        \n",
    "        # Xác định row bắt đầu append\n",
    "        start_row = existing_rows + 2\n",
    "        print(f\"Appending {len(new_df)} rows starting from row {start_row}\")\n",
    "        \n",
    "        # Copy & xử lý dữ liệu\n",
    "        safe_df = new_df.copy()\n",
    "        \n",
    "        # Format cột datetime thành M/D/YYYY\n",
    "        datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "        for col in datetime_cols:\n",
    "            safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "        \n",
    "        # Thay NaN = \"\"\n",
    "        safe_df = safe_df.fillna(\"\")\n",
    "        \n",
    "        # Convert sang list of lists\n",
    "        values = safe_df.values.tolist()\n",
    "        \n",
    "        # 🔥 Đảm bảo sheet có đủ rows\n",
    "        needed_rows = start_row + len(values) - 1\n",
    "        if needed_rows > sheet.row_count:\n",
    "            add_count = needed_rows - sheet.row_count\n",
    "            print(f\"Adding {add_count} new rows to sheet...\")\n",
    "            sheet.add_rows(add_count)\n",
    "        \n",
    "        # Append dữ liệu\n",
    "        sheet.update(f\"A{start_row}\", values)\n",
    "        print(f\"Successfully appended {len(new_df)} rows to Google Sheet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to Google Sheet: {e}\")\n",
    "        try:\n",
    "            print(\"Trying fallback method...\")\n",
    "            safe_df = new_df.copy()\n",
    "            datetime_cols = safe_df.select_dtypes(include=[\"datetime64[ns]\", \"datetimetz\"]).columns\n",
    "            for col in datetime_cols:\n",
    "                safe_df[col] = safe_df[col].dt.strftime(\"%-m/%-d/%Y\")\n",
    "            safe_df = safe_df.fillna(\"\")\n",
    "            \n",
    "            set_with_dataframe(sheet, safe_df, row=start_row, include_column_header=False)\n",
    "            print(\"Fallback append completed\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback method also failed: {e2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # MODIFIED: Run only Month 9 processing and append to Google Sheet\n",
    "    print(\"Starting Amazon Ads Data Processing - Month 9 Only with Append...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process Month 9 data\n",
    "    result_df = main_month9_only()\n",
    "    \n",
    "    if not result_df.empty:\n",
    "        try:\n",
    "            # Save to local Excel file\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_filename = f\"Month9_Ads_Data_US_{timestamp}.xlsx\"\n",
    "            save_to_excel(result_df, output_filename)\n",
    "            \n",
    "            \n",
    "            # MODIFIED: Connect to Google Sheets and append data\n",
    "            print(f\"\\n=== Uploading to Google Sheet ===\")\n",
    "            \n",
    "            try:\n",
    "                scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                          \"https://www.googleapis.com/auth/drive\"]\n",
    "                creds = Credentials.from_service_account_file(\"C:/Users/admin1/Downloads/new_credential.json\", scopes=scopes)\n",
    "                client = gspread.authorize(creds)\n",
    "\n",
    "                # Open Google Sheet\n",
    "                sheet_id = \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\"\n",
    "                spreadsheet = client.open_by_key(sheet_id)\n",
    "                sheet1 = spreadsheet.worksheet(\"Raw_XN_Q3_2025_US\")\n",
    "                \n",
    "                # Get existing row count\n",
    "                existing_rows = get_existing_row_count(sheet1)\n",
    "                print(f\"Existing rows in sheet: {existing_rows}\")\n",
    "                \n",
    "                # Append new data\n",
    "                append_to_google_sheet(sheet1, result_df, existing_rows)\n",
    "                \n",
    "                print(\"Google Sheet update completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "                print(\"Local Excel file has been saved successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in final processing: {e}\")\n",
    "    else:\n",
    "        print(\"No data processed for Month 9\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Month 9 processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe770be",
   "metadata": {},
   "source": [
    "# SellerBoard (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose run mode:\n",
      "1. Initial run (reprocess all July-August files)\n",
      "2. Incremental run (process only new/modified files)\n",
      "============================================================\n",
      "🚀 INITIAL RUN: Processing all July-August files\n",
      "============================================================\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "📋 Available columns: 19/21\n",
      "⚠️ Missing columns: ['VAT', 'Shipping']\n",
      "🔄 Initial run: Processing July/August file NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📊 Processing: NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "📋 Available columns: 18/21\n",
      "⚠️ Missing columns: ['Sessions', 'VAT', 'Shipping']\n",
      "\n",
      "📈 Combining 64 dataframes...\n",
      "✅ Combined data shape: (15148, 21)\n",
      "📅 Date range: 2025-07-01 00:00:00 to 2025-09-02 00:00:00\n",
      "📤 Uploading to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_21800\\935954928.py:237: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded 15148 rows to Google Sheets\n",
      "🔗 Sheet: Raw_SB_H2_2025_US\n",
      "📋 Columns: Product, ASIN, Date, SKU, Units, Refunds, Sales, Promo, Ads, Sponsored products (PPC), % Refunds, Refund сost, Amazon fees, Cost of Goods, Gross profit, Net profit, Estimated payout, Real ACOS, Sessions, VAT, Shipping\n",
      "\n",
      "🎉 Successfully processed 64 files:\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_07_2025-01_07_2025_(02_16_59_866).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_07_2025-02_07_2025_(02_17_19_401).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_07_2025-03_07_2025_(02_17_35_203).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_07_2025-04_07_2025_(02_17_52_472).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_07_2025-05_07_2025_(02_18_20_680).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_07_2025-06_07_2025_(02_18_35_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_07_2025-07_07_2025_(02_18_54_289).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_07_2025-08_07_2025_(02_19_15_601).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_07_2025-09_07_2025_(02_19_35_777).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_07_2025-10_07_2025_(02_19_53_624).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_07_2025-11_07_2025_(02_20_12_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_07_2025-12_07_2025_(02_20_28_579).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_07_2025-13_07_2025_(02_20_45_881).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_07_2025-14_07_2025_(02_21_47_490).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_07_2025-15_07_2025_(02_22_09_416).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_07_2025-16_07_2025_(02_22_25_149).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_07_2025-17_07_2025_(02_22_44_477).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_07_2025-18_07_2025_(02_23_03_403).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_07_2025-19_07_2025_(02_23_28_630).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_07_2025-20_07_2025_(02_23_44_322).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_07_2025-21_07_2025_(02_24_36_725).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_07_2025-22_07_2025_(02_24_59_422).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_07_2025-23_07_2025_(02_25_27_489).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_07_2025-24_07_2025_(02_25_50_194).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_07_2025-25_07_2025_(02_26_07_983).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_07_2025-26_07_2025_(02_26_32_233).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_07_2025-27_07_2025_(02_27_09_575).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_07_2025-28_07_2025_(02_27_30_663).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_07_2025-29_07_2025_(02_27_54_576).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_07_2025-30_07_2025_(02_28_11_985).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_07_2025-31_07_2025_(02_28_40_743).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_08_2025-01_08_2025_(02_29_37_523).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_08_2025-02_08_2025_(02_31_39_571).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_03_08_2025-03_08_2025_(00_20_13_243).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_04_08_2025-04_08_2025_(00_20_42_850).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_05_08_2025-05_08_2025_(00_21_02_535).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_06_08_2025-06_08_2025_(01_58_45_358).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_07_08_2025-07_08_2025_(01_59_03_545).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_08_08_2025-08_08_2025_(00_21_58_982).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_09_08_2025-09_08_2025_(00_22_17_837).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_10_08_2025-10_08_2025_(00_22_32_925).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_11_08_2025-11_08_2025_(00_22_49_674).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_12_08_2025-12_08_2025_(00_23_06_346).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_13_08_2025-13_08_2025_(00_13_27_500).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_14_08_2025-14_08_2025_(00_14_12_537).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_15_08_2025-15_08_2025_(00_14_36_185).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_16_08_2025-16_08_2025_(00_15_01_117).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_17_08_2025-17_08_2025_(00_15_18_561).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_18_08_2025-18_08_2025_(00_15_40_811).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_19_08_2025-19_08_2025_(00_16_07_413).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_20_08_2025-20_08_2025_(00_16_29_901).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_21_08_2025-21_08_2025_(00_16_50_846).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_22_08_2025-22_08_2025_(00_16_03_351).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_23_08_2025-23_08_2025_(00_16_17_604).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_24_08_2025-24_08_2025_(19_25_08_449).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_25_08_2025-25_08_2025_(19_26_09_208).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_26_08_2025-26_08_2025_(19_26_34_570).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_27_08_2025-27_08_2025_(19_26_49_163).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_28_08_2025-28_08_2025_(19_27_07_383).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_29_08_2025-29_08_2025_(19_27_22_650).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_30_08_2025-30_08_2025_(19_27_39_267).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_31_08_2025-31_08_2025_(19_27_55_200).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_01_09_2025-01_09_2025_(19_28_18_450).xlsx\n",
      "   ✓ NewEleven_Dashboard Products Group by ASIN_02_09_2025-02_09_2025_(19_28_50_332).xlsx\n",
      "\n",
      "📊 PROCESSING SUMMARY\n",
      "=====================\n",
      "July files: 31\n",
      "August files: 31\n",
      "Other files: 2\n",
      "Total files: 64\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-03 11:28:27\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBDataProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_file_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        # Làm sạch tên cột\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        \n",
    "        # Tạo mapping cho các tên cột có thể khác nhau\n",
    "        column_mapping = {}\n",
    "        df_columns_lower = [col.lower() for col in df.columns]\n",
    "        \n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            \n",
    "            # Tìm cột khớp chính xác hoặc gần giống\n",
    "            for i, df_col in enumerate(df.columns):\n",
    "                df_col_lower = df_col.lower()\n",
    "                \n",
    "                # Khớp chính xác\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                # Khớp một phần cho một số trường hợp đặc biệt\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "        \n",
    "        # Rename columns theo mapping\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Chỉ giữ lại các cột cần thiết\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "        \n",
    "        # Thêm các cột thiếu với giá trị None\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = None\n",
    "        \n",
    "        # Sắp xếp lại theo thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "        \n",
    "        print(f\"📋 Available columns: {len(available_columns)}/{len(self.standard_columns)}\")\n",
    "        missing_cols = [col for col in self.standard_columns if col not in available_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_july_august_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to July or August\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month in [7, 8, 9] and file_date.year == 2025  # Adjust year as needed\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date, is_initial_run=False):\n",
    "        \"\"\"Determine if file should be processed\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # For initial run, process all July-August files\n",
    "        if is_initial_run:\n",
    "            if self._is_july_august_file(file_date):\n",
    "                print(f\"🔄 Initial run: Processing July/August file {file_name}\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # For subsequent runs, check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified (hash changed or modification time changed)\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def process_files(self, initial_run=False):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "        Args:\n",
    "            initial_run (bool): If True, reprocess all July-August files from scratch\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        if initial_run:\n",
    "            print(\"🚀 INITIAL RUN: Processing all July-August files\")\n",
    "            # Clear existing July-August metadata for fresh start\n",
    "            files_to_remove = []\n",
    "            for file_name, metadata in self.file_metadata.items():\n",
    "                if isinstance(metadata.get('date'), str):\n",
    "                    file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "                elif metadata.get('date'):\n",
    "                    file_date = metadata['date']\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if self._is_july_august_file(file_date):\n",
    "                    files_to_remove.append(file_name)\n",
    "            \n",
    "            for file_name in files_to_remove:\n",
    "                del self.file_metadata[file_name]\n",
    "                print(f\"🗑️ Cleared metadata for July/August file: {file_name}\")\n",
    "        else:\n",
    "            print(\"🔄 INCREMENTAL RUN: Processing new/modified files only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Scan all Excel files in subfolders\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "                    \n",
    "                    if self._should_process_file(file_path, file_date, initial_run):\n",
    "                        print(f\"📊 Processing: {file}\")\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        \n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "                        else:\n",
    "                            print(f\"⚠️ Empty dataframe for: {file}\")\n",
    "        \n",
    "        # Combine all processed data\n",
    "        if all_dataframes:\n",
    "            print(f\"\\n📈 Combining {len(all_dataframes)} dataframes...\")\n",
    "            master_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Sort by date, then by sales (descending)\n",
    "            if \"Date\" in master_df.columns and \"Sales\" in master_df.columns:\n",
    "                master_df = master_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in master_df.columns:\n",
    "                master_df = master_df.sort_values(\"Date\", ascending=True)\n",
    "            \n",
    "            print(f\"✅ Combined data shape: {master_df.shape}\")\n",
    "            if \"Date\" in master_df.columns:\n",
    "                print(f\"📅 Date range: {master_df['Date'].min()} to {master_df['Date'].max()}\")\n",
    "            \n",
    "            # Upload to Google Sheets\n",
    "            self._upload_to_sheets(master_df)\n",
    "            \n",
    "            # Save metadata\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"\\n🎉 Successfully processed {len(processed_files)} files:\")\n",
    "            for file in processed_files:\n",
    "                print(f\"   ✓ {file}\")\n",
    "            \n",
    "            return master_df\n",
    "        else:\n",
    "            print(\"ℹ️ No files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _upload_to_sheets(self, df):\n",
    "        \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "        try:\n",
    "            print(\"📤 Uploading to Google Sheets...\")\n",
    "            \n",
    "            # Clear existing data (columns A to U to match our 21 standard columns)\n",
    "            self.worksheet.batch_clear(['A:U'])\n",
    "            \n",
    "            # Upload new data\n",
    "            set_with_dataframe(self.worksheet, df)\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {len(df)} rows to Google Sheets\")\n",
    "            print(f\"🔗 Sheet: {self.worksheet_name}\")\n",
    "            print(f\"📋 Columns: {', '.join(self.standard_columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No files processed yet.\"\n",
    "        \n",
    "        july_files = []\n",
    "        august_files = []\n",
    "        other_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 7:\n",
    "                july_files.append(file_name)\n",
    "            elif file_date.month == 8:\n",
    "                august_files.append(file_name)\n",
    "            else:\n",
    "                other_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 PROCESSING SUMMARY\n",
    "=====================\n",
    "July files: {len(july_files)}\n",
    "August files: {len(august_files)}\n",
    "Other files: {len(other_files)}\n",
    "Total files: {len(self.file_metadata)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SBDataProcessor(**config)\n",
    "    \n",
    "    # First time: Run with initial_run=True to reprocess all July-August files\n",
    "    print(\"Choose run mode:\")\n",
    "    print(\"1. Initial run (reprocess all July-August files)\")\n",
    "    print(\"2. Incremental run (process only new/modified files)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        result_df = processor.process_files(initial_run=True)\n",
    "    else:\n",
    "        result_df = processor.process_files(initial_run=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(processor.get_processing_summary())\n",
    "    \n",
    "    # Show column info\n",
    "    print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "    for i, col in enumerate(processor.standard_columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618bef3",
   "metadata": {},
   "source": [
    "# SellerBoard Tháng 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb44420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ September Data Processor\n",
      "This will process ONLY September 2025 files and append to existing sheet data\n",
      "============================================================\n",
      "🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\n",
      "============================================================\n",
      "📊 Current data rows in sheet: 20789\n",
      "➕ New September file detected: NewEleven_Dashboard_Products_Group_by_ASIN_28_09_2025-28_09_2025_(2025_09_29_00_15_973).xlsx\n",
      "📤 Appending September data to Google Sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin1\\AppData\\Local\\Temp\\ipykernel_5944\\1885558304.py:239: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  self.worksheet.update(range_name, values_to_append)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully appended 120 rows to Google Sheets\n",
      "\n",
      "📊 SEPTEMBER PROCESSING SUMMARY\n",
      "===============================\n",
      "September 2025 files: 1\n",
      "Standard columns: 21\n",
      "Last run: 2025-09-29 14:24:20\n",
      "\n",
      "September files processed:\n",
      "  • NewEleven_Dashboard_Products_Group_by_ASIN_28_09_2025-28_09_2025_(2025_09_29_00_15_973).xlsx\n",
      "        \n",
      "\n",
      "📋 Standard columns (21):\n",
      "    1. Product\n",
      "    2. ASIN\n",
      "    3. Date\n",
      "    4. SKU\n",
      "    5. Units\n",
      "    6. Refunds\n",
      "    7. Sales\n",
      "    8. Promo\n",
      "    9. Ads\n",
      "   10. Sponsored products (PPC)\n",
      "   11. % Refunds\n",
      "   12. Refund сost\n",
      "   13. Amazon fees\n",
      "   14. Cost of Goods\n",
      "   15. Gross profit\n",
      "   16. Net profit\n",
      "   17. Estimated payout\n",
      "   18. Real ACOS\n",
      "   19. Sessions\n",
      "   20. VAT\n",
      "   21. Shipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "class SBSeptemberProcessor:\n",
    "    def __init__(self, base_folder, credentials_path, sheet_id, worksheet_name):\n",
    "        self.base_folder = base_folder\n",
    "        self.credentials_path = credentials_path\n",
    "        self.sheet_id = sheet_id\n",
    "        self.worksheet_name = worksheet_name\n",
    "        self.metadata_file = \"sb_september_metadata.json\"\n",
    "        \n",
    "        # Định nghĩa thứ tự cột chuẩn\n",
    "        self.standard_columns = [\n",
    "            'Product', 'ASIN', 'Date', 'SKU', 'Units', 'Refunds', 'Sales', \n",
    "            'Promo', 'Ads', 'Sponsored products (PPC)', '% Refunds', 'Refund сost',\n",
    "            'Amazon fees', 'Cost of Goods', 'Gross profit', 'Net profit', \n",
    "            'Estimated payout', 'Real ACOS', 'Sessions', 'VAT', 'Shipping'\n",
    "        ]\n",
    "        \n",
    "        # Initialize Google Sheets\n",
    "        self._init_google_sheets()\n",
    "        \n",
    "        # Load existing metadata cho tháng 9\n",
    "        self.file_metadata = self._load_metadata()\n",
    "        \n",
    "    def _init_google_sheets(self):\n",
    "        \"\"\"Initialize Google Sheets connection\"\"\"\n",
    "        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \n",
    "                  \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = Credentials.from_service_account_file(self.credentials_path, scopes=scopes)\n",
    "        self.client = gspread.authorize(creds)\n",
    "        self.spreadsheet = self.client.open_by_key(self.sheet_id)\n",
    "        self.worksheet = self.spreadsheet.worksheet(self.worksheet_name)\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load file metadata from JSON file\"\"\"\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save file metadata to JSON file\"\"\"\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.file_metadata, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    def _get_file_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for change detection\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract first DD_MM_YYYY pattern from filename\"\"\"\n",
    "        match = re.search(r\"(\\d{2}_\\d{2}_\\d{4})\", filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(1), \"%d_%m_%Y\").date()\n",
    "        return None\n",
    "    \n",
    "    def _standardize_columns(self, df):\n",
    "        \"\"\"Standardize and select only required columns\"\"\"\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        column_mapping = {}\n",
    "        for std_col in self.standard_columns:\n",
    "            std_col_lower = std_col.lower()\n",
    "            for df_col in df.columns:\n",
    "                df_col_lower = df_col.lower()\n",
    "                if std_col_lower == df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'sponsored' in std_col_lower and 'sponsored' in df_col_lower and 'ppc' in df_col_lower:\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "                elif 'refund' in std_col_lower and 'cost' in std_col_lower and 'refund' in df_col_lower and ('cost' in df_col_lower or 'сost' in df_col_lower):\n",
    "                    column_mapping[df_col] = std_col\n",
    "                    break\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in self.standard_columns if col in df.columns]\n",
    "        df_filtered = df[available_columns].copy()\n",
    "\n",
    "        # Thêm các cột thiếu\n",
    "        for col in self.standard_columns:\n",
    "            if col not in df_filtered.columns:\n",
    "                df_filtered[col] = pd.NA\n",
    "\n",
    "        # Sắp xếp đúng thứ tự chuẩn\n",
    "        df_filtered = df_filtered[self.standard_columns]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "    def process_single_excel(self, file_path):\n",
    "        \"\"\"Process a single Excel file and return DataFrame with Date column\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            df = df.dropna(axis=1, how=\"all\")  \n",
    "            \n",
    "            # Extract date from filename\n",
    "            date_val = self.extract_date_from_filename(os.path.basename(file_path))\n",
    "            if date_val:\n",
    "                df[\"Date\"] = pd.to_datetime(date_val)\n",
    "            \n",
    "            # Standardize columns\n",
    "            df = self._standardize_columns(df)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _is_september_file(self, file_date):\n",
    "        \"\"\"Check if file belongs to September 2025\"\"\"\n",
    "        if not file_date:\n",
    "            return False\n",
    "        return file_date.month == 9 and file_date.year == 2025\n",
    "    \n",
    "    def _should_process_file(self, file_path, file_date):\n",
    "        \"\"\"Determine if September file should be processed\"\"\"\n",
    "        # Chỉ xử lý file tháng 9\n",
    "        if not self._is_september_file(file_date):\n",
    "            return False\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        current_hash = self._get_file_hash(file_path)\n",
    "        modification_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Check if file is new or changed\n",
    "        if file_name not in self.file_metadata:\n",
    "            print(f\"➕ New September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        stored_metadata = self.file_metadata[file_name]\n",
    "        \n",
    "        # Check if file has been modified\n",
    "        if (stored_metadata.get('hash') != current_hash or \n",
    "            stored_metadata.get('modification_time') != modification_time):\n",
    "            print(f\"🔄 Modified September file detected: {file_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"⏭️ Skipping unchanged September file: {file_name}\")\n",
    "        return False\n",
    "    \n",
    "    def _update_file_metadata(self, file_path, file_date):\n",
    "        \"\"\"Update metadata for processed file\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        self.file_metadata[file_name] = {\n",
    "            'path': file_path,\n",
    "            'date': file_date,\n",
    "            'hash': self._get_file_hash(file_path),\n",
    "            'modification_time': os.path.getmtime(file_path),\n",
    "            'processed_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def get_existing_sheet_data_count(self):\n",
    "        \"\"\"Get current number of rows in the sheet\"\"\"\n",
    "        try:\n",
    "            # Lấy tất cả giá trị trong cột A để đếm số dòng có dữ liệu\n",
    "            all_values = self.worksheet.col_values(1)  # Column A\n",
    "            # Trừ đi header row\n",
    "            data_rows = len([val for val in all_values if val.strip()]) - 1 if all_values else 0\n",
    "            print(f\"📊 Current data rows in sheet: {data_rows}\")\n",
    "            return data_rows\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error getting sheet data count: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_september_files(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🗓️ SEPTEMBER PROCESSOR: Processing September 2025 files only\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        existing_rows = self.get_existing_sheet_data_count()\n",
    "        all_dataframes, processed_files = [], []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = self.extract_date_from_filename(file)\n",
    "\n",
    "                    if self._should_process_file(file_path, file_date):\n",
    "                        df = self.process_single_excel(file_path)\n",
    "                        if not df.empty:\n",
    "                            all_dataframes.append(df)\n",
    "                            processed_files.append(file)\n",
    "                            self._update_file_metadata(file_path, file_date)\n",
    "\n",
    "        if all_dataframes:\n",
    "            september_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "            # Sort by Date then Sales\n",
    "            if \"Date\" in september_df.columns and \"Sales\" in september_df.columns:\n",
    "                september_df = september_df.sort_values([\"Date\", \"Sales\"], ascending=[True, False])\n",
    "            elif \"Date\" in september_df.columns:\n",
    "                september_df = september_df.sort_values(\"Date\")\n",
    "\n",
    "            # Đảm bảo cột theo đúng thứ tự chuẩn\n",
    "            september_df = september_df[self.standard_columns]\n",
    "\n",
    "            self._append_to_sheets(september_df, existing_rows)\n",
    "            self._save_metadata()\n",
    "            return september_df\n",
    "        else:\n",
    "            print(\"ℹ️ No new September files to process.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _append_to_sheets(self, df, existing_rows):\n",
    "        try:\n",
    "            print(\"📤 Appending September data to Google Sheets...\")\n",
    "            start_row = existing_rows + 2\n",
    "\n",
    "            values_to_append = []\n",
    "            for _, row in df.iterrows():\n",
    "                row_values = []\n",
    "                for col in self.standard_columns:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val):\n",
    "                        row_values.append(\"\")  # để Sheets giữ trống\n",
    "                    elif isinstance(val, (pd.Timestamp, datetime)):\n",
    "                        row_values.append(val.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        row_values.append(val)\n",
    "                values_to_append.append(row_values)\n",
    "\n",
    "            end_col = chr(ord('A') + len(self.standard_columns) - 1)\n",
    "            end_row = start_row + len(df) - 1\n",
    "            range_name = f\"A{start_row}:{end_col}{end_row}\"\n",
    "\n",
    "            self.worksheet.update(range_name, values_to_append)\n",
    "            print(f\"✅ Successfully appended {len(df)} rows to Google Sheets\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error appending to Google Sheets: {e}\")\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get summary of processed September files\"\"\"\n",
    "        if not self.file_metadata:\n",
    "            return \"No September files processed yet.\"\n",
    "        \n",
    "        september_files = []\n",
    "        \n",
    "        for file_name, metadata in self.file_metadata.items():\n",
    "            if isinstance(metadata.get('date'), str):\n",
    "                file_date = datetime.strptime(metadata['date'], \"%Y-%m-%d\").date()\n",
    "            elif metadata.get('date'):\n",
    "                file_date = metadata['date']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if file_date.month == 9 and file_date.year == 2025:\n",
    "                september_files.append(file_name)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "📊 SEPTEMBER PROCESSING SUMMARY\n",
    "===============================\n",
    "September 2025 files: {len(september_files)}\n",
    "Standard columns: {len(self.standard_columns)}\n",
    "Last run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "September files processed:\n",
    "{chr(10).join([f\"  • {f}\" for f in september_files]) if september_files else \"  None\"}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_folder': \"C:/Users/admin1/Desktop/Performance-Tracking/Agg-SB/H2_2025_US\",\n",
    "        'credentials_path': \"c:/Users/admin1/Downloads/new_credential.json\",\n",
    "        'sheet_id': \"1lZ4dsi94HaeWshsEizKTyNHeOOG0tpLJhzL9pMxvd6k\",\n",
    "        'worksheet_name': \"Raw_SB_H2_2025_US\"\n",
    "    }\n",
    "    \n",
    "    # Initialize September processor\n",
    "    processor = SBSeptemberProcessor(**config)\n",
    "    \n",
    "    print(\"🗓️ September Data Processor\")\n",
    "    print(\"This will process ONLY September 2025 files and append to existing sheet data\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if confirm == 'y':\n",
    "        # Process September files\n",
    "        result_df = processor.process_september_files()\n",
    "        \n",
    "        # Print summary\n",
    "        print(processor.get_processing_summary())\n",
    "        \n",
    "        # Show column info\n",
    "        print(f\"\\n📋 Standard columns ({len(processor.standard_columns)}):\")\n",
    "        for i, col in enumerate(processor.standard_columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    else:\n",
    "        print(\"❌ Operation cancelled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
